{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-70m-deduped\", \"usvsnsp/pythia-6.9b-ppo\", \"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "# cfg.model_name=\"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "cfg.model_name=\"EleutherAI/pythia-6.9b-deduped\"\n",
    "cfg.target_name=\"usvsnsp/pythia-6.9b-ppo\"\n",
    "cfg.layers=[10]\n",
    "cfg.setting=\"residual\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "cfg.l1_alpha=original_l1_alpha\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=8\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Elriggs/openwebtext-100k\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "cfg.seed = 0\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "model = model.to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b71791e158b5e518_*_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 112750592\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "cfg.max_length = 256\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation size: 4096\n"
     ]
    }
   ],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE\n",
    "from torch import nn\n",
    "params = dict()\n",
    "n_dict_components = activation_size*cfg.ratio\n",
    "params[\"encoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"encoder\"])\n",
    "\n",
    "params[\"decoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"decoder\"])\n",
    "\n",
    "params[\"encoder_bias\"] = torch.empty((n_dict_components,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"encoder_bias\"])\n",
    "\n",
    "params[\"shift_bias\"] = torch.empty((activation_size,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"shift_bias\"])\n",
    "\n",
    "autoencoder = AnthropicSAE(  # TiedSAE, UntiedSAE, AnthropicSAE\n",
    "    # n_feats = n_dict_components, \n",
    "    # activation_size=activation_size,\n",
    "    encoder=params[\"encoder\"],\n",
    "    encoder_bias=params[\"encoder_bias\"],\n",
    "    decoder=params[\"decoder\"],\n",
    "    shift_bias=params[\"shift_bias\"],\n",
    ")\n",
    "autoencoder.to_device(cfg.device)\n",
    "autoencoder.set_grad()\n",
    "# autoencoder.encoder.requires_grad = True\n",
    "# autoencoder.encoder_bias.requires_grad = True\n",
    "# autoencoder.decoder.requires_grad = True\n",
    "# autoencoder.shift_bias.requires_grad = True\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        autoencoder.encoder, \n",
    "        autoencoder.encoder_bias,\n",
    "        autoencoder.decoder,\n",
    "        autoencoder.shift_bias,\n",
    "    ], lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sparsity: 204\n"
     ]
    }
   ],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run_name: lomahony/eleuther-pythia6.9b-hh-sft_1012-231100_204\n"
     ]
    }
   ],
   "source": [
    "original_bias = autoencoder.encoder_bias.clone().detach()\n",
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"{cfg.model_name}_{start_time[4:]}_{cfg.sparsity}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:7tju58hz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>▂▁▇█████▇█▇▁</td></tr><tr><td>L1 Loss</td><td>▁▃▂▁▁▁▂▂▂▂▂█</td></tr><tr><td>Reconstruction Loss</td><td>▁▂▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>Self Similarity</td><td>█▄▆███▇████▁</td></tr><tr><td>Sparsity</td><td>▆▅▃▂▂▁▂▂▁▁▁█</td></tr><tr><td>Tokens</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Total Loss</td><td>▁▂▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>l1_alpha</td><td>▁▁▁▁▁▁▄▄▄▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>834</td></tr><tr><td>L1 Loss</td><td>40.15882</td></tr><tr><td>Reconstruction Loss</td><td>258842.82812</td></tr><tr><td>Self Similarity</td><td>0.63484</td></tr><tr><td>Sparsity</td><td>10146.43848</td></tr><tr><td>Tokens</td><td>450560</td></tr><tr><td>Total Loss</td><td>258882.98438</td></tr><tr><td>l1_alpha</td><td>0.00012</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lomahony/eleuther-pythia6.9b-hh-sft_1012-230018_204</strong> at: <a href='https://wandb.ai/benw8888/sparse%20coding/runs/7tju58hz' target=\"_blank\">https://wandb.ai/benw8888/sparse%20coding/runs/7tju58hz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231012_230020-7tju58hz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:7tju58hz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sparse_coding/wandb/run-20231012_231101-0d4s0g73</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benw8888/sparse%20coding/runs/0d4s0g73' target=\"_blank\">lomahony/eleuther-pythia6.9b-hh-sft_1012-231100_204</a></strong> to <a href='https://wandb.ai/benw8888/sparse%20coding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/benw8888/sparse%20coding' target=\"_blank\">https://wandb.ai/benw8888/sparse%20coding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/benw8888/sparse%20coding/runs/0d4s0g73' target=\"_blank\">https://wandb.ai/benw8888/sparse%20coding/runs/0d4s0g73</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/benw8888/sparse%20coding/runs/0d4s0g73?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f357017ded0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55054 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 8141.0 | Dead Features: 1050 | Total Loss: 127.88 | Reconstruction Loss: 100.38 | L1 Loss: 27.50 | l1_alpha: 1.00e-03 | Tokens: 0 | Self Similarity: 1.00\n",
      "Resampling!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b71791e158b5e518_*_of_00008.arrow\n",
      "  0%|          | 21/55054 [00:38<26:33:23,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 6981.5 | Dead Features: 537 | Total Loss: 40316.38 | Reconstruction Loss: 40188.09 | L1 Loss: 128.29 | l1_alpha: 1.00e-03 | Tokens: 40960 | Self Similarity: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/55054 [01:13<26:53:29,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 4907.9 | Dead Features: 6059 | Total Loss: 2266.19 | Reconstruction Loss: 2212.78 | L1 Loss: 53.41 | l1_alpha: 1.00e-03 | Tokens: 81920 | Self Similarity: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 61/55054 [01:49<27:02:40,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 4136.7 | Dead Features: 6458 | Total Loss: 701.96 | Reconstruction Loss: 663.84 | L1 Loss: 38.12 | l1_alpha: 1.00e-03 | Tokens: 122880 | Self Similarity: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 81/55054 [02:24<27:13:16,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 4039.4 | Dead Features: 6696 | Total Loss: 399.27 | Reconstruction Loss: 363.15 | L1 Loss: 36.11 | l1_alpha: 1.00e-03 | Tokens: 163840 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/55054 [03:00<27:43:29,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3885.1 | Dead Features: 6826 | Total Loss: 242.77 | Reconstruction Loss: 208.32 | L1 Loss: 34.44 | l1_alpha: 1.00e-03 | Tokens: 204800 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 121/55054 [03:35<27:19:15,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3763.8 | Dead Features: 6947 | Total Loss: 206.72 | Reconstruction Loss: 168.37 | L1 Loss: 38.35 | l1_alpha: 1.10e-03 | Tokens: 245760 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 141/55054 [04:11<27:20:10,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3814.2 | Dead Features: 6953 | Total Loss: 173.20 | Reconstruction Loss: 136.76 | L1 Loss: 36.44 | l1_alpha: 1.10e-03 | Tokens: 286720 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 161/55054 [04:47<27:20:37,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3657.7 | Dead Features: 6651 | Total Loss: 144.73 | Reconstruction Loss: 109.40 | L1 Loss: 35.33 | l1_alpha: 1.10e-03 | Tokens: 327680 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 181/55054 [05:23<27:22:13,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3294.5 | Dead Features: 6794 | Total Loss: 107.94 | Reconstruction Loss: 75.26 | L1 Loss: 32.69 | l1_alpha: 1.10e-03 | Tokens: 368640 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 201/55054 [05:59<27:43:54,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3311.4 | Dead Features: 6470 | Total Loss: 107.22 | Reconstruction Loss: 74.61 | L1 Loss: 32.62 | l1_alpha: 1.10e-03 | Tokens: 409600 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 221/55054 [06:35<27:21:41,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3818.1 | Dead Features: 6968 | Total Loss: 112.52 | Reconstruction Loss: 69.79 | L1 Loss: 42.74 | l1_alpha: 1.21e-03 | Tokens: 450560 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 241/55054 [07:11<27:20:45,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3393.1 | Dead Features: 6909 | Total Loss: 99.42 | Reconstruction Loss: 63.28 | L1 Loss: 36.15 | l1_alpha: 1.21e-03 | Tokens: 491520 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 261/55054 [07:46<27:20:27,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3519.1 | Dead Features: 6834 | Total Loss: 91.68 | Reconstruction Loss: 52.23 | L1 Loss: 39.46 | l1_alpha: 1.21e-03 | Tokens: 532480 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 281/55054 [08:22<27:18:51,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3241.7 | Dead Features: 6838 | Total Loss: 87.69 | Reconstruction Loss: 53.67 | L1 Loss: 34.01 | l1_alpha: 1.21e-03 | Tokens: 573440 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 301/55054 [08:58<27:42:25,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3343.0 | Dead Features: 6778 | Total Loss: 76.05 | Reconstruction Loss: 40.24 | L1 Loss: 35.81 | l1_alpha: 1.21e-03 | Tokens: 614400 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 321/55054 [09:34<27:19:06,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3173.2 | Dead Features: 6554 | Total Loss: 79.95 | Reconstruction Loss: 41.45 | L1 Loss: 38.50 | l1_alpha: 1.33e-03 | Tokens: 655360 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 341/55054 [10:10<27:18:50,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3347.9 | Dead Features: 6800 | Total Loss: 75.65 | Reconstruction Loss: 38.20 | L1 Loss: 37.46 | l1_alpha: 1.33e-03 | Tokens: 696320 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 361/55054 [10:46<27:18:59,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3130.9 | Dead Features: 6812 | Total Loss: 73.99 | Reconstruction Loss: 37.96 | L1 Loss: 36.03 | l1_alpha: 1.33e-03 | Tokens: 737280 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 381/55054 [11:22<27:18:53,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3152.9 | Dead Features: 6843 | Total Loss: 75.01 | Reconstruction Loss: 37.04 | L1 Loss: 37.97 | l1_alpha: 1.33e-03 | Tokens: 778240 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 401/55054 [11:58<27:40:43,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2957.2 | Dead Features: 6734 | Total Loss: 66.34 | Reconstruction Loss: 30.07 | L1 Loss: 36.27 | l1_alpha: 1.33e-03 | Tokens: 819200 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 421/55054 [12:34<27:15:21,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3211.1 | Dead Features: 6752 | Total Loss: 73.91 | Reconstruction Loss: 32.35 | L1 Loss: 41.55 | l1_alpha: 1.46e-03 | Tokens: 860160 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 441/55054 [13:09<27:15:35,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3054.1 | Dead Features: 6763 | Total Loss: 126.96 | Reconstruction Loss: 86.20 | L1 Loss: 40.76 | l1_alpha: 1.46e-03 | Tokens: 901120 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 461/55054 [13:45<27:13:56,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3604.3 | Dead Features: 6789 | Total Loss: 85.78 | Reconstruction Loss: 38.85 | L1 Loss: 46.93 | l1_alpha: 1.46e-03 | Tokens: 942080 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 481/55054 [14:21<27:14:50,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2823.3 | Dead Features: 6811 | Total Loss: 73.82 | Reconstruction Loss: 36.68 | L1 Loss: 37.14 | l1_alpha: 1.46e-03 | Tokens: 983040 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/55054 [14:57<27:38:24,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2941.0 | Dead Features: 6573 | Total Loss: 76.75 | Reconstruction Loss: 38.04 | L1 Loss: 38.71 | l1_alpha: 1.46e-03 | Tokens: 1024000 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 521/55054 [15:33<27:12:40,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2874.4 | Dead Features: 6879 | Total Loss: 86.41 | Reconstruction Loss: 46.03 | L1 Loss: 40.38 | l1_alpha: 1.61e-03 | Tokens: 1064960 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 541/55054 [16:09<27:12:13,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3302.0 | Dead Features: 6815 | Total Loss: 88.59 | Reconstruction Loss: 42.33 | L1 Loss: 46.26 | l1_alpha: 1.61e-03 | Tokens: 1105920 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 561/55054 [16:45<27:12:13,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 3196.2 | Dead Features: 6863 | Total Loss: 86.50 | Reconstruction Loss: 42.62 | L1 Loss: 43.87 | l1_alpha: 1.61e-03 | Tokens: 1146880 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 581/55054 [17:21<27:11:58,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2275.2 | Dead Features: 6790 | Total Loss: 63.26 | Reconstruction Loss: 29.62 | L1 Loss: 33.64 | l1_alpha: 1.61e-03 | Tokens: 1187840 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 601/55054 [17:57<27:34:51,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2759.2 | Dead Features: 6850 | Total Loss: 71.02 | Reconstruction Loss: 33.88 | L1 Loss: 37.14 | l1_alpha: 1.61e-03 | Tokens: 1228800 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 621/55054 [18:32<27:10:49,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2445.1 | Dead Features: 6892 | Total Loss: 61.29 | Reconstruction Loss: 23.78 | L1 Loss: 37.51 | l1_alpha: 1.77e-03 | Tokens: 1269760 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 641/55054 [19:08<27:10:27,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2412.6 | Dead Features: 6759 | Total Loss: 63.94 | Reconstruction Loss: 24.12 | L1 Loss: 39.83 | l1_alpha: 1.77e-03 | Tokens: 1310720 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 661/55054 [19:44<27:08:13,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2347.0 | Dead Features: 6455 | Total Loss: 63.42 | Reconstruction Loss: 24.82 | L1 Loss: 38.60 | l1_alpha: 1.77e-03 | Tokens: 1351680 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 681/55054 [20:20<27:08:07,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2418.3 | Dead Features: 6390 | Total Loss: 63.34 | Reconstruction Loss: 24.85 | L1 Loss: 38.49 | l1_alpha: 1.77e-03 | Tokens: 1392640 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 701/55054 [20:56<27:30:28,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2200.9 | Dead Features: 6991 | Total Loss: 80.72 | Reconstruction Loss: 44.18 | L1 Loss: 36.53 | l1_alpha: 1.77e-03 | Tokens: 1433600 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 721/55054 [21:32<27:07:55,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2014.4 | Dead Features: 7051 | Total Loss: 75.19 | Reconstruction Loss: 37.82 | L1 Loss: 37.38 | l1_alpha: 1.95e-03 | Tokens: 1474560 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 741/55054 [22:08<27:06:15,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2407.4 | Dead Features: 6940 | Total Loss: 74.70 | Reconstruction Loss: 32.78 | L1 Loss: 41.92 | l1_alpha: 1.95e-03 | Tokens: 1515520 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 761/55054 [22:44<27:06:28,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2123.8 | Dead Features: 6538 | Total Loss: 61.44 | Reconstruction Loss: 21.85 | L1 Loss: 39.59 | l1_alpha: 1.95e-03 | Tokens: 1556480 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 781/55054 [23:20<27:04:37,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2195.0 | Dead Features: 7110 | Total Loss: 72.26 | Reconstruction Loss: 34.62 | L1 Loss: 37.64 | l1_alpha: 1.95e-03 | Tokens: 1597440 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 801/55054 [23:56<27:29:07,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2415.5 | Dead Features: 6854 | Total Loss: 61.08 | Reconstruction Loss: 21.85 | L1 Loss: 39.23 | l1_alpha: 1.95e-03 | Tokens: 1638400 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 821/55054 [24:31<27:03:16,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2061.2 | Dead Features: 6842 | Total Loss: 60.63 | Reconstruction Loss: 21.91 | L1 Loss: 38.72 | l1_alpha: 2.14e-03 | Tokens: 1679360 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 841/55054 [25:07<27:03:44,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2174.2 | Dead Features: 7131 | Total Loss: 68.75 | Reconstruction Loss: 27.93 | L1 Loss: 40.82 | l1_alpha: 2.14e-03 | Tokens: 1720320 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 861/55054 [25:43<27:03:07,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1742.5 | Dead Features: 7081 | Total Loss: 53.16 | Reconstruction Loss: 18.24 | L1 Loss: 34.92 | l1_alpha: 2.14e-03 | Tokens: 1761280 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 881/55054 [26:19<27:02:23,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 2030.7 | Dead Features: 6913 | Total Loss: 64.56 | Reconstruction Loss: 24.82 | L1 Loss: 39.74 | l1_alpha: 2.14e-03 | Tokens: 1802240 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 901/55054 [26:55<27:24:30,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1465.5 | Dead Features: 6979 | Total Loss: 47.02 | Reconstruction Loss: 16.06 | L1 Loss: 30.96 | l1_alpha: 2.14e-03 | Tokens: 1843200 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 921/55054 [27:31<27:01:23,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1481.2 | Dead Features: 7062 | Total Loss: 52.79 | Reconstruction Loss: 18.45 | L1 Loss: 34.34 | l1_alpha: 2.36e-03 | Tokens: 1884160 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 941/55054 [28:07<27:00:09,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1345.0 | Dead Features: 6986 | Total Loss: 48.53 | Reconstruction Loss: 13.09 | L1 Loss: 35.44 | l1_alpha: 2.36e-03 | Tokens: 1925120 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 961/55054 [28:43<26:58:47,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1391.4 | Dead Features: 7052 | Total Loss: 48.94 | Reconstruction Loss: 14.03 | L1 Loss: 34.91 | l1_alpha: 2.36e-03 | Tokens: 1966080 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 981/55054 [29:18<26:58:20,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1676.1 | Dead Features: 6818 | Total Loss: 50.48 | Reconstruction Loss: 15.44 | L1 Loss: 35.04 | l1_alpha: 2.36e-03 | Tokens: 2007040 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1000/55054 [29:52<26:55:24,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1557.6 | Dead Features: 7068 | Total Loss: 52.93 | Reconstruction Loss: 17.47 | L1 Loss: 35.46 | l1_alpha: 2.36e-03 | Tokens: 2048000 | Self Similarity: 1.00\n",
      "Resampling!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b71791e158b5e518_*_of_00008.arrow\n",
      "  2%|▏         | 1021/55054 [30:33<26:56:45,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 962.9 | Dead Features: 6526 | Total Loss: 7648.69 | Reconstruction Loss: 7595.11 | L1 Loss: 53.59 | l1_alpha: 2.59e-03 | Tokens: 2088960 | Self Similarity: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1041/55054 [31:09<26:57:31,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 497.6 | Dead Features: 6886 | Total Loss: 693.27 | Reconstruction Loss: 664.87 | L1 Loss: 28.40 | l1_alpha: 2.59e-03 | Tokens: 2129920 | Self Similarity: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1061/55054 [31:45<26:56:37,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 436.6 | Dead Features: 6952 | Total Loss: 261.01 | Reconstruction Loss: 237.93 | L1 Loss: 23.08 | l1_alpha: 2.59e-03 | Tokens: 2170880 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1081/55054 [32:21<26:55:34,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 428.7 | Dead Features: 7288 | Total Loss: 160.30 | Reconstruction Loss: 140.11 | L1 Loss: 20.18 | l1_alpha: 2.59e-03 | Tokens: 2211840 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1101/55054 [32:57<27:18:50,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 428.6 | Dead Features: 6663 | Total Loss: 113.02 | Reconstruction Loss: 91.14 | L1 Loss: 21.88 | l1_alpha: 2.59e-03 | Tokens: 2252800 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1121/55054 [33:32<26:54:27,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 401.0 | Dead Features: 7322 | Total Loss: 92.44 | Reconstruction Loss: 69.09 | L1 Loss: 23.35 | l1_alpha: 2.85e-03 | Tokens: 2293760 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1141/55054 [34:08<26:54:43,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 310.8 | Dead Features: 7325 | Total Loss: 53.07 | Reconstruction Loss: 33.52 | L1 Loss: 19.55 | l1_alpha: 2.85e-03 | Tokens: 2334720 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1161/55054 [34:44<26:54:06,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 365.6 | Dead Features: 7204 | Total Loss: 55.61 | Reconstruction Loss: 33.43 | L1 Loss: 22.17 | l1_alpha: 2.85e-03 | Tokens: 2375680 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1181/55054 [35:20<26:53:27,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 314.9 | Dead Features: 7079 | Total Loss: 40.39 | Reconstruction Loss: 19.94 | L1 Loss: 20.45 | l1_alpha: 2.85e-03 | Tokens: 2416640 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1201/55054 [35:56<27:13:41,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 319.5 | Dead Features: 7220 | Total Loss: 42.20 | Reconstruction Loss: 23.23 | L1 Loss: 18.96 | l1_alpha: 2.85e-03 | Tokens: 2457600 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1221/55054 [36:32<26:51:09,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 323.8 | Dead Features: 7301 | Total Loss: 36.75 | Reconstruction Loss: 16.34 | L1 Loss: 20.41 | l1_alpha: 3.14e-03 | Tokens: 2498560 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1241/55054 [37:08<26:50:16,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 316.1 | Dead Features: 7213 | Total Loss: 37.97 | Reconstruction Loss: 16.46 | L1 Loss: 21.52 | l1_alpha: 3.14e-03 | Tokens: 2539520 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1261/55054 [37:44<26:48:14,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 324.4 | Dead Features: 7240 | Total Loss: 38.71 | Reconstruction Loss: 17.32 | L1 Loss: 21.39 | l1_alpha: 3.14e-03 | Tokens: 2580480 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1281/55054 [38:19<26:48:34,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 251.5 | Dead Features: 7169 | Total Loss: 31.39 | Reconstruction Loss: 12.62 | L1 Loss: 18.77 | l1_alpha: 3.14e-03 | Tokens: 2621440 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1301/55054 [38:55<27:10:51,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 273.5 | Dead Features: 7154 | Total Loss: 31.76 | Reconstruction Loss: 12.06 | L1 Loss: 19.70 | l1_alpha: 3.14e-03 | Tokens: 2662400 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1321/55054 [39:31<26:45:01,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 342.4 | Dead Features: 6980 | Total Loss: 38.65 | Reconstruction Loss: 15.72 | L1 Loss: 22.93 | l1_alpha: 3.45e-03 | Tokens: 2703360 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1341/55054 [40:07<26:43:28,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 203.4 | Dead Features: 6935 | Total Loss: 28.04 | Reconstruction Loss: 10.37 | L1 Loss: 17.67 | l1_alpha: 3.45e-03 | Tokens: 2744320 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1361/55054 [40:43<26:41:37,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 291.5 | Dead Features: 7450 | Total Loss: 33.06 | Reconstruction Loss: 11.85 | L1 Loss: 21.21 | l1_alpha: 3.45e-03 | Tokens: 2785280 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1381/55054 [41:18<26:40:22,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 255.7 | Dead Features: 7314 | Total Loss: 30.22 | Reconstruction Loss: 11.48 | L1 Loss: 18.74 | l1_alpha: 3.45e-03 | Tokens: 2826240 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1401/55054 [41:54<27:02:56,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 281.9 | Dead Features: 7042 | Total Loss: 33.43 | Reconstruction Loss: 13.54 | L1 Loss: 19.90 | l1_alpha: 3.45e-03 | Tokens: 2867200 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1421/55054 [42:30<26:39:12,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 250.3 | Dead Features: 7427 | Total Loss: 38.99 | Reconstruction Loss: 19.34 | L1 Loss: 19.65 | l1_alpha: 3.80e-03 | Tokens: 2908160 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1441/55054 [43:06<26:37:25,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 232.6 | Dead Features: 7411 | Total Loss: 31.07 | Reconstruction Loss: 9.58 | L1 Loss: 21.49 | l1_alpha: 3.80e-03 | Tokens: 2949120 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1461/55054 [43:41<26:38:03,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 270.0 | Dead Features: 7281 | Total Loss: 32.61 | Reconstruction Loss: 10.77 | L1 Loss: 21.84 | l1_alpha: 3.80e-03 | Tokens: 2990080 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1481/55054 [44:17<26:36:11,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 290.7 | Dead Features: 7225 | Total Loss: 34.47 | Reconstruction Loss: 12.43 | L1 Loss: 22.04 | l1_alpha: 3.80e-03 | Tokens: 3031040 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1501/55054 [44:53<26:58:05,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 240.0 | Dead Features: 7379 | Total Loss: 29.42 | Reconstruction Loss: 9.08 | L1 Loss: 20.34 | l1_alpha: 3.80e-03 | Tokens: 3072000 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1521/55054 [45:28<26:34:32,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 249.4 | Dead Features: 7139 | Total Loss: 36.71 | Reconstruction Loss: 14.64 | L1 Loss: 22.07 | l1_alpha: 4.18e-03 | Tokens: 3112960 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1541/55054 [46:04<26:37:37,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 214.0 | Dead Features: 7401 | Total Loss: 28.53 | Reconstruction Loss: 8.49 | L1 Loss: 20.04 | l1_alpha: 4.18e-03 | Tokens: 3153920 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1561/55054 [46:40<26:38:39,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 235.1 | Dead Features: 7395 | Total Loss: 29.17 | Reconstruction Loss: 7.89 | L1 Loss: 21.29 | l1_alpha: 4.18e-03 | Tokens: 3194880 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1581/55054 [47:16<26:38:29,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 229.2 | Dead Features: 7503 | Total Loss: 29.74 | Reconstruction Loss: 8.58 | L1 Loss: 21.16 | l1_alpha: 4.18e-03 | Tokens: 3235840 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1601/55054 [47:52<27:00:37,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 248.9 | Dead Features: 7260 | Total Loss: 31.16 | Reconstruction Loss: 11.52 | L1 Loss: 19.64 | l1_alpha: 4.18e-03 | Tokens: 3276800 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1621/55054 [48:27<26:37:15,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 216.7 | Dead Features: 7304 | Total Loss: 31.90 | Reconstruction Loss: 9.90 | L1 Loss: 22.01 | l1_alpha: 4.59e-03 | Tokens: 3317760 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1641/55054 [49:03<26:33:26,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 213.6 | Dead Features: 7616 | Total Loss: 32.86 | Reconstruction Loss: 9.71 | L1 Loss: 23.15 | l1_alpha: 4.59e-03 | Tokens: 3358720 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1661/55054 [49:39<26:32:45,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 188.5 | Dead Features: 7100 | Total Loss: 30.02 | Reconstruction Loss: 10.08 | L1 Loss: 19.94 | l1_alpha: 4.59e-03 | Tokens: 3399680 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1681/55054 [50:15<26:30:40,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 210.5 | Dead Features: 7273 | Total Loss: 31.05 | Reconstruction Loss: 8.79 | L1 Loss: 22.27 | l1_alpha: 4.59e-03 | Tokens: 3440640 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1701/55054 [50:50<26:53:49,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 172.3 | Dead Features: 7225 | Total Loss: 26.30 | Reconstruction Loss: 7.14 | L1 Loss: 19.16 | l1_alpha: 4.59e-03 | Tokens: 3481600 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1721/55054 [51:26<26:28:45,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 189.9 | Dead Features: 7670 | Total Loss: 25.18 | Reconstruction Loss: 7.20 | L1 Loss: 17.98 | l1_alpha: 4.14e-03 | Tokens: 3522560 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1741/55054 [52:02<26:29:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 300.4 | Dead Features: 7259 | Total Loss: 41.74 | Reconstruction Loss: 21.07 | L1 Loss: 20.68 | l1_alpha: 4.14e-03 | Tokens: 3563520 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1761/55054 [52:37<26:30:01,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 173.8 | Dead Features: 7191 | Total Loss: 25.51 | Reconstruction Loss: 7.35 | L1 Loss: 18.17 | l1_alpha: 4.14e-03 | Tokens: 3604480 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1781/55054 [53:13<26:26:21,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 209.7 | Dead Features: 7634 | Total Loss: 27.66 | Reconstruction Loss: 7.68 | L1 Loss: 19.98 | l1_alpha: 4.14e-03 | Tokens: 3645440 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1801/55054 [53:49<26:49:48,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 184.3 | Dead Features: 7175 | Total Loss: 25.04 | Reconstruction Loss: 7.21 | L1 Loss: 17.83 | l1_alpha: 4.14e-03 | Tokens: 3686400 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1821/55054 [54:25<26:28:11,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 218.9 | Dead Features: 7521 | Total Loss: 26.63 | Reconstruction Loss: 7.84 | L1 Loss: 18.80 | l1_alpha: 4.14e-03 | Tokens: 3727360 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1841/55054 [55:00<26:25:50,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 174.8 | Dead Features: 7371 | Total Loss: 23.93 | Reconstruction Loss: 7.22 | L1 Loss: 16.71 | l1_alpha: 4.14e-03 | Tokens: 3768320 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1861/55054 [55:36<26:23:48,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 236.5 | Dead Features: 7253 | Total Loss: 29.44 | Reconstruction Loss: 10.52 | L1 Loss: 18.91 | l1_alpha: 4.14e-03 | Tokens: 3809280 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1881/55054 [56:12<26:23:43,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 155.4 | Dead Features: 7478 | Total Loss: 22.75 | Reconstruction Loss: 6.84 | L1 Loss: 15.91 | l1_alpha: 4.14e-03 | Tokens: 3850240 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1901/55054 [56:48<26:45:42,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 169.6 | Dead Features: 7636 | Total Loss: 24.37 | Reconstruction Loss: 7.36 | L1 Loss: 17.01 | l1_alpha: 4.14e-03 | Tokens: 3891200 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1921/55054 [57:23<26:20:23,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 183.0 | Dead Features: 7612 | Total Loss: 23.64 | Reconstruction Loss: 7.15 | L1 Loss: 16.48 | l1_alpha: 3.72e-03 | Tokens: 3932160 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1941/55054 [57:59<26:20:08,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 151.7 | Dead Features: 7530 | Total Loss: 22.38 | Reconstruction Loss: 7.69 | L1 Loss: 14.69 | l1_alpha: 3.72e-03 | Tokens: 3973120 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1961/55054 [58:34<26:19:41,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 165.2 | Dead Features: 7575 | Total Loss: 21.45 | Reconstruction Loss: 7.69 | L1 Loss: 13.76 | l1_alpha: 3.72e-03 | Tokens: 4014080 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1981/55054 [59:10<26:19:39,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 213.4 | Dead Features: 7156 | Total Loss: 23.83 | Reconstruction Loss: 8.22 | L1 Loss: 15.62 | l1_alpha: 3.72e-03 | Tokens: 4055040 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2000/55054 [59:44<26:16:23,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 153.8 | Dead Features: 7650 | Total Loss: 20.80 | Reconstruction Loss: 7.65 | L1 Loss: 13.15 | l1_alpha: 3.72e-03 | Tokens: 4096000 | Self Similarity: 1.00\n",
      "Resampling!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b71791e158b5e518_*_of_00008.arrow\n",
      "  4%|▎         | 2021/55054 [1:00:24<26:17:27,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1222.4 | Dead Features: 1880 | Total Loss: 68389.82 | Reconstruction Loss: 68081.30 | L1 Loss: 308.52 | l1_alpha: 3.35e-03 | Tokens: 4136960 | Self Similarity: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2041/55054 [1:01:00<26:19:48,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 683.3 | Dead Features: 2073 | Total Loss: 11818.96 | Reconstruction Loss: 11585.87 | L1 Loss: 233.09 | l1_alpha: 3.35e-03 | Tokens: 4177920 | Self Similarity: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2061/55054 [1:01:36<26:18:52,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 656.9 | Dead Features: 2336 | Total Loss: 3978.73 | Reconstruction Loss: 3748.73 | L1 Loss: 229.99 | l1_alpha: 3.35e-03 | Tokens: 4218880 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2081/55054 [1:02:11<26:18:10,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 692.7 | Dead Features: 2353 | Total Loss: 1495.34 | Reconstruction Loss: 1259.14 | L1 Loss: 236.20 | l1_alpha: 3.35e-03 | Tokens: 4259840 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2101/55054 [1:02:47<26:39:47,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 649.1 | Dead Features: 2412 | Total Loss: 861.87 | Reconstruction Loss: 623.89 | L1 Loss: 237.98 | l1_alpha: 3.35e-03 | Tokens: 4300800 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2121/55054 [1:03:23<26:15:33,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 590.9 | Dead Features: 2247 | Total Loss: 542.31 | Reconstruction Loss: 305.02 | L1 Loss: 237.29 | l1_alpha: 3.68e-03 | Tokens: 4341760 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2141/55054 [1:03:58<26:17:45,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 618.9 | Dead Features: 2119 | Total Loss: 581.16 | Reconstruction Loss: 342.47 | L1 Loss: 238.69 | l1_alpha: 3.68e-03 | Tokens: 4382720 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2161/55054 [1:04:34<26:13:57,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 629.7 | Dead Features: 2187 | Total Loss: 539.79 | Reconstruction Loss: 297.09 | L1 Loss: 242.70 | l1_alpha: 3.68e-03 | Tokens: 4423680 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2181/55054 [1:05:10<26:14:05,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 637.8 | Dead Features: 1829 | Total Loss: 461.66 | Reconstruction Loss: 237.66 | L1 Loss: 224.00 | l1_alpha: 3.68e-03 | Tokens: 4464640 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2201/55054 [1:05:46<26:36:28,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 611.7 | Dead Features: 1982 | Total Loss: 428.45 | Reconstruction Loss: 172.66 | L1 Loss: 255.78 | l1_alpha: 3.68e-03 | Tokens: 4505600 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2221/55054 [1:06:21<26:11:43,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 614.5 | Dead Features: 2333 | Total Loss: 428.60 | Reconstruction Loss: 144.72 | L1 Loss: 283.87 | l1_alpha: 4.05e-03 | Tokens: 4546560 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2241/55054 [1:06:57<26:13:05,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 613.9 | Dead Features: 2153 | Total Loss: 389.20 | Reconstruction Loss: 115.80 | L1 Loss: 273.40 | l1_alpha: 4.05e-03 | Tokens: 4587520 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2261/55054 [1:07:33<26:15:40,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 625.9 | Dead Features: 2462 | Total Loss: 470.61 | Reconstruction Loss: 178.82 | L1 Loss: 291.79 | l1_alpha: 4.05e-03 | Tokens: 4628480 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2281/55054 [1:08:08<26:14:44,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 635.9 | Dead Features: 2383 | Total Loss: 423.92 | Reconstruction Loss: 141.29 | L1 Loss: 282.63 | l1_alpha: 4.05e-03 | Tokens: 4669440 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2301/55054 [1:08:44<26:36:38,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 587.7 | Dead Features: 2246 | Total Loss: 365.01 | Reconstruction Loss: 95.43 | L1 Loss: 269.59 | l1_alpha: 4.05e-03 | Tokens: 4710400 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2321/55054 [1:09:20<26:14:19,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 593.2 | Dead Features: 2147 | Total Loss: 382.01 | Reconstruction Loss: 97.32 | L1 Loss: 284.69 | l1_alpha: 4.46e-03 | Tokens: 4751360 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2341/55054 [1:09:56<26:12:36,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 598.4 | Dead Features: 2238 | Total Loss: 367.81 | Reconstruction Loss: 77.45 | L1 Loss: 290.36 | l1_alpha: 4.46e-03 | Tokens: 4792320 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2361/55054 [1:10:31<26:10:06,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 610.0 | Dead Features: 2246 | Total Loss: 387.74 | Reconstruction Loss: 93.20 | L1 Loss: 294.54 | l1_alpha: 4.46e-03 | Tokens: 4833280 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2381/55054 [1:11:07<26:09:54,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 608.8 | Dead Features: 2317 | Total Loss: 412.15 | Reconstruction Loss: 113.16 | L1 Loss: 298.99 | l1_alpha: 4.46e-03 | Tokens: 4874240 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2401/55054 [1:11:43<26:32:09,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 595.9 | Dead Features: 2530 | Total Loss: 371.84 | Reconstruction Loss: 74.75 | L1 Loss: 297.09 | l1_alpha: 4.46e-03 | Tokens: 4915200 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2421/55054 [1:12:18<26:09:36,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 602.2 | Dead Features: 2091 | Total Loss: 396.61 | Reconstruction Loss: 61.72 | L1 Loss: 334.88 | l1_alpha: 4.90e-03 | Tokens: 4956160 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2441/55054 [1:12:54<26:08:54,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 610.7 | Dead Features: 2184 | Total Loss: 402.14 | Reconstruction Loss: 73.28 | L1 Loss: 328.86 | l1_alpha: 4.90e-03 | Tokens: 4997120 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2460/55054 [1:13:28<26:07:46,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 675.9 | Dead Features: 2171 | Total Loss: 512.79 | Reconstruction Loss: 191.09 | L1 Loss: 321.69 | l1_alpha: 4.90e-03 | Tokens: 5038080 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2480/55054 [1:14:26<41:45:18,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 590.3 | Dead Features: 2176 | Total Loss: 385.50 | Reconstruction Loss: 51.91 | L1 Loss: 333.60 | l1_alpha: 4.90e-03 | Tokens: 5079040 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2500/55054 [1:15:28<47:06:25,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 570.3 | Dead Features: 2337 | Total Loss: 367.45 | Reconstruction Loss: 48.65 | L1 Loss: 318.80 | l1_alpha: 4.90e-03 | Tokens: 5120000 | Self Similarity: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2503/55054 [1:15:38<26:28:06,  1.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(): \u001b[39m# As long as not doing KL divergence, don't need gradients for model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(model, tensor_names[\u001b[39m0\u001b[39m]) \u001b[39mas\u001b[39;00m ret:\n\u001b[0;32m---> 15\u001b[0m         _ \u001b[39m=\u001b[39m model(tokens)\n\u001b[1;32m     16\u001b[0m         representation \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39moutput\n\u001b[1;32m     17\u001b[0m         \u001b[39mif\u001b[39;00m(\u001b[39misinstance\u001b[39m(representation, \u001b[39mtuple\u001b[39m)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:673\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[39mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[39m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    671\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 673\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt_neox(\n\u001b[1;32m    674\u001b[0m     input_ids,\n\u001b[1;32m    675\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    676\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    677\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    678\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    679\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    680\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    681\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    682\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    683\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    684\u001b[0m )\n\u001b[1;32m    686\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    687\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:564\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    556\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    557\u001b[0m         create_custom_forward(layer),\n\u001b[1;32m    558\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    561\u001b[0m         head_mask[i],\n\u001b[1;32m    562\u001b[0m     )\n\u001b[1;32m    563\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 564\u001b[0m     outputs \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    565\u001b[0m         hidden_states,\n\u001b[1;32m    566\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    567\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    568\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    569\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    570\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    571\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    572\u001b[0m     )\n\u001b[1;32m    573\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:331\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     hidden_states: Optional[torch\u001b[39m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    330\u001b[0m ):\n\u001b[0;32m--> 331\u001b[0m     attention_layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    332\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states),\n\u001b[1;32m    333\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    334\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    335\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    336\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    337\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    338\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    339\u001b[0m     )\n\u001b[1;32m    340\u001b[0m     attn_output \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     outputs \u001b[39m=\u001b[39m attention_layer_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:149\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m has_layer_past:\n\u001b[1;32m    148\u001b[0m     seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layer_past[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 149\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value, seq_len\u001b[39m=\u001b[39;49mseq_len)\n\u001b[1;32m    150\u001b[0m query, key \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n\u001b[1;32m    151\u001b[0m query \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((query, query_pass), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:278\u001b[0m, in \u001b[0;36mRotaryEmbedding.forward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcos_cached \u001b[39m=\u001b[39m emb\u001b[39m.\u001b[39mcos()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :]\n\u001b[1;32m    277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached \u001b[39m=\u001b[39m emb\u001b[39m.\u001b[39msin()[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :, :]\n\u001b[0;32m--> 278\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcos_cached[:seq_len, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\u001b[39m.\u001b[39;49mto(x\u001b[39m.\u001b[39;49mdevice), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msin_cached[:seq_len, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_since_activation = torch.zeros(autoencoder.encoder.shape[0])\n",
    "total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "max_num_tokens = 30_000_000\n",
    "save_every = 5_000_000\n",
    "num_saved_so_far = 0\n",
    "# Freeze model parameters \n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(cfg.device)\n",
    "last_encoder = autoencoder.encoder.clone().detach()\n",
    "for i, batch in enumerate(tqdm(token_loader)):\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "    with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "        with Trace(model, tensor_names[0]) as ret:\n",
    "            _ = model(tokens)\n",
    "            representation = ret.output\n",
    "            if(isinstance(representation, tuple)):\n",
    "                representation = representation[0]\n",
    "    layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    # activation_saver.save_batch(layer_activations.clone().cpu().detach())\n",
    "\n",
    "    c = autoencoder.encode(layer_activations)\n",
    "    x_hat = autoencoder.decode(c)\n",
    "    \n",
    "    reconstruction_loss = (x_hat - layer_activations).pow(2).mean()\n",
    "    l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "    total_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "\n",
    "    time_since_activation += 1\n",
    "    time_since_activation = time_since_activation * (c.sum(dim=0).cpu()==0)\n",
    "    total_activations += c.sum(dim=0).cpu()\n",
    "    if ((i+1) % 10 == 0): # Check here so first check is model w/o change\n",
    "        # self_similarity = torch.cosine_similarity(c, last_encoder, dim=-1).mean().cpu().item()\n",
    "        # Above is wrong, should be similarity between encoder and last encoder\n",
    "        self_similarity = torch.cosine_similarity(autoencoder.encoder, last_encoder, dim=-1).mean().cpu().item()\n",
    "        last_encoder = autoencoder.encoder.clone().detach()\n",
    "        num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # Count number of dead_features are zero\n",
    "            num_dead_features = (time_since_activation >= min(i, 200)).sum().item()\n",
    "        print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Total Loss: {total_loss:.2f} | Reconstruction Loss: {reconstruction_loss:.2f} | L1 Loss: {cfg.l1_alpha*l1_loss:.2f} | l1_alpha: {cfg.l1_alpha:.2e} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "        wandb.log({\n",
    "            'Sparsity': sparsity,\n",
    "            'Dead Features': num_dead_features,\n",
    "            'Total Loss': total_loss.item(),\n",
    "            'Reconstruction Loss': reconstruction_loss.item(),\n",
    "            'L1 Loss': (cfg.l1_alpha*l1_loss).item(),\n",
    "            'l1_alpha': cfg.l1_alpha,\n",
    "            'Tokens': num_tokens_so_far,\n",
    "            'Self Similarity': self_similarity\n",
    "        })\n",
    "        \n",
    "        dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "        \n",
    "        if(num_tokens_so_far > max_num_tokens):\n",
    "            print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "            break\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # resample_period = 10000\n",
    "    # if (i % resample_period == 0):\n",
    "    #     # RESAMPLING\n",
    "    #     with torch.no_grad():\n",
    "    #         # Count number of dead_features are zero\n",
    "    #         num_dead_features = (total_activations == 0).sum().item()\n",
    "    #         print(f\"Dead Features: {num_dead_features}\")\n",
    "            \n",
    "    #     if num_dead_features > 0:\n",
    "    #         print(\"Resampling!\")\n",
    "    #         # hyperparams:\n",
    "    #         max_resample_tokens = 1000 # the number of token activations that we consider for inserting into the dictionary\n",
    "    #         # compute loss of model on random subset of inputs\n",
    "    #         resample_loader = setup_token_data(cfg, tokenizer, model, seed=i)\n",
    "    #         num_resample_data = 0\n",
    "\n",
    "    #         resample_activations = torch.empty(0, activation_size)\n",
    "    #         resample_losses = torch.empty(0)\n",
    "\n",
    "    #         for resample_batch in resample_loader:\n",
    "    #             resample_tokens = resample_batch[\"input_ids\"].to(cfg.device)\n",
    "    #             with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "    #                 with Trace(model, tensor_names[0]) as ret:\n",
    "    #                     _ = model(resample_tokens)\n",
    "    #                     representation = ret.output\n",
    "    #                     if(isinstance(representation, tuple)):\n",
    "    #                         representation = representation[0]\n",
    "    #             layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    #             resample_activations = torch.cat((resample_activations, layer_activations.detach().cpu()), dim=0)\n",
    "\n",
    "    #             c = autoencoder.encode(layer_activations)\n",
    "    #             x_hat = autoencoder.decode(c)\n",
    "                \n",
    "    #             reconstruction_loss = (x_hat - layer_activations).pow(2).mean(dim=-1)\n",
    "    #             l1_loss = torch.norm(c, 1, dim=-1)\n",
    "    #             temp_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "                \n",
    "    #             resample_losses = torch.cat((resample_losses, temp_loss.detach().cpu()), dim=0)\n",
    "                \n",
    "    #             num_resample_data +=layer_activations.shape[0]\n",
    "    #             if num_resample_data > max_resample_tokens:\n",
    "    #                 break\n",
    "\n",
    "                \n",
    "    #         # sample num_dead_features vectors of input activations\n",
    "    #         probabilities = resample_losses**2\n",
    "    #         probabilities /= probabilities.sum()\n",
    "    #         sampled_indices = torch.multinomial(probabilities, num_dead_features, replacement=True)\n",
    "    #         new_vectors = resample_activations[sampled_indices]\n",
    "\n",
    "    #         # calculate average encoder norm of alive neurons\n",
    "    #         alive_neurons = list((total_activations!=0))\n",
    "    #         modified_columns = total_activations==0\n",
    "    #         avg_norm = autoencoder.encoder.data[alive_neurons].norm(dim=-1).mean()\n",
    "\n",
    "    #         # replace dictionary and encoder weights with vectors\n",
    "    #         new_vectors = new_vectors / new_vectors.norm(dim=1, keepdim=True)\n",
    "            \n",
    "    #         params_to_modify = [autoencoder.encoder, autoencoder.encoder_bias]\n",
    "\n",
    "    #         current_weights = autoencoder.encoder.data\n",
    "    #         current_weights[modified_columns] = (new_vectors.to(cfg.device) * avg_norm * 0.02)\n",
    "    #         autoencoder.encoder.data = current_weights\n",
    "\n",
    "    #         current_weights = autoencoder.encoder_bias.data\n",
    "    #         current_weights[modified_columns] = 0\n",
    "    #         autoencoder.encoder_bias.data = current_weights\n",
    "            \n",
    "    #         if hasattr(autoencoder, 'decoder'):\n",
    "    #             current_weights = autoencoder.decoder.data\n",
    "    #             current_weights[modified_columns] = new_vectors.to(cfg.device)\n",
    "    #             autoencoder.decoder.data = current_weights\n",
    "    #             params_to_modify += [autoencoder.decoder]\n",
    "\n",
    "    #         for param_group in optimizer.param_groups:\n",
    "    #             for param in param_group['params']:\n",
    "    #                 if any(param is d_ for d_ in params_to_modify):\n",
    "    #                     # Extract the corresponding rows from m and v\n",
    "    #                     m = optimizer.state[param]['exp_avg']\n",
    "    #                     v = optimizer.state[param]['exp_avg_sq']\n",
    "                        \n",
    "    #                     # Update the m and v values for the modified columns\n",
    "    #                     m[modified_columns] = 0  # Reset moving average for modified columns\n",
    "    #                     v[modified_columns] = 0  # Reset squared moving average for modified columns\n",
    "        \n",
    "    #     total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "    if ((i+2) % 10_000==0): # save periodically but before big changes\n",
    "        model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "        save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}_ckpt{num_saved_so_far}\"  # trim year\n",
    "\n",
    "        # Make directory traiend_models if it doesn't exist\n",
    "        import os\n",
    "        if not os.path.exists(\"trained_models\"):\n",
    "            os.makedirs(\"trained_models\")\n",
    "        # Save model\n",
    "        torch.save(autoencoder, f\"trained_models/{save_name}.pt\")\n",
    "        \n",
    "        num_saved_so_far += 1\n",
    "\n",
    "    # Running sparsity check\n",
    "    num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    if(num_tokens_so_far > 200000):\n",
    "        if(i % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            if sparsity > target_upper_sparsity:\n",
    "                cfg.l1_alpha *= (1 + adjustment_factor)\n",
    "            elif sparsity < target_lower_sparsity:\n",
    "                cfg.l1_alpha *= (1 - adjustment_factor)\n",
    "            # print(f\"Sparsity: {sparsity:.1f} | l1_alpha: {cfg.l1_alpha:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}\"  # trim year\n",
    "\n",
    "# Make directory traiend_models if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"trained_models\"):\n",
    "    os.makedirs(\"trained_models\")\n",
    "# Save model\n",
    "torch.save(autoencoder, f\"trained_models/{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>▁▁▁▁▁▂▃▂█▅▃▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>L1 Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Reconstruction Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Self Similarity</td><td>█▁▃▃▃▃▂▄▂▄▅▄▄▅▅▅▅▄▂▅▅▅▅▄▄▅▄▅▅▅▄▅▅▅▅▄▅▅▄▅</td></tr><tr><td>Sparsity</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Total Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>l1_alpha</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>0</td></tr><tr><td>L1 Loss</td><td>0.04268</td></tr><tr><td>Reconstruction Loss</td><td>0.04672</td></tr><tr><td>Self Similarity</td><td>0.97426</td></tr><tr><td>Sparsity</td><td>139.97461</td></tr><tr><td>Tokens</td><td>50688000</td></tr><tr><td>Total Loss</td><td>0.08939</td></tr><tr><td>l1_alpha</td><td>0.001</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EleutherAI/pythia-70m-deduped_1011-014428_51</strong> at: <a href='https://wandb.ai/benw8888/sparse%20coding/runs/jwjmogoe' target=\"_blank\">https://wandb.ai/benw8888/sparse%20coding/runs/jwjmogoe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231011_014746-jwjmogoe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
