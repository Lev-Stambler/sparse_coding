{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from standard_metrics import run_with_model_intervention, perplexity_under_reconstruction, mean_nonzero_activations\n",
    "# Create \n",
    "# # make an argument parser directly below\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--model_name\", type=str, default=\"EleutherAI/pythia-70m-deduped\")\n",
    "# parser.add_argument(\"--layer\", type=int, default=4)\n",
    "# parser.add_argument(\"--setting\", type=str, default=\"residual\")\n",
    "# parser.add_argument(\"--l1_alpha\", type=float, default=3e-3)\n",
    "# parser.add_argument(\"--num_epochs\", type=int, default=10)\n",
    "# parser.add_argument(\"--model_batch_size\", type=int, default=4)\n",
    "# parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "# parser.add_argument(\"--kl\", type=bool, default=False)\n",
    "# parser.add_argument(\"--reconstruction\", type=bool, default=False)\n",
    "# parser.add_argument(\"--dataset_name\", type=str, default=\"NeelNanda/pile-10k\")\n",
    "# parser.add_argument(\"--device\", type=str, default=\"cuda:4\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "cfg = dotdict()\n",
    "cfg.model_name=\"EleutherAI/pythia-70m-deduped\"\n",
    "cfg.layers=[4]\n",
    "cfg.setting=\"residual\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "cfg.l1_alpha=1e-3\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=4\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "# cfg.device=\"cuda:0\"\n",
    "cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name).to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bf32bd9ecc05cbbb_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "cfg.max_length = 256\n",
    "cfg.model_batch_size = 4\n",
    "token_loader = setup_token_data(cfg, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation size: 512\n"
     ]
    }
   ],
   "source": [
    "# Run 1 datapoint on model\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE\n",
    "import torch.nn as nn\n",
    "ratio = 4\n",
    "params = dict()\n",
    "n_dict_components = activation_size*ratio\n",
    "params[\"encoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"encoder\"])\n",
    "\n",
    "params[\"encoder_bias\"] = torch.empty((n_dict_components,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"encoder_bias\"])\n",
    "\n",
    "autoencoder = TiedSAE(\n",
    "    # n_feats = n_dict_components, \n",
    "    # activation_size=activation_size,\n",
    "    encoder=params[\"encoder\"],\n",
    "    encoder_bias=params[\"encoder_bias\"],\n",
    ")\n",
    "autoencoder.to_device(cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/mchorse/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run_name: EleutherAI/pythia-70m-deduped_0929-205548_51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mchorse/logdog/sparse_coding/wandb/run-20230929_205548-2vm7jsly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sparse_coding/sparse%20coding/runs/2vm7jsly' target=\"_blank\">EleutherAI/pythia-70m-deduped_0929-205548_51</a></strong> to <a href='https://wandb.ai/sparse_coding/sparse%20coding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sparse_coding/sparse%20coding' target=\"_blank\">https://wandb.ai/sparse_coding/sparse%20coding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sparse_coding/sparse%20coding/runs/2vm7jsly' target=\"_blank\">https://wandb.ai/sparse_coding/sparse%20coding/runs/2vm7jsly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sparse_coding/sparse%20coding/runs/2vm7jsly?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f410e963a00>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam([autoencoder.encoder, autoencoder.encoder_bias], lr=1e-3)\n",
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"{cfg.model_name}_{start_time[4:]}_{cfg.sparsity}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")\n",
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name, entity=\"sparse_coding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1029.8 | Dead Features: 0 | Total Loss: 0.93 | Reconstruction Loss: 0.56 | L1 Loss: 0.37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mchorse/logdog/sparse_coding/easy_training_code.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231362e3135332e35322e3132222c2275736572223a226d63686f727365227d/home/mchorse/logdog/sparse_coding/easy_training_code.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m layer_activations \u001b[39m=\u001b[39m rearrange(representation, \u001b[39m\"\u001b[39m\u001b[39mb seq d_model -> (b seq) d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231362e3135332e35322e3132222c2275736572223a226d63686f727365227d/home/mchorse/logdog/sparse_coding/easy_training_code.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m c \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39mencode(layer_activations)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231362e3135332e35322e3132222c2275736572223a226d63686f727365227d/home/mchorse/logdog/sparse_coding/easy_training_code.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m x_hat \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39;49mdecode(c)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231362e3135332e35322e3132222c2275736572223a226d63686f727365227d/home/mchorse/logdog/sparse_coding/easy_training_code.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m reconstruction_loss \u001b[39m=\u001b[39m (x_hat \u001b[39m-\u001b[39m layer_activations)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a223231362e3135332e35322e3132222c2275736572223a226d63686f727365227d/home/mchorse/logdog/sparse_coding/easy_training_code.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m l1_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(c, \u001b[39m1\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/logdog/sparse_coding/autoencoders/learned_dict.py:31\u001b[0m, in \u001b[0;36mLearnedDict.decode\u001b[0;34m(self, code)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, code: TensorType[\u001b[39m\"\u001b[39m\u001b[39m_batch_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_n_dict_components\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorType[\u001b[39m\"\u001b[39m\u001b[39m_batch_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_activation_size\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     30\u001b[0m     learned_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_learned_dict()\n\u001b[0;32m---> 31\u001b[0m     x_hat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m\"\u001b[39;49m\u001b[39mnd,bn->bd\u001b[39;49m\u001b[39m\"\u001b[39;49m, learned_dict, code)\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m x_hat\n",
      "File \u001b[0;32m~/logdog/logan/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "for i, batch in enumerate(token_loader):\n",
    "    if(i > 250000):\n",
    "        break\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "    layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "\n",
    "    c = autoencoder.encode(layer_activations)\n",
    "    x_hat = autoencoder.decode(c)\n",
    "    \n",
    "    reconstruction_loss = (x_hat - layer_activations).pow(2).mean()\n",
    "    l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "    total_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "\n",
    "    dead_features += c.sum(dim=0).cpu()\n",
    "    if (i % 500 == 0): # Check here so first check is model w/o change\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # Count number of dead_features are zero\n",
    "            num_dead_features = (dead_features == 0).sum().item()\n",
    "        print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Total Loss: {total_loss:.2f} | Reconstruction Loss: {reconstruction_loss:.2f} | L1 Loss: {cfg.l1_alpha*l1_loss:.2f}\")\n",
    "        wandb.log({\n",
    "            'Sparsity': sparsity,\n",
    "            'Dead Features': num_dead_features,\n",
    "            'Total Loss': total_loss.item(),\n",
    "            'Reconstruction Loss': reconstruction_loss.item(),\n",
    "            'L1 Loss': (cfg.l1_alpha*l1_loss).item(),\n",
    "        })\n",
    "        dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Running sparsity check\n",
    "    if(i % 10 == 0):\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "        if(i == 0):\n",
    "            target_lower_sparsity = sparsity - 5.0\n",
    "            target_upper_sparsity = sparsity + 5.0\n",
    "            adjustment_factor = 0.1  # You can set this to whatever you like\n",
    "        if sparsity > target_upper_sparsity:\n",
    "            cfg.l1_alpha *= (1 + adjustment_factor)\n",
    "        elif sparsity < target_lower_sparsity:\n",
    "            cfg.l1_alpha *= (1 - adjustment_factor)\n",
    "        # print(f\"Sparsity: {sparsity:.1f} | l1_alpha: {cfg.l1_alpha:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{ratio}_{tensor_names[0]}\"  # trim year\n",
    "\n",
    "# Make directory traiend_models if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"trained_models\"):\n",
    "    os.makedirs(\"trained_models\")\n",
    "# Save model\n",
    "torch.save(autoencoder, f\"trained_models/{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'EleutherAI/pythia-70m-deduped',\n",
       " 'layers': [4],\n",
       " 'setting': 'residual',\n",
       " 'tensor_name': 'gpt_neox.layers.{layer}',\n",
       " 'l1_alpha': 0.001,\n",
       " 'sparsity': 51,\n",
       " 'num_epochs': 10,\n",
       " 'model_batch_size': 4,\n",
       " 'lr': 0.001,\n",
       " 'kl': False,\n",
       " 'reconstruction': False,\n",
       " 'dataset_name': 'NeelNanda/pile-10k',\n",
       " 'device': 'cpu'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
