{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "# from standard_metrics import run_with_model_intervention, perplexity_under_reconstruction, mean_nonzero_activations\n",
    "# Create \n",
    "# # make an argument parser directly below\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--model_name\", type=str, default=\"EleutherAI/pythia-70m-deduped\")\n",
    "# parser.add_argument(\"--layer\", type=int, default=4)\n",
    "# parser.add_argument(\"--setting\", type=str, default=\"residual\")\n",
    "# parser.add_argument(\"--l1_alpha\", type=float, default=3e-3)\n",
    "# parser.add_argument(\"--num_epochs\", type=int, default=10)\n",
    "# parser.add_argument(\"--model_batch_size\", type=int, default=4)\n",
    "# parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "# parser.add_argument(\"--kl\", type=bool, default=False)\n",
    "# parser.add_argument(\"--reconstruction\", type=bool, default=False)\n",
    "# parser.add_argument(\"--dataset_name\", type=str, default=\"NeelNanda/pile-10k\")\n",
    "# parser.add_argument(\"--device\", type=str, default=\"cuda:4\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "cfg = dotdict()\n",
    "# cfg.model_name=\"EleutherAI/pythia-70m-deduped\"\n",
    "cfg.model_name=\"usvsnsp/pythia-6.9b-rm-full-hh-rlhf\"\n",
    "cfg.layers=[4]\n",
    "cfg.setting=\"residual\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "cfg.l1_alpha=1e-3\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=4\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Skylion007/openwebtext\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 912/912 [00:00<00:00, 4.38MB/s]\n",
      "Downloading (…)model.bin.index.json: 100%|██████████| 36.9k/36.9k [00:00<00:00, 9.10MB/s]\n",
      "Downloading (…)l-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [03:47<00:00, 43.7MB/s]\n",
      "Downloading (…)l-00002-of-00003.bin: 100%|██████████| 9.93G/9.93G [03:48<00:00, 43.5MB/s]\n",
      "Downloading (…)l-00003-of-00003.bin: 100%|██████████| 6.71G/6.71G [02:30<00:00, 44.5MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [10:07<00:00, 202.64s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.10s/it]\n",
      "Some weights of the model checkpoint at usvsnsp/pythia-6.9b-rm-full-hh-rlhf were not used when initializing GPTNeoXForCausalLM: ['score.weight']\n",
      "- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPTNeoXForCausalLM were not initialized from the model checkpoint at usvsnsp/pythia-6.9b-rm-full-hh-rlhf and are newly initialized: ['gpt_neox.layers.9.attention.bias', 'gpt_neox.layers.31.attention.masked_bias', 'gpt_neox.layers.0.attention.masked_bias', 'gpt_neox.layers.8.attention.masked_bias', 'gpt_neox.layers.30.attention.masked_bias', 'gpt_neox.layers.1.attention.masked_bias', 'gpt_neox.layers.9.attention.masked_bias', 'gpt_neox.layers.28.attention.masked_bias', 'gpt_neox.layers.7.attention.bias', 'gpt_neox.layers.7.attention.masked_bias', 'gpt_neox.layers.14.attention.masked_bias', 'gpt_neox.layers.3.attention.masked_bias', 'gpt_neox.layers.6.attention.bias', 'gpt_neox.layers.11.attention.bias', 'gpt_neox.layers.14.attention.bias', 'gpt_neox.layers.25.attention.masked_bias', 'gpt_neox.layers.25.attention.bias', 'gpt_neox.layers.8.attention.bias', 'gpt_neox.layers.29.attention.masked_bias', 'gpt_neox.layers.16.attention.masked_bias', 'gpt_neox.layers.6.attention.masked_bias', 'gpt_neox.layers.23.attention.bias', 'gpt_neox.layers.19.attention.masked_bias', 'gpt_neox.layers.20.attention.masked_bias', 'gpt_neox.layers.26.attention.bias', 'gpt_neox.layers.2.attention.bias', 'gpt_neox.layers.3.attention.bias', 'gpt_neox.layers.12.attention.bias', 'gpt_neox.layers.24.attention.masked_bias', 'gpt_neox.layers.15.attention.masked_bias', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.0.attention.bias', 'gpt_neox.layers.4.attention.masked_bias', 'gpt_neox.layers.17.attention.bias', 'gpt_neox.layers.1.attention.bias', 'gpt_neox.layers.24.attention.bias', 'gpt_neox.layers.20.attention.bias', 'gpt_neox.layers.27.attention.bias', 'gpt_neox.layers.17.attention.masked_bias', 'gpt_neox.layers.30.attention.bias', 'gpt_neox.layers.13.attention.bias', 'gpt_neox.layers.22.attention.masked_bias', 'gpt_neox.layers.26.attention.masked_bias', 'gpt_neox.layers.16.attention.bias', 'gpt_neox.layers.27.attention.masked_bias', 'gpt_neox.layers.18.attention.bias', 'gpt_neox.layers.19.attention.bias', 'embed_out.weight', 'gpt_neox.layers.10.attention.bias', 'gpt_neox.layers.21.attention.masked_bias', 'gpt_neox.layers.21.attention.bias', 'gpt_neox.layers.29.attention.bias', 'gpt_neox.layers.18.attention.masked_bias', 'gpt_neox.layers.2.attention.masked_bias', 'gpt_neox.layers.31.attention.bias', 'gpt_neox.layers.22.attention.bias', 'gpt_neox.layers.15.attention.bias', 'gpt_neox.layers.4.attention.bias', 'gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.28.attention.bias', 'gpt_neox.layers.12.attention.masked_bias', 'gpt_neox.layers.23.attention.masked_bias', 'gpt_neox.layers.10.attention.masked_bias', 'gpt_neox.layers.11.attention.masked_bias', 'gpt_neox.layers.13.attention.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 264/264 [00:00<00:00, 1.08MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 14.6MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 442kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name).to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 2.73k/2.73k [00:00<00:00, 10.7MB/s]\n",
      "Downloading readme: 100%|██████████| 7.33k/7.33k [00:00<00:00, 20.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset openwebtext/plain_text to /root/.cache/huggingface/datasets/Skylion007___openwebtext/plain_text/1.0.0/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 633M/633M [00:07<00:00, 87.0MB/s]\n",
      "Downloading data: 100%|██████████| 629M/629M [00:06<00:00, 100MB/s]it]\n",
      "Downloading data: 100%|██████████| 629M/629M [00:06<00:00, 90.0MB/s]t]\n",
      "Downloading data: 100%|██████████| 628M/628M [00:08<00:00, 76.6MB/s]t]\n",
      "Downloading data: 100%|██████████| 627M/627M [00:06<00:00, 97.2MB/s]t]\n",
      "Downloading data: 100%|██████████| 630M/630M [00:10<00:00, 61.9MB/s]t]\n",
      "Downloading data: 100%|██████████| 626M/626M [00:10<00:00, 58.0MB/s]t]\n",
      "Downloading data: 100%|██████████| 625M/625M [00:14<00:00, 44.6MB/s]t]\n",
      "Downloading data: 100%|██████████| 625M/625M [00:12<00:00, 51.0MB/s]t]\n",
      "Downloading data: 100%|██████████| 626M/626M [00:10<00:00, 58.7MB/s]t]\n",
      "Downloading data: 100%|██████████| 625M/625M [00:11<00:00, 52.9MB/s]it]\n",
      "Downloading data: 100%|██████████| 625M/625M [00:06<00:00, 95.7MB/s]it]\n",
      "Downloading data: 100%|██████████| 624M/624M [00:10<00:00, 59.0MB/s]it]\n",
      "Downloading data: 100%|██████████| 629M/629M [00:07<00:00, 89.4MB/s]it]\n",
      "Downloading data: 100%|██████████| 627M/627M [00:10<00:00, 58.4MB/s]it]\n",
      "Downloading data: 100%|██████████| 621M/621M [00:06<00:00, 92.4MB/s]it]\n",
      "Downloading data: 100%|██████████| 619M/619M [00:06<00:00, 89.1MB/s]it]\n",
      "Downloading data: 100%|██████████| 619M/619M [00:07<00:00, 87.2MB/s]it]\n",
      "Downloading data: 100%|██████████| 618M/618M [00:06<00:00, 88.3MB/s]it]\n",
      "Downloading data: 100%|██████████| 619M/619M [00:06<00:00, 94.0MB/s]it]\n",
      "Downloading data: 100%|██████████| 377M/377M [00:06<00:00, 59.1MB/s]it]\n",
      "Downloading data files: 100%|██████████| 21/21 [03:10<00:00,  9.07s/it]\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset openwebtext downloaded and prepared to /root/.cache/huggingface/datasets/Skylion007___openwebtext/plain_text/1.0.0/6f68e85c16ccc770c0dd489f4008852ea9633604995addd0cd76e293aed9e521. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/datasets/utils/py_utils.py:1373\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1373\u001b[0m     \u001b[39myield\u001b[39;00m queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m)\n\u001b[1;32m   1374\u001b[0m \u001b[39mexcept\u001b[39;00m Empty:\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/multiprocess/managers.py:818\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m conn\u001b[39m.\u001b[39msend((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 818\u001b[0m kind, result \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mrecv()\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m kind \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m#RETURN\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/multiprocess/connection.py:258\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 258\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[1;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39mloads(buf\u001b[39m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/multiprocess/connection.py:422\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_recv_bytes\u001b[39m(\u001b[39mself\u001b[39m, maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 422\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv(\u001b[39m4\u001b[39;49m)\n\u001b[1;32m    423\u001b[0m     size, \u001b[39m=\u001b[39m struct\u001b[39m.\u001b[39munpack(\u001b[39m\"\u001b[39m\u001b[39m!i\u001b[39m\u001b[39m\"\u001b[39m, buf\u001b[39m.\u001b[39mgetvalue())\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/multiprocess/connection.py:387\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mwhile\u001b[39;00m remaining \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     chunk \u001b[39m=\u001b[39m read(handle, remaining)\n\u001b[1;32m    388\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/sparse_coding/easy_training_code.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B50.217.254.168/root/sparse_coding/easy_training_code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m cfg\u001b[39m.\u001b[39mmax_length \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B50.217.254.168/root/sparse_coding/easy_training_code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m cfg\u001b[39m.\u001b[39mmodel_batch_size \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B50.217.254.168/root/sparse_coding/easy_training_code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m token_loader \u001b[39m=\u001b[39m setup_token_data(cfg, tokenizer, model)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B50.217.254.168/root/sparse_coding/easy_training_code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m num_tokens \u001b[39m=\u001b[39m cfg\u001b[39m.\u001b[39mmax_length\u001b[39m*\u001b[39mcfg\u001b[39m.\u001b[39mmodel_batch_size\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(token_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B50.217.254.168/root/sparse_coding/easy_training_code.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of tokens: \u001b[39m\u001b[39m{\u001b[39;00mnum_tokens\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sparse_coding/activation_dataset.py:465\u001b[0m, in \u001b[0;36msetup_token_data\u001b[0;34m(cfg, tokenizer, model)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_token_data\u001b[39m(cfg, tokenizer, model):\n\u001b[1;32m    464\u001b[0m     sentence_dataset \u001b[39m=\u001b[39m make_sentence_dataset(cfg\u001b[39m.\u001b[39mdataset_name)\n\u001b[0;32m--> 465\u001b[0m     tokenized_sentence_dataset, bits_per_byte \u001b[39m=\u001b[39m chunk_and_tokenize(sentence_dataset, tokenizer, max_length\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mmax_length)\n\u001b[1;32m    466\u001b[0m     token_loader \u001b[39m=\u001b[39m DataLoader(tokenized_sentence_dataset, batch_size\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mmodel_batch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m token_loader\n",
      "File \u001b[0;32m~/sparse_coding/activation_dataset.py:225\u001b[0m, in \u001b[0;36mchunk_and_tokenize\u001b[0;34m(data, tokenizer, format, num_proc, text_key, max_length, return_final_batch, load_from_cache_file)\u001b[0m\n\u001b[1;32m    221\u001b[0m     output[\u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rem\n\u001b[1;32m    223\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[0;32m--> 225\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    226\u001b[0m     _tokenize_fn,\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Batching is important for ensuring that we don't waste tokens\u001b[39;49;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# since we always throw away the last element of the batch we\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m# want to keep the batch size as large as possible\u001b[39;49;00m\n\u001b[1;32m    230\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    231\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m,\n\u001b[1;32m    232\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    233\u001b[0m     remove_columns\u001b[39m=\u001b[39;49mget_columns_all_equal(data),\n\u001b[1;32m    234\u001b[0m     load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    236\u001b[0m total_bytes: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mbytes\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    237\u001b[0m total_tokens: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/datasets/arrow_dataset.py:3166\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3158\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3159\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3160\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3161\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3164\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3165\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3166\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3167\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3168\u001b[0m     ):\n\u001b[1;32m   3169\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3170\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/datasets/utils/py_utils.py:1379\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1378\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     [async_result\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/datasets/utils/py_utils.py:1379\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1378\u001b[0m     \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/logan/lib/python3.10/site-packages/multiprocess/pool.py:770\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[0;32m--> 770\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_success:\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "cfg.max_length = 256\n",
    "cfg.model_batch_size = 4\n",
    "token_loader = setup_token_data(cfg, tokenizer, model)\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation size: 512\n"
     ]
    }
   ],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE\n",
    "from torch import nn\n",
    "params = dict()\n",
    "n_dict_components = activation_size*cfg.ratio\n",
    "params[\"encoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"encoder\"])\n",
    "\n",
    "params[\"encoder_bias\"] = torch.empty((n_dict_components,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"encoder_bias\"])\n",
    "\n",
    "autoencoder = TiedSAE(\n",
    "    # n_feats = n_dict_components, \n",
    "    # activation_size=activation_size,\n",
    "    encoder=params[\"encoder\"],\n",
    "    encoder_bias=params[\"encoder_bias\"],\n",
    ")\n",
    "autoencoder.to_device(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sparsity: 51\n"
     ]
    }
   ],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.1)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity - 5.0\n",
    "target_upper_sparsity = cfg.sparsity + 5.0\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Only works on tied SAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melriggs\u001b[0m (\u001b[33msparse_coding\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run_name: EleutherAI/pythia-70m-deduped_1003-153306_51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sparse_coding/wandb/run-20231003_153306-7stl9owp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sparse_coding/sparse%20coding/runs/7stl9owp' target=\"_blank\">EleutherAI/pythia-70m-deduped_1003-153306_51</a></strong> to <a href='https://wandb.ai/sparse_coding/sparse%20coding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sparse_coding/sparse%20coding' target=\"_blank\">https://wandb.ai/sparse_coding/sparse%20coding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sparse_coding/sparse%20coding/runs/7stl9owp' target=\"_blank\">https://wandb.ai/sparse_coding/sparse%20coding/runs/7stl9owp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sparse_coding/sparse%20coding/runs/7stl9owp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f68506ca0b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"WARNING: Only works on tied SAE\")\n",
    "# Set gradient to true for encoder & bias\n",
    "autoencoder.encoder.requires_grad = True\n",
    "autoencoder.encoder_bias.requires_grad = True\n",
    "optimizer = torch.optim.Adam([autoencoder.encoder, autoencoder.encoder_bias], lr=1e-3)\n",
    "original_bias = autoencoder.encoder_bias.clone().detach()\n",
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"{cfg.model_name}_{start_time[4:]}_{cfg.sparsity}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")\n",
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name, entity=\"sparse_coding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 1025.0 | Dead Features: 0 | Total Loss: 0.90 | Reconstruction Loss: 0.53 | L1 Loss: 0.37 | l1_alpha: 1.00e-03 | Tokens: 0 | Self Similarity: 1.00\n",
      "Sparsity: 321.5 | Dead Features: 0 | Total Loss: 0.16 | Reconstruction Loss: 0.08 | L1 Loss: 0.08 | l1_alpha: 1.00e-03 | Tokens: 512000 | Self Similarity: 0.83\n",
      "Sparsity: 270.2 | Dead Features: 0 | Total Loss: 0.13 | Reconstruction Loss: 0.06 | L1 Loss: 0.07 | l1_alpha: 1.00e-03 | Tokens: 1024000 | Self Similarity: 0.87\n",
      "Sparsity: 253.7 | Dead Features: 1 | Total Loss: 0.12 | Reconstruction Loss: 0.05 | L1 Loss: 0.07 | l1_alpha: 1.00e-03 | Tokens: 1536000 | Self Similarity: 0.90\n",
      "Sparsity: 222.1 | Dead Features: 0 | Total Loss: 0.11 | Reconstruction Loss: 0.05 | L1 Loss: 0.06 | l1_alpha: 1.00e-03 | Tokens: 2048000 | Self Similarity: 0.92\n",
      "Sparsity: 237.8 | Dead Features: 1 | Total Loss: 0.12 | Reconstruction Loss: 0.06 | L1 Loss: 0.06 | l1_alpha: 1.00e-03 | Tokens: 2560000 | Self Similarity: 0.93\n",
      "Sparsity: 211.0 | Dead Features: 1 | Total Loss: 0.11 | Reconstruction Loss: 0.05 | L1 Loss: 0.06 | l1_alpha: 1.00e-03 | Tokens: 3072000 | Self Similarity: 0.94\n",
      "Sparsity: 214.7 | Dead Features: 0 | Total Loss: 0.12 | Reconstruction Loss: 0.05 | L1 Loss: 0.06 | l1_alpha: 1.00e-03 | Tokens: 3584000 | Self Similarity: 0.95\n",
      "Sparsity: 195.7 | Dead Features: 1 | Total Loss: 0.11 | Reconstruction Loss: 0.05 | L1 Loss: 0.06 | l1_alpha: 1.00e-03 | Tokens: 4096000 | Self Similarity: 0.96\n",
      "Sparsity: 187.4 | Dead Features: 0 | Total Loss: 0.11 | Reconstruction Loss: 0.05 | L1 Loss: 0.06 | l1_alpha: 1.00e-03 | Tokens: 4608000 | Self Similarity: 0.95\n",
      "Sparsity: 178.0 | Dead Features: 0 | Total Loss: 0.11 | Reconstruction Loss: 0.05 | L1 Loss: 0.06 | l1_alpha: 1.00e-03 | Tokens: 5120000 | Self Similarity: 0.96\n",
      "Sparsity: 134.0 | Dead Features: 0 | Total Loss: 0.12 | Reconstruction Loss: 0.06 | L1 Loss: 0.07 | l1_alpha: 1.33e-03 | Tokens: 5632000 | Self Similarity: 0.96\n",
      "Sparsity: 97.5 | Dead Features: 0 | Total Loss: 0.14 | Reconstruction Loss: 0.07 | L1 Loss: 0.07 | l1_alpha: 1.61e-03 | Tokens: 6144000 | Self Similarity: 0.95\n",
      "Sparsity: 74.6 | Dead Features: 0 | Total Loss: 0.16 | Reconstruction Loss: 0.08 | L1 Loss: 0.08 | l1_alpha: 2.14e-03 | Tokens: 6656000 | Self Similarity: 0.94\n",
      "Sparsity: 46.8 | Dead Features: 0 | Total Loss: 0.17 | Reconstruction Loss: 0.10 | L1 Loss: 0.07 | l1_alpha: 2.59e-03 | Tokens: 7168000 | Self Similarity: 0.93\n",
      "Sparsity: 54.8 | Dead Features: 0 | Total Loss: 0.19 | Reconstruction Loss: 0.10 | L1 Loss: 0.09 | l1_alpha: 2.59e-03 | Tokens: 7680000 | Self Similarity: 0.94\n",
      "Sparsity: 53.9 | Dead Features: 0 | Total Loss: 0.17 | Reconstruction Loss: 0.09 | L1 Loss: 0.08 | l1_alpha: 2.33e-03 | Tokens: 8192000 | Self Similarity: 0.94\n",
      "Sparsity: 52.3 | Dead Features: 0 | Total Loss: 0.17 | Reconstruction Loss: 0.10 | L1 Loss: 0.08 | l1_alpha: 2.33e-03 | Tokens: 8704000 | Self Similarity: 0.95\n",
      "Sparsity: 49.7 | Dead Features: 0 | Total Loss: 0.16 | Reconstruction Loss: 0.08 | L1 Loss: 0.08 | l1_alpha: 2.33e-03 | Tokens: 9216000 | Self Similarity: 0.95\n",
      "Sparsity: 55.7 | Dead Features: 0 | Total Loss: 0.16 | Reconstruction Loss: 0.08 | L1 Loss: 0.07 | l1_alpha: 2.10e-03 | Tokens: 9728000 | Self Similarity: 0.95\n",
      "Sparsity: 47.3 | Dead Features: 0 | Total Loss: 0.16 | Reconstruction Loss: 0.09 | L1 Loss: 0.08 | l1_alpha: 2.31e-03 | Tokens: 10240000 | Self Similarity: 0.96\n",
      "Sparsity: 48.6 | Dead Features: 0 | Total Loss: 0.16 | Reconstruction Loss: 0.08 | L1 Loss: 0.08 | l1_alpha: 2.31e-03 | Tokens: 10752000 | Self Similarity: 0.96\n",
      "Sparsity: 48.4 | Dead Features: 1 | Total Loss: 0.15 | Reconstruction Loss: 0.08 | L1 Loss: 0.07 | l1_alpha: 2.08e-03 | Tokens: 11264000 | Self Similarity: 0.96\n",
      "Sparsity: 51.3 | Dead Features: 1 | Total Loss: 0.16 | Reconstruction Loss: 0.09 | L1 Loss: 0.07 | l1_alpha: 2.08e-03 | Tokens: 11776000 | Self Similarity: 0.96\n",
      "Sparsity: 44.2 | Dead Features: 2 | Total Loss: 0.14 | Reconstruction Loss: 0.08 | L1 Loss: 0.07 | l1_alpha: 2.08e-03 | Tokens: 12288000 | Self Similarity: 0.96\n",
      "Sparsity: 51.1 | Dead Features: 0 | Total Loss: 0.14 | Reconstruction Loss: 0.08 | L1 Loss: 0.07 | l1_alpha: 1.87e-03 | Tokens: 12800000 | Self Similarity: 0.96\n",
      "Sparsity: 47.9 | Dead Features: 1 | Total Loss: 0.15 | Reconstruction Loss: 0.08 | L1 Loss: 0.07 | l1_alpha: 2.06e-03 | Tokens: 13312000 | Self Similarity: 0.97\n",
      "Sparsity: 52.7 | Dead Features: 1 | Total Loss: 0.17 | Reconstruction Loss: 0.09 | L1 Loss: 0.07 | l1_alpha: 2.06e-03 | Tokens: 13824000 | Self Similarity: 0.97\n",
      "Sparsity: 54.9 | Dead Features: 1 | Total Loss: 0.16 | Reconstruction Loss: 0.08 | L1 Loss: 0.08 | l1_alpha: 2.06e-03 | Tokens: 14336000 | Self Similarity: 0.96\n",
      "Sparsity: 47.4 | Dead Features: 1 | Total Loss: 0.15 | Reconstruction Loss: 0.09 | L1 Loss: 0.07 | l1_alpha: 2.06e-03 | Tokens: 14848000 | Self Similarity: 0.96\n"
     ]
    }
   ],
   "source": [
    "dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "max_num_tokens = 100000000\n",
    "# Freeze model parameters \n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "last_encoder = autoencoder.encoder.clone().detach()\n",
    "for i, batch in enumerate(token_loader):\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "    with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "        with Trace(model, tensor_names[0]) as ret:\n",
    "            _ = model(tokens)\n",
    "            representation = ret.output\n",
    "            if(isinstance(representation, tuple)):\n",
    "                representation = representation[0]\n",
    "    layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "\n",
    "    c = autoencoder.encode(layer_activations)\n",
    "    x_hat = autoencoder.decode(c)\n",
    "    \n",
    "    reconstruction_loss = (x_hat - layer_activations).pow(2).mean()\n",
    "    l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "    total_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "\n",
    "    dead_features += c.sum(dim=0).cpu()\n",
    "    if (i % 500 == 0): # Check here so first check is model w/o change\n",
    "        # self_similarity = torch.cosine_similarity(c, last_encoder, dim=-1).mean().cpu().item()\n",
    "        # Above is wrong, should be similarity between encoder and last encoder\n",
    "        self_similarity = torch.cosine_similarity(autoencoder.encoder, last_encoder, dim=-1).mean().cpu().item()\n",
    "        last_encoder = autoencoder.encoder.clone().detach()\n",
    "        num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # Count number of dead_features are zero\n",
    "            num_dead_features = (dead_features == 0).sum().item()\n",
    "        print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Total Loss: {total_loss:.2f} | Reconstruction Loss: {reconstruction_loss:.2f} | L1 Loss: {cfg.l1_alpha*l1_loss:.2f} | l1_alpha: {cfg.l1_alpha:.2e} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "        wandb.log({\n",
    "            'Sparsity': sparsity,\n",
    "            'Dead Features': num_dead_features,\n",
    "            'Total Loss': total_loss.item(),\n",
    "            'Reconstruction Loss': reconstruction_loss.item(),\n",
    "            'L1 Loss': (cfg.l1_alpha*l1_loss).item(),\n",
    "            'l1_alpha': cfg.l1_alpha,\n",
    "            'Tokens': num_tokens_so_far,\n",
    "            'Self Similarity': self_similarity\n",
    "        })\n",
    "        dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "        \n",
    "        if(num_tokens_so_far > max_num_tokens):\n",
    "            print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "            break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Running sparsity check\n",
    "    if(num_tokens_so_far > 5000000):\n",
    "        if(i % 200 == 0):\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            if sparsity > target_upper_sparsity:\n",
    "                cfg.l1_alpha *= (1 + adjustment_factor)\n",
    "            elif sparsity < target_lower_sparsity:\n",
    "                cfg.l1_alpha *= (1 - adjustment_factor)\n",
    "            # print(f\"Sparsity: {sparsity:.1f} | l1_alpha: {cfg.l1_alpha:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>▁▁▁▅▁▅▅▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅█▁▅▅▅▅</td></tr><tr><td>L1 Loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Reconstruction Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁▂▁▁▂▁▁▁▂▁▁</td></tr><tr><td>Self Similarity</td><td>█▁▃▄▅▅▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇</td></tr><tr><td>Sparsity</td><td>█▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>Total Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>l1_alpha</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▄▆██▇▇▇▆▇▇▆▆▆▅▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Dead Features</td><td>1</td></tr><tr><td>L1 Loss</td><td>0.06866</td></tr><tr><td>Reconstruction Loss</td><td>0.08504</td></tr><tr><td>Self Similarity</td><td>0.96401</td></tr><tr><td>Sparsity</td><td>47.42188</td></tr><tr><td>Tokens</td><td>14848000</td></tr><tr><td>Total Loss</td><td>0.1537</td></tr><tr><td>l1_alpha</td><td>0.00206</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EleutherAI/pythia-70m-deduped_1003-153306_51</strong> at: <a href='https://wandb.ai/sparse_coding/sparse%20coding/runs/7stl9owp' target=\"_blank\">https://wandb.ai/sparse_coding/sparse%20coding/runs/7stl9owp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231003_153306-7stl9owp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}\"  # trim year\n",
    "\n",
    "# Make directory traiend_models if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"trained_models\"):\n",
    "    os.makedirs(\"trained_models\")\n",
    "# Save model\n",
    "torch.save(autoencoder, f\"trained_models/{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'EleutherAI/pythia-70m-deduped',\n",
       " 'layers': [4],\n",
       " 'setting': 'residual',\n",
       " 'tensor_name': 'gpt_neox.layers.{layer}',\n",
       " 'l1_alpha': 0.0020591228579666505,\n",
       " 'sparsity': 51,\n",
       " 'num_epochs': 10,\n",
       " 'model_batch_size': 4,\n",
       " 'lr': 0.001,\n",
       " 'kl': False,\n",
       " 'reconstruction': False,\n",
       " 'dataset_name': 'NeelNanda/pile-10k',\n",
       " 'device': 'cuda:0',\n",
       " 'ratio': 4,\n",
       " 'max_length': 256}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
