{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-70m-deduped\", \"usvsnsp/pythia-6.9b-ppo\", \"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "cfg.model_name=\"EleutherAI/pythia-6.9b\"\n",
    "cfg.target_name=\"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "cfg.layers=[10]\n",
    "cfg.setting=\"residual\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "cfg.l1_alpha=original_l1_alpha\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=8\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Elriggs/openwebtext-100k\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "cfg.seed = 0\n",
    "# cfg.device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = [cfg.tensor_name.format(layer=layer) for layer in cfg.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdbce8bfdd641e6a850d73fca910212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b12f9e1d074b8da18fd0d2dae95638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/42.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16424190dd364e51912397911107d420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf19a54aa464385aea5767e947824d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ac2307c532476fb00caf1a3432ff62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3651b5c5c4764f99946319043dbafb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269e96de48434a358dfee5caa2a9f111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39079af1d7b74343af4efc776e0b153b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e028d5c5fc2b4e8591dc7d755279679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
    "model = model.to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970303a2a6fa4ad2abaa6d20aa3be806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 112750592\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "cfg.max_length = 256\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed, split=\"train\")\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation size: 4096\n"
     ]
    }
   ],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sparsity: 204\n"
     ]
    }
   ],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fe451efebb4582b542c5f58252690d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26500b0efa0415594ba6d15505429a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/13.8G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder loaded from fbase_sae_6b\n",
      "target_autoencoder loaded from fsft_sae_6b\n"
     ]
    }
   ],
   "source": [
    "# Load base and target autoencoders\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(cfg.target_name).cpu()\n",
    "\n",
    "save_name = f\"base_sae_6b\"  # trim year\n",
    "autoencoder = torch.load(f\"trained_models/{save_name}.pt\")\n",
    "print(f\"autoencoder loaded from f{save_name}\")\n",
    "autoencoder.to_device(cfg.device)\n",
    "\n",
    "save_name = f\"sft_sae_6b\" \n",
    "target_autoencoder = torch.load(f\"trained_models/{save_name}.pt\")\n",
    "print(f\"target_autoencoder loaded from f{save_name}\")\n",
    "target_autoencoder.to_device(cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New transfer autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE, TransferSAE\n",
    "from torch import nn\n",
    "\n",
    "modes = [\"scale\", \"rotation\", \"bias\", \"free\"]\n",
    "transfer_autoencoders = []\n",
    "for mode in modes:\n",
    "    # mode_tsae = TransferSAE(\n",
    "    #     # n_feats = n_dict_components, \n",
    "    #     # activation_size=activation_size,\n",
    "    #     autoencoder,\n",
    "    #     decoder=autoencoder.get_learned_dict().detach().clone(),\n",
    "    #     decoder_bias=autoencoder.shift_bias.detach().clone(),\n",
    "    #     mode=mode,\n",
    "    # )\n",
    "    mode_tsae = torch.load(f\"trained_models/transfer_base_sft_6b_{mode}.pt\")\n",
    "    mode_tsae.to_device(cfg.device)\n",
    "    transfer_autoencoders.append(mode_tsae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_run_name: testing_lomahony/eleuther-pythia6.9b-hh-sft_transfer_1026-221948\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sparse_coding/wandb/run-20231026_221948-r8w8anl2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/benw8888/sparse%20coding/runs/r8w8anl2' target=\"_blank\">testing_lomahony/eleuther-pythia6.9b-hh-sft_transfer_1026-221948</a></strong> to <a href='https://wandb.ai/benw8888/sparse%20coding' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/benw8888/sparse%20coding' target=\"_blank\">https://wandb.ai/benw8888/sparse%20coding</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/benw8888/sparse%20coding/runs/r8w8anl2' target=\"_blank\">https://wandb.ai/benw8888/sparse%20coding/runs/r8w8anl2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/benw8888/sparse%20coding/runs/r8w8anl2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff7e931b010>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"testing_{cfg.target_name}_transfer_{start_time[4:]}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")\n",
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activations(model, inputs):\n",
    "    acts = []\n",
    "    for tokens in inputs:\n",
    "        with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "            with Trace(model, tensor_names[0]) as ret:\n",
    "                _ = model(tokens)\n",
    "                representation = ret.output\n",
    "                if(isinstance(representation, tuple)):\n",
    "                    representation = representation[0]\n",
    "        layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "        acts.append(layer_activations.cpu())\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activations(model, target_model, token_loader, cfg, model_on_gpu=True, num_batches=500):\n",
    "    saved_inputs = []\n",
    "    for k, (batch) in enumerate(token_loader):\n",
    "        saved_inputs.append(batch[\"input_ids\"].to(cfg.device))\n",
    "        \n",
    "        if (k+1)%num_batches==0:\n",
    "            # compute base and target model activations\n",
    "            if model_on_gpu:\n",
    "                base_activations = compute_activations(model, saved_inputs)\n",
    "                model = model.cpu()\n",
    "                target_model = target_model.to(cfg.device)\n",
    "            target_activations = compute_activations(target_model, saved_inputs)\n",
    "            if not model_on_gpu:\n",
    "                target_model = target_model.cpu()\n",
    "                model = model.to(cfg.device)\n",
    "                base_activations = compute_activations(model, saved_inputs)\n",
    "            model_on_gpu = not model_on_gpu\n",
    "            \n",
    "            for base_activation, target_activation in zip(base_activations, target_activations):\n",
    "                yield base_activation, target_activation\n",
    "\n",
    "            # wipe saved inputs\n",
    "            saved_inputs = []\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e97599381c06f161_*_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [01:03<13:50, 63.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Loss: 0.69 | Tokens: 0 | Self Similarity: 1.00\n",
      "Reconstruction Loss: 0.68 | Tokens: 0 | Self Similarity: 1.00\n",
      "Reconstruction Loss: 0.69 | Tokens: 0 | Self Similarity: 1.00\n",
      "Reconstruction Loss: 0.67 | Tokens: 0 | Self Similarity: 1.00\n",
      "Sparsity: 39.9 | Dead Features: 15280 | Reconstruction Loss: 0.69 | Tokens: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:06<00:00,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached max number of tokens: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing transfer autoencoders\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed, split=\"train\")\n",
    "dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "target_dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "sft_dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "max_num_tokens = 300_000\n",
    "log_every=100\n",
    "# Freeze model parameters \n",
    "target_model = target_model.cpu()\n",
    "target_model.eval()\n",
    "model = model.to(cfg.device)\n",
    "model.eval()\n",
    "\n",
    "target_model.requires_grad_(False)\n",
    "model.requires_grad_(False)\n",
    "\n",
    "last_decoders = dict([(modes[i],transfer_autoencoders[i].decoder.clone().detach()) for i in range(len(transfer_autoencoders))])\n",
    "model_on_gpu = True\n",
    "\n",
    "saved_inputs = []\n",
    "i = 0 # counts all optimization steps\n",
    "num_saved_so_far = 0\n",
    "print(\"starting loop\")\n",
    "\n",
    "auto_total_loss = 0\n",
    "auto_base_loss = 0\n",
    "auto_sft_loss = 0\n",
    "\n",
    "target_base_loss = 0\n",
    "target_sft_loss = 0\n",
    "\n",
    "target_total_loss = 0\n",
    "total_losses = dict((mode,0) for mode in modes)\n",
    "\n",
    "for (base_activation, target_activation) in tqdm(generate_activations(model, target_model, token_loader, cfg, model_on_gpu=model_on_gpu, num_batches=100), \n",
    "                                                 total=int(max_num_tokens/(cfg.max_length*cfg.model_batch_size))):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        c = autoencoder.encode(base_activation.to(cfg.device))\n",
    "        x_hat = autoencoder.decode(c)\n",
    "        autoencoder_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        auto_total_loss += autoencoder_loss\n",
    "        auto_base_loss += (x_hat - base_activation.to(cfg.device)).pow(2).mean()\n",
    "        c_sft = autoencoder.encode(target_activation.to(cfg.device))\n",
    "        auto_sft_loss += (autoencoder.decode(c_sft) - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        dead_features += c.sum(dim=0).cpu()\n",
    "        \n",
    "        \n",
    "        target_c = target_autoencoder.encode(base_activation.to(cfg.device))\n",
    "        target_x_hat = target_autoencoder.decode(c)\n",
    "        target_autoencoder_loss = (target_x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        target_total_loss += target_autoencoder_loss\n",
    "        target_base_loss += (target_x_hat - base_activation.to(cfg.device)).pow(2).mean()\n",
    "        target_c_sft = target_autoencoder.encode(target_activation.to(cfg.device))\n",
    "        target_sft_loss += (target_autoencoder.decode(target_c_sft) - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        target_dead_features += target_c.sum(dim=0).cpu()\n",
    "        sft_dead_features += target_c_sft.sum(dim=0).cpu()\n",
    "    \n",
    "    wandb_log = {}\n",
    "    \n",
    "    for tsae, mode in zip(transfer_autoencoders, modes):\n",
    "        with torch.no_grad():\n",
    "            x_hat = tsae.decode(c)\n",
    "        \n",
    "        reconstruction_loss = (x_hat - target_activation.to(cfg.device)).pow(2).mean()\n",
    "        total_loss = reconstruction_loss # NO L1 LOSS\n",
    "        total_losses[mode] += total_loss\n",
    "\n",
    "        if (i % log_every == 0): # Check here so first check is model w/o change\n",
    "            self_similarity = torch.cosine_similarity(tsae.decoder, last_decoders[mode], dim=-1).mean().cpu().item()\n",
    "            last_decoders[mode] = tsae.decoder.clone().detach()\n",
    "            num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            print(f\"Reconstruction Loss: {reconstruction_loss:.2f} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "            wandb_log.update({\n",
    "                f'{mode} Reconstruction Loss': reconstruction_loss.item(),\n",
    "                f'{mode} Self Similarity': self_similarity\n",
    "            })\n",
    "\n",
    "    if (i % log_every == 0):\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            num_dead_features = (dead_features == 0).sum().item()\n",
    "            \n",
    "            target_sparsity = (target_c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            target_num_dead_features = (target_dead_features == 0).sum().item()\n",
    "            \n",
    "        print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Reconstruction Loss: {autoencoder_loss:.2f} | Tokens: {num_tokens_so_far}\")\n",
    "        \n",
    "        wandb_log.update({  # Base SAE log\n",
    "                f'SAE Sparsity': sparsity,\n",
    "                f'Dead Features': num_dead_features,\n",
    "                f'SAE Reconstruction Loss': autoencoder_loss.item(),\n",
    "                f'Tokens': num_tokens_so_far,\n",
    "            })\n",
    "        \n",
    "        wandb_log.update({  # Target SAE log\n",
    "                f'Target SAE Sparsity': target_sparsity,\n",
    "                f'Target Dead Features': target_num_dead_features,\n",
    "                f'Target SAE Reconstruction Loss': target_autoencoder_loss.item(),\n",
    "            })\n",
    "        \n",
    "        # Non transfer statistics (only base, or only sft)\n",
    "        with torch.no_grad():\n",
    "            sft_sparsity = (c_sft != 0).float().mean(dim=0).sum().cpu().item()            \n",
    "            target_sft_sparsity = (target_c_sft != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            num_sft_dead_features = (sft_dead_features == 0).sum().item()\n",
    "            \n",
    "        wandb_log.update({  # Base only and Target only losses\n",
    "                f'Sparsity on SFT': sft_sparsity,\n",
    "                f'Target Sparsity on SFT': target_sft_sparsity,\n",
    "                f'SFT Dead Features': num_sft_dead_features,\n",
    "            })\n",
    "        wandb.log(wandb_log)\n",
    "    i+=1\n",
    "    \n",
    "                \n",
    "    \n",
    "    num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    if(num_tokens_so_far > max_num_tokens):\n",
    "        print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log total average loss and finish wandb\n",
    "wandb_log = {\n",
    "    'SAE Average Loss': auto_total_loss/i,\n",
    "    'Target SAE Average Loss': target_total_loss/i,\n",
    "    \n",
    "    'SAE Average Loss on Base': auto_base_loss/i,\n",
    "    'Target SAE Average Loss on Base': target_base_loss/i,\n",
    "    \n",
    "    'SAE Average Loss on SFT': auto_sft_loss/i,\n",
    "    'Target SAE Average Loss on SFT': target_sft_loss/i,\n",
    "    }\n",
    "for mode in modes:\n",
    "    wandb_log.update({  # Target SAE log\n",
    "                    f'{mode} Average Loss': total_losses[mode]/i,\n",
    "                })\n",
    "    \n",
    "wandb.log(wandb_log)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SAE Average Loss': tensor(0.6964, device='cuda:0'),\n",
      " 'SAE Average Loss on Base': tensor(0.7079, device='cuda:0'),\n",
      " 'SAE Average Loss on SFT': tensor(0.6943, device='cuda:0'),\n",
      " 'Target SAE Average Loss': tensor(11.6935, device='cuda:0'),\n",
      " 'Target SAE Average Loss on Base': tensor(11.7038, device='cuda:0'),\n",
      " 'Target SAE Average Loss on SFT': tensor(0.6150, device='cuda:0'),\n",
      " 'bias Average Loss': tensor(0.6892, device='cuda:0'),\n",
      " 'free Average Loss': tensor(0.6775, device='cuda:0'),\n",
      " 'rotation Average Loss': tensor(0.6865, device='cuda:0'),\n",
      " 'scale Average Loss': tensor(0.6919, device='cuda:0')}\n",
      "{'SAE Average Loss': tensor(0.6964, device='cuda:0'), 'Target SAE Average Loss': tensor(11.6935, device='cuda:0'), 'SAE Average Loss on Base': tensor(0.7079, device='cuda:0'), 'Target SAE Average Loss on Base': tensor(11.7038, device='cuda:0'), 'SAE Average Loss on SFT': tensor(0.6943, device='cuda:0'), 'Target SAE Average Loss on SFT': tensor(0.6150, device='cuda:0'), 'scale Average Loss': tensor(0.6919, device='cuda:0'), 'rotation Average Loss': tensor(0.6865, device='cuda:0'), 'bias Average Loss': tensor(0.6892, device='cuda:0'), 'free Average Loss': tensor(0.6775, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Prints the nicely formatted dictionary\n",
    "pprint.pprint(wandb_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.6964, device='cuda:0'), tensor(0.7079, device='cuda:0'), tensor(0.6943, device='cuda:0'), tensor(11.7038, device='cuda:0'), tensor(0.6150, device='cuda:0'), tensor(11.6935, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "auto_and_target_losses = [\n",
    "    auto_total_loss,\n",
    "    auto_base_loss,\n",
    "    auto_sft_loss,\n",
    "    target_base_loss,\n",
    "    target_sft_loss,\n",
    "    target_total_loss\n",
    "]\n",
    "\n",
    "print([x/i for x in auto_and_target_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dead features\n",
    "import os\n",
    "if not os.path.exists(\"trained_models\"):\n",
    "    os.makedirs(\"trained_models\")\n",
    "# Save model\n",
    "torch.save(dead_features, f\"trained_models/base_dead_features.pt\")\n",
    "torch.save(target_dead_features, f\"trained_models/target_dead_features.pt\")\n",
    "torch.save(sft_dead_features, f\"trained_models/sft_dead_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
