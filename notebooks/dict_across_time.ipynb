{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch import nn\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components, t_type=torch.float16):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        #Â create decoder using float16 to save memory\n",
    "        self.decoder = nn.Linear(n_dict_components, activation_size, bias=True)\n",
    "        # Initialize the decoder weights orthogonally\n",
    "        nn.init.orthogonal_(self.decoder.weight)\n",
    "        self.decoder = self.decoder.to(t_type)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(activation_size, n_dict_components).to(t_type),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = self.encoder(x)\n",
    "        # Apply unit norm constraint to the decoder weights\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "    \n",
    "        x_hat = self.decoder(c)\n",
    "        return x_hat, c\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    \n",
    "# Initialize an empty list to store the loaded data\n",
    "autoencoders = []\n",
    "av_activations = []\n",
    "\n",
    "# Loop over the file indices\n",
    "for i in range(20):  # 20 because you said up to 19, range stops one step before the stop argument\n",
    "    # Create the file path\n",
    "    # file_path_auto = f\"outputs/20230624-035145/minirun{i}/autoencoders.pkl\"\n",
    "    # file_path_activation = f\"outputs/20230624-035145/minirun{i}/av_activations.pkl\"\n",
    "    # file_path_auto = f\"/root/sparse_coding/outputs/20230709-120910-EleutherAI/pythia-1.4b-deduped-15/minirun{i}/autoencoders.pkl\"\n",
    "    # file_path_activation = f\"/root/sparse_coding/outputs/20230709-120910-EleutherAI/pythia-1.4b-deduped-15/minirun{i}/av_activations.pkl\"\n",
    "    file_path_auto = f\"/root/sparse_coding/outputs/20230709-120910-EleutherAI/pythia-1.4b-deduped-15/minirun{i}/autoencoders.pkl\"\n",
    "    file_path_activation = f\"/root/sparse_coding/outputs/20230709-120910-EleutherAI/pythia-1.4b-deduped-15/minirun{i}/av_activations.pkl\"\n",
    "\n",
    "    # Open the file and load the data\n",
    "    with open(file_path_auto, 'rb') as file:\n",
    "        autoencoders.append(pickle.load(file))\n",
    "\n",
    "    with open(file_path_activation, 'rb') as file:\n",
    "        av_activations.append(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_2k = [ae[0][0].decoder.weight.data.T for ae in autoencoders]\n",
    "dictionary_4k = [ae[0][1].decoder.weight.data.T for ae in autoencoders]\n",
    "dictionary_8k = [ae[0][2].decoder.weight.data.T for ae in autoencoders]\n",
    "\n",
    "av_2k = [act[0][0] for act in av_activations]\n",
    "av_4k = [act[0][1] for act in av_activations]\n",
    "av_8k = [act[0][2] for act in av_activations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary_4k[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "def run_mmcs_with_larger(smaller_dict, larger_dict, device):\n",
    "    smaller_dict_features, _ = smaller_dict.shape\n",
    "    larger_dict_features, _ = larger_dict.shape\n",
    "    # Hungary algorithm\n",
    "    # Calculate all cosine similarities and store in a 2D array\n",
    "    cos_sims = np.zeros((smaller_dict_features, larger_dict_features))\n",
    "    larger_dict = larger_dict.to(device)\n",
    "    for idx, vector in enumerate(smaller_dict):\n",
    "        cos_sims[idx] = torch.nn.functional.cosine_similarity(vector.to(device), larger_dict, dim=1).cpu().numpy()\n",
    "    # Convert to a minimization problem\n",
    "    cos_sims = 1 - cos_sims\n",
    "    # Use the Hungarian algorithm to solve the assignment problem\n",
    "    row_ind, col_ind = linear_sum_assignment(cos_sims)\n",
    "    # Retrieve the max cosine similarities and corresponding indices\n",
    "    max_cosine_similarities = 1 - cos_sims[row_ind, col_ind]\n",
    "    \n",
    "    return max_cosine_similarities, row_ind, col_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "time_series_N = len(dictionary_2k)\n",
    "neurons = dictionary_2k[i].shape[0]\n",
    "self_sim = np.zeros((time_series_N-1, neurons))\n",
    "self_sim_larger = np.zeros((time_series_N-1, neurons))\n",
    "mcs = np.zeros((time_series_N, neurons))\n",
    "matched_larger_features_index = np.zeros((time_series_N, neurons), dtype=int)\n",
    "for i in range(time_series_N):\n",
    "    print(\"i:\", i)\n",
    "    mcs[i], _, col = run_mmcs_with_larger(dictionary_2k[i], dictionary_4k[i], device)\n",
    "    # Index larger self_sim by col\n",
    "    matched_larger_features_index[i] = col\n",
    "    if(i != time_series_N-1):\n",
    "        self_sim_larger[i] =  torch.nn.functional.cosine_similarity(dictionary_4k[i].to(device), dictionary_4k[i+1].to(device), dim=1).cpu().numpy()[col]\n",
    "        self_sim[i] = torch.nn.functional.cosine_similarity(dictionary_2k[i].to(device), dictionary_2k[i+1].to(device), dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av_2k_np = np.array(av_2k)\n",
    "av_4k_np = np.array(av_4k)\n",
    "av_8k_np = np.array(av_8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "time_steps = np.arange(time_series_N)\n",
    "time_steps_self_sim = np.arange(1, time_series_N)\n",
    "N = 0\n",
    "max_activation = av_2k_np.max()\n",
    "for N in range(N, N + 20):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.title(f\"Neuron {N}\")\n",
    "    # set ylim to 0, 1\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.plot(time_steps, mcs[:,N], label=\"mcs\", color=\"red\")\n",
    "    for i, txt in enumerate(matched_larger_features_index[:, N]):\n",
    "        plt.text(time_steps[i], mcs[i, N], str(txt), fontsize=8)\n",
    "    plt.plot(time_steps_self_sim, self_sim[:,N], label=\"self_sim smaller\", color=\"blue\")\n",
    "    plt.plot(time_steps_self_sim, self_sim_larger[:,N], label=\"self_sim larger\", color=\"green\")\n",
    "    # Annotate each point on the line\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax.set_ylabel('Cosine Similarity', color=color)\n",
    "    ax.tick_params(axis='y', labelcolor=color)\n",
    "    ax.legend()\n",
    "    ax2 = ax.twinx()  \n",
    "    ax2.plot(av_2k_np[:,N], label=\"average_activations\", color=\"orange\")\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylim(1e-6, max_activation)\n",
    "    color = 'tab:orange'\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_ylabel('average activation', color=color)\n",
    "    ax2.legend()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "matrix = self_sim_larger\n",
    "\n",
    "# Assuming 'matrix' is your 19 x 2048 matrix\n",
    "# matrix = np.random.rand(19, 2048)  # Just a placeholder\n",
    "\n",
    "# Define your linspace values\n",
    "threshold_1_values = np.linspace(0, 1, 20)\n",
    "threshold_2_values = np.linspace(0, 1, 20)\n",
    "\n",
    "# Initialize a 2D array to store the counts for each combination\n",
    "counts = np.zeros((len(threshold_1_values), len(threshold_2_values)))\n",
    "\n",
    "for i, threshold_1 in enumerate(threshold_1_values):\n",
    "    for j, threshold_2 in enumerate(threshold_2_values):\n",
    "        if threshold_2 > threshold_1:\n",
    "            continue  # Skip if threshold_2 is greater than threshold_1\n",
    "\n",
    "        # Calculate count for this combination of thresholds\n",
    "        count = 0\n",
    "        for k in range(matrix.shape[1]):  # Loop over each vector (columns)\n",
    "            vector = matrix[:, k]\n",
    "            indices = np.where(vector >= threshold_1)[0]\n",
    "            if len(indices) == 0:\n",
    "                continue\n",
    "\n",
    "            for index in indices:\n",
    "                if np.any(vector[index + 1:] < threshold_2):\n",
    "                    count += 1\n",
    "                    break\n",
    "\n",
    "        # Store the count\n",
    "        counts[i, j] = count\n",
    "\n",
    "# Create the plot\n",
    "plt.imshow(counts.T, origin='lower', extent=[0, 1, 0, 1])\n",
    "plt.colorbar(label='Count')\n",
    "plt.ylabel('< Threshold_2 after')\n",
    "plt.xlabel('> Threshold_1 at one time')\n",
    "plt.title('Count for different combinations of thresholds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems like high MCS is correlated w/ high activation. We can check this by plotting the MCS vs. activation\n",
    "# First reduce each neuron dim to it's max value (max mcs & max activation))\n",
    "max_mcs = mcs.max(axis=0) \n",
    "max_activation = av_2k_np.max(axis=0)\n",
    "\n",
    "# Plot the max mcs vs. max activation\n",
    "plt.scatter(max_mcs, max_activation)\n",
    "plt.xlabel(\"max mcs\")\n",
    "plt.ylabel(\"max activation\")\n",
    "# put y axis on log scale\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis: many high MCS features have many matching features in the larger dictionary\n",
    "# to start, can just plot the full cos_sim matrix for a high MCS feautre & a low MCS feature\n",
    "# Let's restrict ourselves to the last time step\n",
    "high_mcs_feature_index = np.argmax(mcs[-1])\n",
    "low_mcs_feature_index = np.argmin(mcs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_mcs_feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_mcs_cs = torch.nn.functional.cosine_similarity(dictionary_2k[16][45].to(device), dictionary_4k[-1].to(device), dim=1).cpu().numpy()\n",
    "low_mcs_cs = torch.nn.functional.cosine_similarity(dictionary_2k[-1][low_mcs_feature_index].to(device), dictionary_4k[-1].to(device), dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the cosine similarities\n",
    "plt.hist(low_mcs_cs, bins=100, alpha=0.5, label=\"low mcs\")\n",
    "plt.hist(high_mcs_cs, bins=100, alpha=0.5, label=\"high mcs\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 2\n",
    "for x in range(20):\n",
    "    high_mcs_cs = torch.nn.functional.cosine_similarity(dictionary_2k[x][feature].to(device), dictionary_4k[x].to(device), dim=1).cpu().numpy()\n",
    "    thresh = 0.5\n",
    "    print(f\"Number of high mcs features w/ cs > {thresh}:\", (high_mcs_cs > thresh).sum())\n",
    "    print(high_mcs_cs.argmax(), \":\", high_mcs_cs.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis seems false. There are at most 2 or 3 that have high MCS w/ the feature, but it's inconsistant. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Encoder Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if the encoder has a huge negative bias towards specific features? Let's just plot them for the diff dictionaries\n",
    "encoder_2k = [ae[0][0].encoder[0] for ae in autoencoders]\n",
    "encoder_4k = [ae[0][1].encoder[0] for ae in autoencoders]\n",
    "encoder_8k = [ae[0][2].encoder[0] for ae in autoencoders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(5):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     print(f\"T={n} | mean: {bias_2k.mean():.3f}, min: {bias_2k.min():.3f}, max: {bias_2k.max():.3f}, std: {bias_2k.std():.3f}\")\n",
    "# bias_4k = encoder_4k[n].bias.detach().cpu().numpy()\n",
    "# bias_8k = encoder_8k[n].bias.detach().cpu().numpy()\n",
    "# Plot the bias agains the mcs\n",
    "    bias_2k = encoder_2k[n].bias.detach().cpu().numpy()\n",
    "    ax.scatter(bias_2k, mcs[n])\n",
    "    plt.xlabel(\"bias\")\n",
    "    plt.ylabel(\"mcs\")\n",
    "    plt.title(f\"T={n}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps.shape, self_sim_np.shape, mcs_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_2k[i].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = torch.rand(10,2)\n",
    "d2 = torch.rand(10,2)\n",
    "torch.nn.functional.cosine_similarity(d1, d1, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
