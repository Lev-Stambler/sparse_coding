{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchorse/logan/t/logan/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded autoencoder w/ {'dict_size': 3072, 'l1_alpha': 0.0013894954463467002} on cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "\n",
    "features = 1\n",
    "autoencoder_path = \"/mnt/ssd-cluster/longrun2408/tied_residual_l2_r6/_31/learned_dicts.pt\"\n",
    "autoencoder_index = 5\n",
    "layer = 2\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "device = \"cuda:3\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "setting= \"residual\"\n",
    "max_seq_length=30\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "all_autoencoders = torch.load(autoencoder_path)\n",
    "autoencoder, hyperparams = all_autoencoders[autoencoder_index]\n",
    "autoencoder.to_device(device)\n",
    "print(f\"Loaded autoencoder w/ {hyperparams} on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained_no_processing(model_name, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NeelNanda/pile-10k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-e8c69c6c2a788d7f.arrow\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-491db685689a5b37.arrow\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3ac317cfd6c1b211.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def download_dataset(dataset_name, tokenizer, max_length=256, num_datapoints=None):\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    if(num_datapoints):\n",
    "        split_text = f\"train[:{num_datapoints}]\"\n",
    "    else:\n",
    "        split_text = \"train\"\n",
    "    dataset = load_dataset(dataset_name, split=split_text).map(\n",
    "        lambda x: tokenizer(x['text']),\n",
    "        batched=True,\n",
    "    ).filter(\n",
    "        lambda x: len(x['input_ids']) > max_length\n",
    "    ).map(\n",
    "        lambda x: {'input_ids': x['input_ids'][:max_length]}\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "print(f\"Downloading {dataset_name}\")\n",
    "dataset = download_dataset(dataset_name, tokenizer= model.tokenizer, max_length=max_seq_length, num_datapoints=None) # num_datapoints grabs all of them if None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = model.cfg.d_model\n",
    "assert (d_model == autoencoder.encoder.shape[-1]), f\"Model and autoencoder must have same hidden size. Model: {d_model}, Autoencoder: {autoencoder.encoder.shape[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dictionary activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:06<00:00, 51.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "def get_dictionary_activations(model, dataset, cache_name, autoencoder, batch_size=32):\n",
    "    num_features, d_model = autoencoder.encoder.shape\n",
    "    datapoints = dataset.num_rows\n",
    "    dictionary_activations = torch.zeros((datapoints*max_seq_length, num_features))\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "            batched_dictionary_activations = autoencoder.encode(batched_neuron_activations)\n",
    "            dictionary_activations[i*batch_size*max_seq_length:(i+1)*batch_size*max_seq_length,:] = batched_dictionary_activations.cpu()\n",
    "    return dictionary_activations\n",
    "\n",
    "print(\"Getting dictionary activations\")\n",
    "dictionary_activations = get_dictionary_activations(model, dataset, cache_name, autoencoder, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchorse/logan/test/sparse_coding/interp_utils.py:24: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:33.)\n",
      "  bins = torch.bucketize(best_feature_activations, bin_boundaries)\n"
     ]
    }
   ],
   "source": [
    "# from interp_utils import *\n",
    "if isinstance(features, int):\n",
    "    features = [features]\n",
    "for feature in features:\n",
    "    text_list, full_text, token_list, full_token_list = get_feature_datapoints(feature, dictionary_activations, model.tokenizer, max_seq_length, dataset, setting=\"uniform\")\n",
    "    # text_list, full_text, token_list, full_token_list = get_feature_datapoints(feature, dictionary_activations, dataset, setting=\"max\")\n",
    "    # visualize_text(full_text, feature, model, autoencoder, layer)\n",
    "l = visualize_text(text_list, feature, model, autoencoder, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_datapoints(feature_index, dictionary_activations, tokenizer, token_amount, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        # min_value = torch.min(best_feature_activations)\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            if(bin_idx==0): # Skip the first one. This is below the median\n",
    "                continue\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    num_datapoints = int(dictionary_activations.shape[0]/token_amount)\n",
    "    datapoint_indices =[np.unravel_index(i, (num_datapoints, token_amount)) for i in found_indices]\n",
    "    all_activations = best_feature_activations.reshape(num_datapoints, token_amount).tolist()\n",
    "    full_activations = []\n",
    "    partial_activations = []\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for i, (md, s_ind) in enumerate(datapoint_indices):\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        full_activations.append(all_activations[md])\n",
    "        partial_activations.append(all_activations[md][:s_ind+1])\n",
    "        text = tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list, partial_activations, full_activations\n",
    "\n",
    "text_list, full_text, token_list, full_token_list, partial_activations, full_activations = get_feature_datapoints(feature, dictionary_activations, model.tokenizer, max_seq_length, dataset, setting=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-mchorse'\n",
      "Loading page (1/2)\n",
      "Rendering (2/2)                                                    \n",
      "Done                                                               \n"
     ]
    }
   ],
   "source": [
    "# display_tokens(full_token_list, full_activations, model.tokenizer)\n",
    "# display_tokens(token_list, partial_activations, model.tokenizer)\n",
    "save_token_display(token_list, partial_activations, model.tokenizer, f\"feature_{feature}_layer_{layer}_setting_{setting}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-mchorse'\n",
      "Loading page (1/2)\n",
      "Rendering (2/2)                                                    \n",
      "Done                                                               \n",
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-mchorse'\n",
      "Loading page (1/2)\n",
      "Rendering (2/2)                                                    \n",
      "Done                                                               \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(217,217,255,1); color:rgb(0,0,0)\"> I</span><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\"> like</span><span style=\"background-color:rgba(0,0,255,1); color:rgb(255,255,255)\"> eggs</span><span style=\"background-color:rgba(0,0,255,1); color:rgb(255,255,255)\"> and</span><span style=\"background-color:rgba(217,217,255,1); color:rgb(0,0,0)\"> ↵</span><br><br><span style=\"background-color:rgba(245,245,245,1);color:rgb(0,0,0)\">&nbsp0.0&nbsp</span><span style=\"background-color:rgba(165,165,255,1);color:rgb(0,0,0)\">&nbsp2.0&nbsp</span><span style=\"background-color:rgba(110,110,255,1);color:rgb(0,0,0)\">&nbsp4.0&nbsp</span><span style=\"background-color:rgba(55,55,255,1);color:rgb(255,255,255)\">&nbsp6.0&nbsp</span><span style=\"background-color:rgba(0,0,255,1);color:rgb(255,255,255)\">&nbsp8.0&nbsp</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import imgkit\n",
    "def tokens_and_activations_to_html(toks, activations, tokenizer):\n",
    "    if isinstance(toks, torch.Tensor):\n",
    "        if toks.dim() == 1:\n",
    "            toks = [toks.tolist()]\n",
    "        elif toks.dim()==2:\n",
    "            toks = toks.tolist()\n",
    "        else: \n",
    "            raise NotImplementedError(\"tokens must be 1 or 2 dimensional\")\n",
    "    elif isinstance(toks, list):\n",
    "        # ensure it's a list of lists\n",
    "        if isinstance(toks[0], int):\n",
    "            toks = [toks]\n",
    "    if isinstance(activations, torch.Tensor):\n",
    "        if(activations.dim() == 1):\n",
    "            activations = [activations.tolist()]\n",
    "        elif(activations.dim() == 2):\n",
    "            activations = activations.tolist()\n",
    "        else:\n",
    "            raise NotImplementedError(\"activations must be 1 or 2 dimensional\")\n",
    "    elif isinstance(activations, list):\n",
    "        # ensure it's a list of lists\n",
    "        if isinstance(activations[0], float) or isinstance(activations[0], int):\n",
    "            activations = [activations]\n",
    "    # convert tokens into strings\n",
    "    toks = [[tokenizer.decode(t).replace('Ġ', '&nbsp').replace('\\n', '↵') for t in tok] for tok in toks]\n",
    "    highlighted_text = []\n",
    "    max_value = max([max(activ) for activ in activations])\n",
    "    min_value = min([min(activ) for activ in activations])\n",
    "    white = 245\n",
    "    for act, tok in zip(activations, toks):\n",
    "        for a, t in zip(act, tok):\n",
    "            if a > 0.0:\n",
    "                ratio = a/max_value\n",
    "                text_color = \"0,0,0\" if ratio <= 0.5 else \"255,255,255\"\n",
    "                highlighted_text.append(f'<span style=\"background-color:rgba({int(220-(220*ratio))},{int(220-(220*ratio))},255,1); color:rgb({text_color})\">{t}</span>')\n",
    "            elif a < 0.0:\n",
    "                ratio = a/min_value\n",
    "                text_color = \"0,0,0\" if ratio <= 0.5 else \"255,255,255\"\n",
    "                highlighted_text.append(f'<span style=\"background-color:rgba(255, {int(220-(220*ratio))},{int(220-(220*ratio))},1);color:rgb({text_color})\">{t}</span>')\n",
    "            else:\n",
    "                # highlighted_text.append(f'<span style=\"background-color:rgba(255,255,255,1);color:rgb(0,0,0)\">{t}</span>')\n",
    "                highlighted_text.append(f'<span style=\"background-color:rgba({white},{white},{white},1);color:rgb(0,0,0)\">{t}</span>')\n",
    "        highlighted_text.append('<br><br>')\n",
    "    # Add color bar\n",
    "    num_colors = 4\n",
    "    if(min_value < 0):\n",
    "        for i in range(num_colors, 0, -1):\n",
    "            ratio = i / (num_colors)\n",
    "            value = round((min_value*ratio),1)\n",
    "            text_color = \"255,255,255\" if ratio > 0.5 else \"0,0,0\"\n",
    "            highlighted_text.append(f'<span style=\"background-color:rgba(255, {int(220-(220*ratio))},{int(220-(220*ratio))},1); color:rgb({text_color})\">&nbsp{value}&nbsp</span>')\n",
    "    # Do zero\n",
    "    highlighted_text.append(f'<span style=\"background-color:rgba({white},{white},{white},1);color:rgb(0,0,0)\">&nbsp0.0&nbsp</span>')\n",
    "    # Do positive\n",
    "    if(max_value > 0):\n",
    "        for i in range(1, num_colors+1):\n",
    "            ratio = i / (num_colors)\n",
    "            value = round((max_value*ratio),1)\n",
    "            text_color = \"255,255,255\" if ratio > 0.5 else \"0,0,0\"\n",
    "            highlighted_text.append(f'<span style=\"background-color:rgba({int(220-(220*ratio))},{int(220-(220*ratio))},255,1);color:rgb({text_color})\">&nbsp{value}&nbsp</span>')\n",
    "    highlighted_text = ''.join(highlighted_text)\n",
    "    return highlighted_text\n",
    "\n",
    "def display_tokens(tokens, activations, tokenizer):\n",
    "    return display(HTML(tokens_and_activations_to_html(tokens, activations, tokenizer)))\n",
    "\n",
    "def save_token_display(tokens, activations, tokenizer, path):\n",
    "    html = tokens_and_activations_to_html(tokens, activations, tokenizer)\n",
    "    imgkit.from_string(html, path)\n",
    "    # print(f\"Saved to {path}\")\n",
    "    return\n",
    "tokenizer = model.tokenizer\n",
    "text = \" I like eggs and \\n\"\n",
    "activations = [0.1, 0.0, 8.0, 8.0, 0.1]\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
    "save_token_display(tokens, activations, tokenizer, \"test.jpg\")\n",
    "save_token_display(tokens, activations, tokenizer, \"test.jpg\")\n",
    "display_tokens(tokens, activations, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span>↵ hey ↵ ya</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"<span>↵ hey ↵ ya</span>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  309,   751, 11624,   285,  2490])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' I', ' like', ' eggs', ' and', ' ↵']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[tokenizer.decode(t).replace('Ġ', '&nbsp').replace('\\n', '↵') for t in tok] for tok in [tokens]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠĊ']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.convert_ids_to_tokens([2490])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode([2490])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↵\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".replace(\"\\n\", \"↵\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3848,\n",
       "  18415,\n",
       "  26203,\n",
       "  187,\n",
       "  187,\n",
       "  3848,\n",
       "  18415,\n",
       "  8966,\n",
       "  25418,\n",
       "  26203,\n",
       "  313,\n",
       "  6448,\n",
       "  3495,\n",
       "  4162,\n",
       "  12034,\n",
       "  10,\n",
       "  310,\n",
       "  271,\n",
       "  4383,\n",
       "  31926,\n",
       "  665,\n",
       "  7120,\n",
       "  347,\n",
       "  247,\n",
       "  259,\n",
       "  4940,\n",
       "  390],\n",
       " [18,\n",
       "  15,\n",
       "  7327,\n",
       "  273,\n",
       "  253,\n",
       "  14723,\n",
       "  187,\n",
       "  510,\n",
       "  1246,\n",
       "  3688,\n",
       "  7033,\n",
       "  3839,\n",
       "  281,\n",
       "  22486,\n",
       "  273,\n",
       "  2144,\n",
       "  534,\n",
       "  403,\n",
       "  20618,\n",
       "  689,\n",
       "  247,\n",
       "  12045,\n",
       "  285,\n",
       "  4845,\n",
       "  387,\n",
       "  253,\n",
       "  5024,\n",
       "  390],\n",
       " [1532,\n",
       "  187,\n",
       "  15834,\n",
       "  27,\n",
       "  686,\n",
       "  37,\n",
       "  17799,\n",
       "  29886,\n",
       "  3210,\n",
       "  2085,\n",
       "  247,\n",
       "  4217,\n",
       "  7792,\n",
       "  323,\n",
       "  14053,\n",
       "  19349,\n",
       "  390],\n",
       " [7475, 19020, 310, 247, 8723, 382, 3367, 390],\n",
       " [2214, 247, 12732, 22, 19327, 253, 4394, 3551, 275, 432, 34538, 2207],\n",
       " [18968,\n",
       "  16078,\n",
       "  13,\n",
       "  367,\n",
       "  610,\n",
       "  18367,\n",
       "  13,\n",
       "  48585,\n",
       "  13,\n",
       "  10388,\n",
       "  478,\n",
       "  13,\n",
       "  2325,\n",
       "  7885,\n",
       "  13,\n",
       "  21570,\n",
       "  13,\n",
       "  330,\n",
       "  1595,\n",
       "  1351,\n",
       "  80,\n",
       "  13,\n",
       "  401,\n",
       "  2682,\n",
       "  285,\n",
       "  16,\n",
       "  263],\n",
       " [187, 22313, 367, 15, 20, 69, 5329, 313, 7330, 10, 187, 27018, 2207],\n",
       " [510, 17068, 3448, 273, 9550, 310, 48526, 2584, 18000, 285, 48984, 2581, 685],\n",
       " [42, 452, 247, 2622, 966, 6335, 313, 1439],\n",
       " [48943,\n",
       "  187,\n",
       "  187,\n",
       "  6080,\n",
       "  273,\n",
       "  253,\n",
       "  1682,\n",
       "  5074,\n",
       "  403,\n",
       "  1110,\n",
       "  326,\n",
       "  2226,\n",
       "  760,\n",
       "  275,\n",
       "  776,\n",
       "  4440,\n",
       "  7097,\n",
       "  15,\n",
       "  27746,\n",
       "  556,\n",
       "  253,\n",
       "  3745,\n",
       "  281,\n",
       "  1056,\n",
       "  667,\n",
       "  1894,\n",
       "  247,\n",
       "  3331,\n",
       "  285,\n",
       "  667]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['&nbspI', '&nbsplike', '&nbspeggs', '&nbspand', '&nbspĊ']]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].tolist()\n",
    "[[t.replace('Ġ', '&nbsp').replace('\\n', '↵') for t in tokenizer.convert_ids_to_tokens(tok)] for tok in tt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2611541/2288632079.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(torch.tensor(activations)).max()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(torch.tensor(activations)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Hello, World!!</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('<h1>Hello, World!!</h1>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(render_toks_w_weights(torch.tensor([3,4,5, 9, 10]), [0.5, 0.0, 100, 4, -4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', '#', '$']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([tok]) for tok in [3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8ef809aa-1dc6\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8ef809aa-1dc6\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"ing\", \" is\", \" hard\", \".\", \" But\", \" like\", \" anything\", \" else\", \",\", \" practice\", \" helps\", \" \\u2014\", \" whether\", \" you\", \"\\u2019\", \"re\", \" a\", \" professionally\", \" trained\", \" vocal\", \"ist\", \" or\", \" just\", \" want\", \" to\", \" sound\", \" less\", \" pitch\", \"y\", \"\\n\", \"ised\", \" trial\", \" comparing\", \" forced\", \"-\", \"air\", \" warming\", \" to\", \" the\", \" upper\", \" or\", \" lower\", \" body\", \" to\", \" prevent\", \" hyp\", \"other\", \"mia\", \" during\", \" thor\", \"ac\", \"oscopic\", \" surgery\", \" in\", \" the\", \" lateral\", \" dec\", \"ub\", \"itus\", \"\\n\", \"P\", \"5\", \" and\", \" plasma\", \" cholesterol\", \" levels\", \" modulate\", \" the\", \" canonical\", \" Wnt\", \" pathway\", \" in\", \" peripheral\", \" blood\", \" leukocytes\", \".\", \"\\\\newline\", \"Infl\", \"ammation\", \" is\", \" triggered\", \" after\", \" invasion\", \" or\", \" injury\", \" to\", \" restore\", \" homeostasis\", \".\", \"\\n\", \"han\", \"\\u2013\", \"Y\", \"ich\", \"ang\", \" railway\", \"\\\\newline\", \"\\\\newline\", \"H\", \"ank\", \"ou\", \"\\u2013\", \"Y\", \"ich\", \"ang\", \" railway\", \" (),\", \" or\", \" H\", \"any\", \"i\", \" railway\", \",\", \" is\", \" a\", \"  \", \"long\", \" high\", \"-\", \"\\n\", \" by\", \" Or\", \"dain\", \"Women\", \" on\", \" Oct\", \" 12\", \",\", \" 2016\", \" in\", \" Blog\", \" |\", \"\\\\newline\", \"\\\\newline\", \"L\", \"orie\", \" W\", \"inder\", \" St\", \"rom\", \"berg\", \" serves\", \" on\", \" the\", \" Or\", \"dain\", \" Women\", \" executive\", \" board\", \"\\n\", \"\\\\newline\", \" *\", \" Copyright\", \" (\", \"c\", \")\", \" 2012\", \",\", \" 2013\", \" Oracle\", \" and\", \"/\", \"or\", \" its\", \" affiliates\", \".\", \" All\", \" rights\", \" reserved\", \".\", \"\\\\newline\", \" *\", \" DO\", \" NOT\", \" AL\", \"TER\", \" OR\", \" REM\", \"OVE\", \"\\n\", \" pas\", \" de\", \" sou\", \"ci\", \" \\u00bb\", \" ou\", \" \\u00ab\", \" pas\", \" de\", \" sou\", \"cis\", \" \\u00bb\", \"?\", \" \\u2013\", \" orth\", \"ograp\", \"he\", \"\\\\newline\", \"\\\\newline\", \"\\u00ab\", \" Il\", \" n\", \"\\u2019\", \"y\", \" a\", \" pas\", \" de\", \" sou\", \"ci\", \"\\n\", \"rina\", \" Pat\", \"ridge\", \" Co\", \"vers\", \" Maxim\", \" October\", \" 2009\", \"\\\\newline\", \"\\\\newline\", \"\\u2018\", \"S\", \"or\", \"ority\", \" Row\", \"\\u2019\", \" star\", \" Aud\", \"rina\", \" Pat\", \"ridge\", \" strips\", \" down\", \" to\", \" her\", \" b\", \"ik\", \"ini\", \" on\", \"\\n\", \" content\", \" published\", \" in\", \" C\", \"ureus\", \" is\", \" the\", \" result\", \" of\", \" clinical\", \" experience\", \" and\", \"/\", \"or\", \" research\", \" by\", \" independent\", \" individuals\", \" or\", \" organizations\", \".\", \" C\", \"ureus\", \" is\", \" not\", \" responsible\", \" for\", \" the\", \" scientific\", \"\\n\", \"orts\", \" that\", \" Ryan\", \"air\", \" has\", \" cancelled\", \" 82\", \" flights\", \" on\", \" Sunday\", \" and\", \" will\", \" cancel\", \" between\", \" 40\", \" and\", \" 50\", \" flights\", \" every\", \" day\", \" for\", \" the\", \" next\", \" six\", \" weeks\", \" because\", \" it\", \" \\u201c\", \"m\", \"\\n\"], \"activations\": [[[0.0]], [[-0.00023746490478515625]], [[-0.0014643669128417969]], [[-0.00039887428283691406]], [[0.0007448196411132812]], [[-0.0004277229309082031]], [[-0.006717681884765625]], [[-0.004892230033874512]], [[1.5497207641601562e-06]], [[0.0012340545654296875]], [[-0.0020990371704101562]], [[0.0013151168823242188]], [[-0.0013575553894042969]], [[-0.0002532005310058594]], [[0.0]], [[0.0]], [[-0.0008273124694824219]], [[-0.0016431808471679688]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-1.816587209701538]], [[0.02392578125]], [[-0.0005402565002441406]], [[0.23009777069091797]], [[-0.2352771759033203]], [[0.06609916687011719]], [[-0.0055425167083740234]], [[0.0]], [[0.0]], [[0.002334117889404297]], [[0.0]], [[-0.004767417907714844]], [[0.0016672611236572266]], [[0.0]], [[-0.0062389373779296875]], [[-0.00014162063598632812]], [[0.0012674331665039062]], [[0.007875919342041016]], [[-0.0018343925476074219]], [[-0.17068611085414886]], [[0.07341146469116211]], [[0.024710893630981445]], [[-0.005968570709228516]], [[0.0028438568115234375]], [[-0.01045835018157959]], [[0.00013724341988563538]], [[0.029314517974853516]], [[-0.00032520294189453125]], [[-0.00022839009761810303]], [[-0.0059621334075927734]], [[-0.0036346912384033203]], [[0.020476818084716797]], [[-0.0016058683395385742]], [[0.005356788635253906]], [[0.020630836486816406]], [[-0.001405075192451477]], [[0.002175837755203247]], [[0.0]], [[0.0]], [[0.0]], [[-1.9073486328125e-05]], [[-0.0027370452880859375]], [[0.0]], [[0.0]], [[0.0]], [[-0.000830531120300293]], [[0.0]], [[0.0]], [[0.0018783807754516602]], [[-0.001032114028930664]], [[0.0015916824340820312]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0060558319091796875]], [[0.0006865262985229492]], [[1.5497207641601562e-05]], [[0.0]], [[0.0]], [[-0.0069599151611328125]], [[-0.002220630645751953]], [[-0.4006061553955078]], [[-0.15980172157287598]], [[-0.024616241455078125]], [[0.011848688125610352]], [[0.00702667236328125]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0066776275634765625]], [[-0.0011012554168701172]], [[0.0]], [[-0.0033330917358398438]], [[0.0]], [[-0.00041961669921875]], [[0.003985404968261719]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-3.910064697265625e-05]], [[0.0]], [[-1.1134687662124634]], [[-0.25479841232299805]], [[0.07966375350952148]], [[0.37284111976623535]], [[-0.2591346502304077]], [[0.0723503828048706]], [[-0.0012261271476745605]], [[-0.10661172866821289]], [[0.0002040863037109375]], [[-0.03472328186035156]], [[0.007073760032653809]], [[0.0]], [[0.0]], [[0.0069065093994140625]], [[-1.0531749725341797]], [[-0.021783828735351562]], [[0.0036191940307617188]], [[0.005428314208984375]], [[0.0021696090698242188]], [[-6.932206451892853e-05]], [[0.00883173942565918]], [[-0.021070003509521484]], [[0.0008716583251953125]], [[-0.002768278121948242]], [[-0.020995497703552246]], [[0.0019206702709197998]], [[-0.0029196739196777344]], [[-0.004593372344970703]], [[-0.0036067962646484375]], [[-0.0006732940673828125]], [[0.011550426483154297]], [[0.000888824462890625]], [[0.011250972747802734]], [[0.013962745666503906]], [[0.0015931129455566406]], [[0.0010612905025482178]], [[0.05533027648925781]], [[-0.03099319338798523]], [[-0.01694643497467041]], [[-0.00046443939208984375]], [[0.0010904669761657715]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.001284956932067871]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-1.3826786016579717e-05]], [[-0.40341803431510925]], [[-0.006339691579341888]], [[-0.0015674401074647903]], [[0.005136892199516296]], [[0.001294642686843872]], [[-0.0001518353819847107]], [[-0.000351522583514452]], [[0.0003688409924507141]], [[-0.0001417212188243866]], [[0.016992926597595215]], [[-1.1920838005607948e-07]], [[-4.803761839866638e-06]], [[1.1920838005607948e-07]], [[1.1909287422895432e-06]], [[-0.011980246752500534]], [[-0.003623682539910078]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.21987247467041016]], [[-0.040915489196777344]], [[-0.1696711778640747]], [[-0.005129456520080566]], [[0.03847861289978027]], [[-0.007074117660522461]], [[-0.3247866630554199]], [[0.006501674652099609]], [[-0.035144805908203125]], [[-0.04765748977661133]], [[0.01087486743927002]], [[0.01676464080810547]], [[-0.023861825466156006]], [[-0.03476691246032715]], [[-0.004690647125244141]], [[0.010896921157836914]], [[-0.0006933212280273438]], [[-0.00037300586700439453]], [[-0.001154184341430664]], [[-0.002430737018585205]], [[-0.002175569534301758]], [[0.00443112850189209]], [[-0.00011897087097167969]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.002483367919921875]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0013866424560546875]], [[0.0036344528198242188]], [[0.0]], [[0.01322925090789795]], [[0.0007505416870117188]], [[-0.0006957054138183594]], [[0.0020885467529296875]], [[0.0007181167602539062]], [[-0.0007607936859130859]], [[-6.575882434844971e-05]], [[-7.382477633655071e-06]], [[-0.0037794113159179688]], [[-0.004160404205322266]], [[-0.00011408329010009766]], [[-0.001241922378540039]], [[0.0005397796630859375]], [[0.004241466522216797]], [[0.00010187551379203796]], [[0.00077056884765625]], [[0.0]], [[0.0]], [[0.0]], [[0.0019073486328125]], [[-0.0015516281127929688]], [[4.45246696472168e-05]], [[0.00022014975547790527]], [[-9.584426879882812e-05]], [[-0.00014537572860717773]], [[0.0]], [[0.001556694507598877]], [[-0.04683876037597656]], [[0.0007231831550598145]], [[2.975459210574627e-06]], [[0.0]], [[-0.0024757618084549904]], [[-0.2764911651611328]], [[-0.04724989831447601]], [[0.00011292315321043134]], [[-0.07685253024101257]], [[-0.012034907005727291]], [[-0.34463828802108765]], [[0.06031656265258789]], [[0.004752159118652344]], [[0.000960472971200943]], [[0.016215145587921143]], [[0.0012414772063493729]], [[1.1920838005607948e-07]], [[0.0013832785189151764]], [[0.028005197644233704]], [[0.0]], [[0.0]], [[0.0]], [[-0.0007190704345703125]], [[-0.0006971359252929688]], [[-0.0008492469787597656]], [[0.0]], [[-0.00038433074951171875]], [[0.0]], [[0.0007398128509521484]], [[-0.00013780593872070312]], [[-0.0008139610290527344]], [[0.005222320556640625]], [[-0.0022225379943847656]], [[-0.0016241073608398438]], [[-0.0024590492248535156]], [[-0.0002580881118774414]], [[-0.002585291862487793]], [[0.0010194182395935059]], [[0.00022220611572265625]], [[-0.0006151199340820312]], [[-0.0012485980987548828]], [[-0.00033676624298095703]], [[0.0013473033905029297]], [[0.00026345252990722656]], [[-0.00010406970977783203]], [[-0.0004706382751464844]], [[0.0006222724914550781]], [[-0.0006117820739746094]], [[0.0006170272827148438]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f94a5834250>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_feature_direction_display(full_text, autoencoder, model, layer, features=feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
