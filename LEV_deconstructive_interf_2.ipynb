{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ad2ecfd158f710eb.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d173f762c78db5b8.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bd49b6b2e3856158.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"JeanKaddour/minipile\"\n",
    "token_amount= 40\n",
    "#TODO: change train[:1000] to train if you want whole dataset\n",
    "# 100_000 datasets\n",
    "# I think that we want to use the full 100_000 at some point...\n",
    "# dataset = load_dataset(dataset_name, split=\"train[:100000]\").map(\n",
    "dataset = load_dataset(dataset_name, split=\"train[:10000]\").map( # 1_000 to get started\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")\n",
    "# TODO: we can maybe make this faster for the larger dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"residual\"\n",
    "\n",
    "def get_cache_name_neurons(layer: int):\n",
    "    if setting == \"residual\":\n",
    "        cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp\":\n",
    "        cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "        neurons = model.cfg.d_mlp\n",
    "    elif setting == \"attention\":\n",
    "        cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp_out\":\n",
    "        cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return cache_name, neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "model.cfg.d_model, n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008df0b6708744d2bcdaa9970f065bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4629dc619d0e43fc8ad7f8e88ab40a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fd04e706f648a0930d12668f96059e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bbe6aa458840379a4489d33419eb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caadaeff01fa4dd28c2a6775cc0a4541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c1d286f0f94d00a6376b0f99042605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: in chunks...\n",
    "# TODO: cache?\n",
    "\n",
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "# MAX_CHUNK_SIZE = 1_000\n",
    "\n",
    "# TODO: move to a separate file or something\n",
    "def get_activations(layer: int):\n",
    "    datapoints = dataset.num_rows\n",
    "    embedding_size = model.cfg.d_model\n",
    "    activations_final = np.memmap(f'layer-{layer}.mymemmap', dtype='float32', mode='w+', shape=(datapoints, token_amount, embedding_size))\n",
    "    batch_size = 32\n",
    "\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        cache_name = get_cache_name_neurons(layer)[0]\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            # print(batch)\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            # print(\"AA\", cache[cache_name].shape)\n",
    "            # batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "\n",
    "            real_batch_size = batch.shape[0]\n",
    "            activations_final[i*batch_size:i*batch_size + real_batch_size, :, :] = cache[cache_name].cpu().numpy()\n",
    "    return activations_final\n",
    "\n",
    "model_activations = [get_activations(layer) for layer in range(n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get activations for a specific feature and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9909, 40, 512), (396360, 512))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_activations[0].shape, model_activations[LAYER].reshape(-1, model_activations[LAYER].shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp_utils import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints_with_idx(feature_index, dictionary_activations, tokenizer, token_amount, dataset, k=10, setting=\"max\"):\n",
    "    if len(dictionary_activations.shape) == 3:\n",
    "        best_feature_activations = dictionary_activations[:, :, feature_index].flatten()\n",
    "    else:\n",
    "        best_feature_activations = dictionary_activations\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        # TODO:! Urrr.... is this backwards? CHECK IF ::-1 is correct but I think that it is\n",
    "        found_indices = np.argsort(best_feature_activations)[::-1][:k]\n",
    "        # found_indices = np.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        # min_value = torch.min(best_feature_activations)\n",
    "        min_value = np.min(best_feature_activations)\n",
    "        max_value = np.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        # TODO: hmm\n",
    "        # np bucketize?\n",
    "        # bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "        bins = np.digitize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in np.unique(bins):\n",
    "            if(bin_idx==0): # Skip the first one. This is below the median\n",
    "                continue\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = np.array(np.nonzero(bins == bin_idx)).squeeze(axis=0)\n",
    "            # print(bin_indices.shape)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = np.flip(np.array(sampled_indices), axis=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    num_datapoints = int(dictionary_activations.shape[0])\n",
    "    datapoint_indices =[np.unravel_index(i, (num_datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list, found_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline before looking at \"deconstructive interference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b74d21d0-8608\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b74d21d0-8608\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"26\", \"\\n\", \"17\", \"\\n\", \"5\", \"\\n\", \"05\", \"\\n\", \"9\", \"\\n\", \"9\", \"17\", \"\\n\", \"7\", \"\\n\", \"605\", \"\\n\", \"Cong\", \"ressional\", \" ins\", \"iders\", \" leading\", \" multiple\", \" probes\", \" into\", \"\\n\", \"686\", \"38\", \"556\", \"88\", \"15\", \"\\\\newline\", \"Calculate\", \" -\", \"0\", \".\", \"2\", \" +\", \" 624\", \"85\", \"78\", \"75\", \"79\", \"48\", \"\\n\", \"8\", \"90\", \"\\n\", \"You\", \" put\", \" hour\", \" upon\", \" gru\", \"eling\", \" hour\", \" into\", \"\\n\", \"1\", \"\\n\", \"400\", \"000\", \" (\", \"base\", \" 5\", \")\", \" in\", \" base\", \" 3\", \"?\", \"\\\\newline\", \"12\", \"101\", \"20\", \"2012\", \"\\\\newline\", \"1111\", \"11\", \"0011\", \"01\", \"\\n\", \"V\", \"ascular\", \" surgery\", \" residents\", \" spend\", \" one\", \" fifth\", \" of\", \" their\", \" time\", \" on\", \"\\n\", \"s\", \" the\", \" remainder\", \" when\", \" 322\", \"70\", \"38\", \"17\", \" is\", \" divided\", \" by\", \" 7\", \"74\", \"?\", \"\\\\newline\", \"771\", \"\\\\newline\", \"Calculate\", \" the\", \" remainder\", \" when\", \" 57\", \"32\", \"46\", \"\\n\", \"The\", \" potential\", \" for\", \" tornado\", \"es\", \",\", \" some\", \" of\", \" which\", \" could\", \" be\", \" strong\", \",\", \" is\", \" part\", \" of\", \" a\", \" severe\", \" weather\", \" threat\", \" across\", \" Alabama\", \" starting\", \" early\", \" next\", \" week\", \" and\", \" continuing\", \" perhaps\", \" into\", \"\\n\", \"War\", \" Is\", \" A\", \" Lie\", \"\\\\newline\", \"\\\\newline\", \"War\", \" Is\", \" A\", \" Lie\", \" is\", \" a\", \" thorough\", \" ref\", \"utation\", \" of\", \" every\", \" major\", \" argument\", \" used\", \" to\", \" justify\", \" wars\", \",\", \" drawing\", \" on\", \" evidence\", \" from\", \" numerous\", \" past\", \" wars\", \",\", \" with\", \" a\", \" focus\", \" on\", \"\\n\", \"About\", \" 8\", \"39\", \" results\", \" for\", \" \\\"\", \"yon\", \"h\", \"ap\", \"\\\"\", \"\\\\newline\", \"\\\\newline\", \"The\", \" Democratic\", \" People\", \"'s\", \" Republic\", \" of\", \" Korea\", \" (\", \"D\", \"PR\", \"K\", \")\", \" fired\", \" two\", \" short\", \"-\", \"range\", \" missiles\", \" into\", \"\\n\", \"Michael\", \" Br\", \"iss\", \"enden\", \" presents\", \" AM\", \" Monday\", \" to\", \" Friday\", \" from\", \" 8\", \":\", \"00\", \"am\", \" on\", \" ABC\", \" Local\", \" Radio\", \" and\", \" 7\", \":\", \"10\", \"am\", \" on\", \" Radio\", \" National\", \".\", \" Join\", \" Elizabeth\", \" Jackson\", \" for\", \" the\", \" Saturday\", \" edition\", \" at\", \" 8\", \"am\", \" on\", \"\\n\", \"About\", \" the\", \" Book\", \"\\\\newline\", \"\\\\newline\", \"In\", \" this\", \" book\", \",\", \" Ad\", \"olph\", \"s\", \" and\", \" Carter\", \" explore\", \" key\", \" approaches\", \" to\", \"\\n\", \"The\", \" Pac\", \"-\", \"12\", \" has\", \" decided\", \" not\", \" to\", \"\\n\", \"Hyp\", \"other\", \"mia\", \" among\", \"\\n\", \"The\", \" sight\", \" of\", \" Baltimore\", \" city\", \" students\", \" h\", \"udd\", \"ling\", \" in\", \" coats\", \" in\", \" freezing\", \" classrooms\", \" amid\", \" bitter\", \" cold\", \" temperatures\", \" touched\", \" a\", \" nerve\", \" across\", \" the\", \" country\", \" yesterday\", \",\", \" with\", \" Republican\", \" Governor\", \" Larry\", \" Hogan\", \" dismissing\", \" talk\", \" of\", \" providing\", \" aid\", \" for\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" prevent\", \" fixed\", \" div\", \" block\", \" touch\", \" scroll\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" container\", \" div\", \" which\", \" creates\", \" scroll\", \" when\", \" content\", \" overflow\", \"s\", \",\", \" as\", \" the\", \" content\", \"'s\", \" sibling\", \" I\", \" want\", \" to\", \"\\n\", \"C\", \"overed\", \" versus\", \" uncovered\", \" self\", \"-\", \"expand\", \"able\", \" nit\", \"in\", \"ol\", \" st\", \"ents\", \" in\", \"\\n\", \"The\", \" Kremlin\", \"\\u2019\", \"s\", \" multif\", \"acet\", \"ed\", \" strategy\", \" in\", \" France\", \"\\\\newline\", \"\\\\newline\", \"St\", \"ories\", \" in\", \" the\", \" Context\", \" section\", \" are\", \" not\", \" f\", \"akes\", \".\", \" We\", \" publish\", \" them\", \" in\", \" order\", \" to\", \" provide\", \" greater\", \" insight\", \" for\", \"\\n\", \"Main\", \" navigation\", \"\\\\newline\", \"\\\\newline\", \"SP\", \"UR\", \"S\", \":\", \" THE\", \" SE\", \"AS\", \"ON\", \" THAT\", \" HAD\", \" IT\", \" ALL\", \"\\\\newline\", \"\\\\newline\", \"On\", \" 30\", \" April\", \" 2017\", \",\", \" T\", \"ottenham\", \" beat\", \" Arsenal\", \" 2\", \"-\", \"0\", \" to\", \" finish\", \" above\", \" their\", \" rivals\", \" for\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" remove\", \" the\", \" Time\", \" part\", \" of\", \" a\", \" Date\", \" in\", \" a\", \" Lin\", \"q\", \" Query\", \"\\\\newline\", \"\\\\newline\", \"I\", \" tried\", \" to\", \"\\n\", \"The\", \" Post\", \"menopausal\", \" Est\", \"rogen\", \"/\", \"Pro\", \"gest\", \"ins\", \" Inter\", \"vention\", \" (\", \"PE\", \"PI\", \"-\", \"1\", \")\", \" trial\", \" of\", \" 8\", \"75\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"What\", \" means\", \" \\\"\", \"not\", \" running\", \" under\", \" some\", \" shell\", \"\\\"\", \" in\", \" Perl\", \" scripts\", \"?\", \"\\\\newline\", \"\\\\newline\", \"In\", \" many\", \" per\", \"l\", \" scripts\", \" (\", \"especially\", \" in\", \"\\n\", \"tag\", \":\", \"blogger\", \".\", \"com\", \",\", \"1999\", \":\", \"blog\", \"-\", \"275\", \"878\", \"01\", \".\", \"post\", \"516\", \"95\", \"40\", \"90\", \"34\", \"67\", \"01\", \"28\", \"76\", \"..\", \"comments\", \"2019\", \"-\", \"02\", \"-\", \"13\", \"\\n\", \"BM\", \"G\", \"\\u2019\", \"s\", \" latest\", \" poll\", \" for\", \" the\", \" Independent\", \" reveals\", \" strong\", \" public\", \" opposition\", \" to\", \" the\", \" current\", \" tuition\", \" fee\", \" upper\", \" limit\", \" of\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Access\", \"ing\", \" a\", \" variable\", \" defined\", \" inside\", \" a\", \" function\", \" Python\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" working\", \" on\", \" a\", \" project\", \" and\", \" I\", \" am\", \" running\", \" Python\", \" 2\", \".\", \"7\", \" and\", \" P\", \"anda\", \"3\", \"D\", \" v\", \"1\", \".\", \"8\", \".\", \"1\", \"\\n\", \"Car\", \"bon\", \" mon\", \"oxide\", \" detector\", \" had\", \" gone\", \" off\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Mult\", \"ival\", \"ued\", \" Depend\", \"encies\", \" Task\", \"\\\\newline\", \"\\\\newline\", \"We\", \" have\", \" R\", \"(\", \"A\", \",\", \"B\", \",\", \"C\", \")\", \" \\\\newline\", \"\\\\newline\", \"1\", \" 2\", \" 3\", \"\\\\newline\", \"1\", \" 3\", \" 2\", \"\\\\newline\", \"1\", \" 2\", \" 2\", \"\\\\newline\", \"3\", \"\\n\", \"Univers\", \"ities\", \" in\", \" SF\", \",\", \" KC\", \" make\", \" bet\", \" on\", \" World\", \" Series\", \"\\\\newline\", \"\\\\newline\", \"Tools\", \"\\\\newline\", \"\\\\newline\", \"The\", \" ch\", \"ancell\", \"ors\", \" from\", \"\\n\", \"More\", \"\\\\newline\", \"\\\\newline\", \"Miller\", \" St\", \"ops\", \" 41\", \" Sh\", \"ots\", \",\", \" D\", \"ucks\", \" Beat\", \" Stars\", \" 2\", \"-\", \"0\", \"\\n\", \"Pro\", \"D\", \"rivers\", \".\", \" Class\", \" A\", \" shuttle\", \" drivers\", \".\", \" Sh\", \"uttle\", \" D\", \"rivers\", \" run\", \" from\", \" Dallas\", \" Fort\", \" Worth\", \",\", \" TX\", \" DC\", \"\\n\", \"\\u00d7\", \" Thanks\", \" for\", \" reading\", \"!\", \" Log\", \" in\", \" to\", \" continue\", \".\", \" Enjoy\", \" more\", \" articles\", \" by\", \" logging\", \" in\", \" or\", \" creating\", \" a\", \" free\", \" account\", \".\", \" No\", \" credit\", \" card\", \" required\", \"\\n\", \"Paris\", \" Saint\", \"-\", \"Germ\", \"ain\", \" coach\", \" Un\", \"ai\", \" Emer\", \"y\", \" thought\", \" his\", \" team\", \" played\", \" \\\"\", \"very\", \" well\", \"\\\"\", \" against\", \" bitter\", \" rivals\", \" Mar\", \"se\", \"\\n\", \"\\\"\\\"\\\"\", \"Test\", \" that\", \" all\", \" docs\", \" are\", \" there\", \".\\\"\\\"\\\"\", \"\\\\newline\", \"from\", \" os\", \" import\", \" path\", \"\\\\newline\", \"from\", \"\\n\", \"F\", \"acial\", \" bone\", \" infar\", \"ct\", \"s\", \" in\", \" sick\", \"le\", \" cell\", \" syndromes\", \".\", \"\\\\newline\", \"B\", \"one\", \" infarction\", \" in\", \" the\", \" sick\", \"le\", \" cell\", \" syndromes\", \" (\", \"s\", \"ick\", \"le\", \" cell\", \" anemia\", \",\", \" sick\", \"le\", \" beta\", \" thal\", \"as\", \"\\n\", \"#\", \" ------------------------------------------------\", \"--------\", \"\\\\newline\", \"#\", \" Dec\", \"ou\", \"pled\", \" Classification\", \" Ref\", \"inement\", \"\\\\newline\", \"#\", \" Copyright\", \" (\", \"c\", \")\", \" 2018\", \" University\", \" of\", \" Illinois\", \"\\\\newline\", \"#\", \" Licensed\", \" under\", \"\\n\", \"The\", \" Cit\", \"adel\", \"\\u2019\", \"s\", \" leading\", \" tack\", \"ler\", \",\", \" linebacker\", \" Carl\", \" Robinson\", \",\", \" will\", \" miss\", \" the\", \" rest\", \" of\", \" the\", \" football\", \" season\", \" after\", \" tearing\", \" the\", \" anterior\", \" cruc\", \"iate\", \"\\n\", \"Big\", \" Red\", \" Bar\", \"rel\", \"cast\", \" 48\", \":\", \" The\", \" Old\", \" Patch\", \" And\", \" B\", \"rick\", \"\\\\newline\", \"\\\\newline\", \"The\", \" Big\", \" Red\", \" Bar\", \"rel\", \"cast\", \" returns\", \" to\", \" tick\", \"le\", \" your\", \" ear\", \" drums\", \" with\", \" their\", \" r\", \"amb\", \"lings\", \"!\", \" On\", \" this\", \" week\", \"\\u2019\", \"s\", \" episode\", \"\\n\", \"NEW\", \" YORK\", \" (\", \"AP\", \")\", \" \\u0097\", \" The\", \" Lat\", \"est\", \" on\", \" Time\", \"\\u201d\", \"s\", \" Up\", \" calling\", \" for\", \" a\", \" probe\", \" into\", \" the\", \" Manhattan\", \" District\", \" Attorney\", \"\\u201d\", \"s\", \" office\", \" (\", \"all\", \" times\", \" local\", \"\\n\", \"V\", \"ac\", \"ation\", \" Review\", \"\\\\newline\", \"\\\\newline\", \"Re\", \"boot\", \"?\", \" Rem\", \"ake\", \"?\", \" Re\", \"hash\", \"?\", \" N\", \"ost\", \"alg\", \"ic\", \"\\n\", \"A\", \" student\", \" seeks\", \" help\", \" with\", \" two\", \" separate\", \" questions\", \":\", \" proving\", \" that\", \"\\n\", \"Ari\", \"zona\", \" and\", \" Iowa\", \" State\", \" also\", \" offered\", \" the\", \" 5\", \"-\", \"11\", \",\", \" 201\", \"-\", \"pound\", \" RB\", \"\\\\newline\", \"\\\\newline\", \"K\", \"iss\", \"imme\", \"e\", \" Os\", \"ce\", \"ola\", \" RB\", \" St\", \"af\", \"on\", \" McC\", \"ray\", \" was\", \"\\n\", \"Thomas\", \" R\", \"uther\", \"forth\", \"\\\\newline\", \"\\\\newline\", \"Thomas\", \" R\", \"uther\", \"forth\", \" (\", \"also\", \" R\", \"uther\", \"ford\", \")\", \" (\", \"17\", \"12\", \"\\u2013\", \"17\", \"71\", \")\", \" was\", \" an\", \" English\", \" church\", \"man\", \" and\", \" academic\", \",\", \" Reg\", \"ius\", \"\\n\", \"a\", \" be\", \"\\n\", \"Bank\", \"rupt\", \" Pal\", \"let\", \" Co\", \".\", \" Says\", \" $\", \"39\", \"M\", \" PE\", \" Sale\", \" A\", \" Square\", \" Deal\", \"http\", \"://\", \"www\", \".\", \"lex\", \"is\", \"nex\", \"is\", \".\", \"com\", \"/\", \"\\n\", \"Mic\", \"rowave\", \" Synthesis\", \" Connect\", \"s\", \"\\n\", \"It\", \" is\", \" reported\", \" that\", \" Im\", \"\\u0101m\", \" Al\", \"-\", \"Sh\", \"\\u0101\", \"f\", \"\\ufffd\", \"\\ufffd\", \"ee\", \" \\u2013\", \" All\", \"\\u0101\", \"h\", \" have\", \" mercy\", \" on\", \" him\", \" \\u2013\", \" said\", \",\", \" The\", \" lo\", \"ft\", \"iest\", \" in\", \" status\", \" are\", \"\\n\", \"What\", \" do\", \" \\\"\", \"inter\", \"person\", \"ally\", \" sensitive\", \"\\\"\", \" supervisors\", \" do\", \" and\", \" how\", \"\\n\", \"Embed\", \" This\", \" Story\", \"board\", \" on\", \" Your\", \" Website\", \"\\\\newline\", \"\\\\newline\", \"Copy\", \" This\", \" Code\", \" Sn\", \"ippet\", \"\\n\", \"<?\", \"php\", \"\\\\newline\", \"\\n\", \"The\", \" Lo\", \"oney\", \" T\", \"unes\", \" Show\", \"\\n\", \"European\", \" St\", \"ocks\", \" Mixed\", \" on\", \" Fall\", \" in\", \" German\", \" If\", \"o\", \" Index\", \"\\\\newline\", \"\\\\newline\", \"European\", \" stock\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Account\", \" linked\", \" for\", \" Actions\", \" on\", \" Google\", \",\", \" which\", \" field\", \" to\", \" keep\", \" in\", \" DB\", \"?\", \"\\\\newline\", \"\\\\newline\", \"I\", \" tested\", \" Account\", \" L\", \"inking\", \" using\", \" the\", \" Sim\", \"ulator\", \"\\n\", \"The\", \" U\", \".\", \"S\", \"\\n\", \"#\", \"!/\", \"bin\", \"/\", \"bash\", \"\\\\newline\", \"set\", \" -\", \"e\", \"\\\\newline\", \"\\\\newline\", \"#\", \" Usage\", \"\\\\newline\", \"usage\", \"()\", \" {\", \"\\\\newline\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Get\", \" Google\", \" Maps\", \" v\", \"3\", \" corner\", \" coordinates\", \" from\", \" i\", \"Frame\", \" to\", \" parent\", \" page\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" a\", \" Google\", \" Map\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"get\", \" data\", \" from\", \" JSON\", \"Object\", \" according\", \" to\", \" a\", \" key\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" this\", \" JSON\", \"Object\", \".\", \"\\\\newline\", \"{\", \"\\n\", \"     \", \"13\", \"-\", \"3\", \"144\", \"-\", \"cv\", \"\\\\newline\", \"     \", \"S\", \"iro\", \"b\", \" Im\", \"ports\", \",\", \" Inc\", \".\", \" v\", \".\", \" Peer\", \"less\", \" Ins\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"In\", \"\\n\", \"#\", \"!/\", \"usr\", \"/\", \"bin\", \"/\", \"env\", \" bash\", \"\\\\newline\", \"\\\\newline\", \"#\", \" This\", \" Source\", \" Code\", \" Form\", \" is\", \" subject\", \" to\", \" the\", \" terms\", \" of\", \" the\", \"\\n\", \"TE\", \"HR\", \"AN\", \",\", \" Jan\", \".\", \" 15\", \" (\", \"M\", \"NA\", \")\", \" \\u2013\", \" A\", \" research\", \" team\", \" at\", \" Is\", \"f\", \"ahan\", \" University\", \" in\", \" cooperation\", \" with\", \" a\", \" Japanese\", \" team\", \" designed\", \" a\", \" nan\", \"ocar\", \"rier\", \" to\", \" deliver\", \" anti\", \"-\", \"cancer\", \" drugs\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Question\", \" about\", \" C\", \"#\", \" and\", \" function\", \" prot\", \"otypes\", \"\\\\newline\", \"\\\\newline\", \"I\", \" was\", \" looking\", \" at\", \" the\", \" Text\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" get\", \" a\", \" wall\", \" of\", \" picture\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"mod\", \"_\", \"rew\", \"rite\", \" -\", \" Another\", \" simple\", \"\\n\", \"#\", \"!/\", \"usr\", \"/\", \"bin\", \"\\n\", \"Star\", \" Wars\", \" Commander\", \"\\\\newline\", \"\\\\newline\", \"A\", \" Force\", \"-\", \"Inf\", \"used\", \" Cl\", \"ash\", \" Of\", \" Cl\", \"ans\", \"\\\\newline\", \"\\\\newline\", \"Super\", \"cell\", \"\\u2019\", \"s\", \" Cl\", \"ash\", \" of\", \" Cl\", \"ans\", \" has\", \" inspired\", \" a\", \" number\", \" of\", \" \\u201c\", \"hom\", \"ages\", \",\\u201d\", \" but\", \" few\", \"\\n\", \"In\", \"fluence\", \" of\", \" the\", \" phen\", \"oph\", \"ase\", \" on\", \" the\", \" phenolic\", \" profile\", \" and\", \" antioxidant\", \" properties\", \" of\", \" Dal\", \"mat\", \"ian\", \" sage\", \".\", \"\\\\newline\", \"This\", \"\\n\", \"Effects\", \" of\", \" stere\", \"ois\", \"omer\", \"ism\", \" on\", \" the\", \" crystallization\", \" behavior\", \" and\", \" opt\", \"oelect\", \"rical\", \" properties\", \" of\", \" conjugated\", \" molecules\", \".\", \"\\\\newline\", \"Three\", \"\\n\", \"Child\", \" T\", \"akes\", \" Water\", \" After\", \" Being\", \" Res\", \"c\", \"ued\", \" From\", \" Under\", \" Rub\", \"ble\", \" in\", \" Old\", \" Mos\", \"ul\", \"\\\\newline\", \"\\\\newline\", \"As\", \" Iraqi\", \" forces\", \" continue\", \" sweeping\", \" the\", \" rub\", \"ble\", \" of\", \" Mos\", \"ul\", \"\\u2019\", \"s\", \" Old\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" do\", \" I\", \" set\", \" the\", \" order\", \" of\", \" nodes\", \" for\", \" a\", \" SL\", \"UR\", \"M\", \" job\", \"?\", \"\\\\newline\", \"\\\\newline\", \"I\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" check\", \" if\", \" a\", \" Chinese\", \" character\", \" is\", \" simplified\", \" or\", \" traditional\", \" in\", \" Python\", \" 3\", \"?\", \"\\\\newline\", \"\\\\newline\", \"I\", \"'m\", \"\\n\", \"December\", \" 2010\", \"\\\\newline\", \"\\\\newline\", \"Just\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" can\", \" I\", \" output\", \" a\", \" command\", \" to\", \" a\", \" file\", \",\", \" without\", \" getting\", \" a\", \" blank\", \" file\", \" on\", \" error\", \"?\", \"\\\\newline\", \"\\\\newline\", \"I\", \"'m\", \"\\n\", \"Overall\", \"\\\\newline\", \"\\\\newline\", \"V\", \"ision\", \" Vision\", \"\\\\newline\", \"\\\\newline\", \"Original\", \"ity\", \" Original\", \"ity\", \"\\\\newline\", \"\\\\newline\", \"Techn\", \"ique\", \" Techn\", \"ique\", \"\\\\newline\", \"\\\\newline\", \"Imp\", \"act\", \" Impact\", \"\\\\newline\", \"\\\\newline\", \"Very\", \"\\n\", \"Nice\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Where\", \"\\n\", \"The\", \"\\n\", \"-\", \" Hello\", \"\\n\", \"Abstract\", \"\\\\newline\", \"\\\\newline\", \"The\", \"\\n\", \"Wild\", \"gall\", \"\\\\newline\", \"\\\\newline\", \"The\", \" Wild\", \"\\n\", \"Pages\", \"\\\\newline\", \"\\\\newline\", \"Sunday\", \",\", \" August\", \" 15\", \",\", \" 2010\", \"\\\\newline\", \"\\\\newline\", \"st\", \"ash\", \"b\", \"ust\", \"ing\", \" aug\", \"ust\", \" 15\", \"\\\\newline\", \"\\\\newline\", \"Good\", \"\\n\", \"GR\", \"FC\", \"f\", \"ans\", \"Don\", \"o\", \"avan\", \".\", \"jpg\", \"\\\\newline\", \"\\\\newline\", \"Grand\", \"\\n\", \"Good\", \"\\n\", \"Struct\", \"ural\", \" rearrang\", \"ements\", \" and\", \" chemical\", \" modifications\", \" in\", \" known\", \" cell\", \" penetrating\", \" peptide\", \" strongly\", \" enhance\", \" DNA\", \" delivery\", \" efficiency\", \".\", \"\\\\newline\", \"Am\", \"\\n\", \"Sym\", \"_\", \"type\", \":\", \" module\", \"\\\\newline\", \"Sym\", \"_\", \"name\", \":\", \" top\", \"\\\\newline\", \"Sym\", \"_\", \"lin\", \"eno\", \":\", \" 0\", \"\\\\newline\", \"Sym\", \"\\n\", \"\\ufeff\", \"#\", \"region\", \" C\", \"md\", \"Mess\", \"enger\", \" -\", \" MIT\", \" -\", \" (\", \"c\", \")\", \" 2014\", \" Th\", \"ij\", \"s\", \" El\", \"en\", \"ba\", \"as\", \".\", \"\\\\newline\", \"/*\", \"\\\\newline\", \"  \", \"Cmd\", \"Mess\", \"\\n\", \"Sym\", \"_\", \"type\", \":\", \" module\", \"\\\\newline\", \"Sym\", \"\\n\", \"Look\", \"\\n\", \"\\\"\", \"Hi\", \" Frank\", \"ie\", \".\\\"\", \" \\\"\", \"Hello\", \".\\\"\", \" \\\"\", \"Get\", \" in\", \" here\", \".\\\"\", \" \\\"\", \"Nothing\", \"\\n\", \"Introduction\", \"\\\\newline\", \"============\", \"\\\\newline\", \"\\\\newline\", \"Sym\", \"\\n\"], \"activations\": [[[2.2343437671661377]], [[0.0]], [[2.184044599533081]], [[0.0]], [[2.153202772140503]], [[0.0]], [[2.087623357772827]], [[0.0]], [[2.0593743324279785]], [[0.0]], [[2.0593743324279785]], [[2.0290493965148926]], [[0.0]], [[1.9980835914611816]], [[0.0]], [[1.951631784439087]], [[0.0]], [[-0.6913985013961792]], [[0.216981902718544]], [[0.13889187574386597]], [[0.1211627721786499]], [[0.8768267035484314]], [[0.45496445894241333]], [[0.4626604914665222]], [[1.920097827911377]], [[0.0]], [[1.3978174924850464]], [[1.6340982913970947]], [[1.6589553356170654]], [[1.8440725803375244]], [[2.000661611557007]], [[0.7615979909896851]], [[-0.0435924232006073]], [[0.06577794998884201]], [[0.4645434617996216]], [[0.177263081073761]], [[0.7606565952301025]], [[0.3216856122016907]], [[0.45957326889038086]], [[1.0140457153320312]], [[1.488625168800354]], [[1.6177778244018555]], [[1.8280707597732544]], [[1.883655309677124]], [[0.0]], [[2.1209120750427246]], [[1.812300443649292]], [[0.0]], [[-0.3105677366256714]], [[0.7118270397186279]], [[-0.012382417917251587]], [[1.4661245346069336]], [[-0.32806822657585144]], [[0.5228796005249023]], [[0.08103631436824799]], [[1.8034155368804932]], [[0.0]], [[1.7576960325241089]], [[0.0]], [[1.3082327842712402]], [[1.2256227731704712]], [[0.7025391459465027]], [[0.2751814126968384]], [[1.000303030014038]], [[0.4032920002937317]], [[1.1357225179672241]], [[0.13844841718673706]], [[0.9246925711631775]], [[0.18231287598609924]], [[0.04856184124946594]], [[0.5971036553382874]], [[1.2207858562469482]], [[1.533987045288086]], [[0.8402941226959229]], [[0.39412832260131836]], [[-0.04515394568443298]], [[1.1906039714813232]], [[0.9551427960395813]], [[1.7212988138198853]], [[0.0]], [[0.2148972451686859]], [[0.14350633323192596]], [[-0.1624346673488617]], [[0.37840214371681213]], [[1.0602227449417114]], [[0.5982714891433716]], [[0.43916821479797363]], [[0.9731621742248535]], [[0.16901051998138428]], [[0.28407251834869385]], [[1.6589677333831787]], [[0.0]], [[0.4786239564418793]], [[-0.4511038362979889]], [[0.14085781574249268]], [[0.3490048050880432]], [[0.7054787278175354]], [[1.2674775123596191]], [[1.6017645597457886]], [[1.9058241844177246]], [[0.721878170967102]], [[0.2008623480796814]], [[0.9040851593017578]], [[0.6768437027931213]], [[1.147731900215149]], [[0.4596159756183624]], [[0.15603747963905334]], [[0.28225135803222656]], [[0.34013405442237854]], [[-0.15982544422149658]], [[-0.5023888349533081]], [[-0.08623354136943817]], [[0.15788936614990234]], [[0.3189227879047394]], [[1.1198256015777588]], [[1.6128053665161133]], [[0.0]], [[-1.066534161567688]], [[-0.2807319462299347]], [[0.9177530407905579]], [[-0.35150980949401855]], [[0.34889358282089233]], [[-0.290359228849411]], [[-0.6762937903404236]], [[0.39273545145988464]], [[0.13232406973838806]], [[-0.07829665392637253]], [[0.18588808178901672]], [[-0.06253519654273987]], [[-0.23285123705863953]], [[0.020483456552028656]], [[-0.2610437572002411]], [[0.6372148990631104]], [[-0.04900113493204117]], [[-0.7375762462615967]], [[0.0008153803646564484]], [[0.13529615104198456]], [[0.9288789629936218]], [[0.6688116788864136]], [[0.6860959529876709]], [[0.391329288482666]], [[0.36884260177612305]], [[0.10068440437316895]], [[0.3093935251235962]], [[0.5097286701202393]], [[0.2995551526546478]], [[1.5843451023101807]], [[0.0]], [[0.03568476438522339]], [[0.19953936338424683]], [[0.25100603699684143]], [[-0.14508847892284393]], [[0.08466190099716187]], [[-0.06987792253494263]], [[-0.8693695664405823]], [[-0.22186894714832306]], [[0.056597065180540085]], [[-0.2967769205570221]], [[0.05786207318305969]], [[-0.10576999932527542]], [[-0.24281980097293854]], [[0.3139895796775818]], [[0.2966960668563843]], [[0.7244458794593811]], [[0.04013246297836304]], [[-0.45245349407196045]], [[0.2850628197193146]], [[1.164082407951355]], [[1.647310495376587]], [[1.136017918586731]], [[-0.1529872715473175]], [[-0.015708327293395996]], [[0.33854779601097107]], [[1.6220026016235352]], [[0.5979759693145752]], [[1.2864880561828613]], [[0.27395060658454895]], [[0.4619639813899994]], [[-0.08576089143753052]], [[-0.035485416650772095]], [[0.8679574728012085]], [[0.10013777017593384]], [[0.23404410481452942]], [[1.561706304550171]], [[0.0]], [[-0.08600394427776337]], [[0.22618617117404938]], [[1.2651324272155762]], [[0.4659687578678131]], [[1.2658594846725464]], [[0.34805023670196533]], [[0.30627453327178955]], [[0.455888569355011]], [[0.38988083600997925]], [[0.11848314106464386]], [[0.008236795663833618]], [[-0.08703508973121643]], [[-0.8743574619293213]], [[-0.09003089368343353]], [[-0.6639841794967651]], [[-0.2226710468530655]], [[0.053274378180503845]], [[0.6158275008201599]], [[0.1832115650177002]], [[0.2853904962539673]], [[0.07318729162216187]], [[0.4836590886116028]], [[0.6725783348083496]], [[0.15741318464279175]], [[0.15627190470695496]], [[0.025934085249900818]], [[0.08583000302314758]], [[0.17650239169597626]], [[-0.2403237223625183]], [[-0.6250309348106384]], [[1.512218952178955]], [[0.0]], [[-0.1239296942949295]], [[-0.0973329022526741]], [[0.0761876106262207]], [[0.5517812967300415]], [[0.1512986719608307]], [[0.1189998984336853]], [[0.3928698003292084]], [[1.6541774272918701]], [[0.15161466598510742]], [[1.2002582550048828]], [[0.7353903651237488]], [[0.763640820980072]], [[0.8728493452072144]], [[0.3821559250354767]], [[1.4687573909759521]], [[0.3052937090396881]], [[0.1912706345319748]], [[0.1080639660358429]], [[0.22763052582740784]], [[0.5271583795547485]], [[0.6667866706848145]], [[1.2971817255020142]], [[0.476511687040329]], [[1.4957388639450073]], [[0.03548496961593628]], [[0.1821730136871338]], [[0.028152454644441605]], [[0.2924543619155884]], [[0.2746856212615967]], [[0.07568961381912231]], [[1.0969882011413574]], [[-0.14676184952259064]], [[0.09156686067581177]], [[0.44084030389785767]], [[1.144913911819458]], [[0.7525514960289001]], [[0.39495357871055603]], [[1.4746822118759155]], [[0.0]], [[-0.08600394427776337]], [[-0.6213040351867676]], [[-0.37489110231399536]], [[-0.02522137761116028]], [[-0.17969170212745667]], [[-0.2608151435852051]], [[-0.24697595834732056]], [[-0.16229963302612305]], [[-0.5136586427688599]], [[0.07533194124698639]], [[-0.13169050216674805]], [[0.05527716130018234]], [[-0.08100105822086334]], [[0.1710965633392334]], [[-0.010383479297161102]], [[-0.13425104320049286]], [[0.16120144724845886]], [[1.4535884857177734]], [[0.0]], [[-1.0665342807769775]], [[-0.4487941563129425]], [[0.12203554064035416]], [[0.5631990432739258]], [[0.4055476784706116]], [[0.15425381064414978]], [[0.4801734983921051]], [[1.3824636936187744]], [[0.0]], [[-0.07733742147684097]], [[-0.2771230936050415]], [[0.3197877109050751]], [[1.3719189167022705]], [[0.0]], [[-1.0665342807769775]], [[-0.7276298999786377]], [[0.351345956325531]], [[0.3990381956100464]], [[0.2576296925544739]], [[0.03949934244155884]], [[0.2796946167945862]], [[0.1633278876543045]], [[0.5024451017379761]], [[1.1870794296264648]], [[0.7649035453796387]], [[1.3144657611846924]], [[0.733723521232605]], [[0.4936170279979706]], [[1.0532571077346802]], [[-0.012135118246078491]], [[-0.06597405672073364]], [[0.43584394454956055]], [[0.6868051290512085]], [[0.29664701223373413]], [[-0.03735211491584778]], [[0.9094505310058594]], [[-0.11535894870758057]], [[0.10341238975524902]], [[0.08915165066719055]], [[-0.07534231245517731]], [[0.7857598066329956]], [[0.4228318929672241]], [[0.2876015305519104]], [[0.6693887114524841]], [[0.7518531680107117]], [[0.716495156288147]], [[0.2525506615638733]], [[0.8949117064476013]], [[0.6580705642700195]], [[0.270244300365448]], [[1.332771897315979]], [[0.0]], [[-0.11315008997917175]], [[-0.16095426678657532]], [[-0.3282749652862549]], [[-0.3177720904350281]], [[-0.9512453079223633]], [[0.8140403032302856]], [[0.3628581762313843]], [[-0.10081475228071213]], [[-0.13354091346263885]], [[-0.3959773778915405]], [[-0.30304503440856934]], [[0.13671272993087769]], [[0.14716848731040955]], [[-0.023441314697265625]], [[-0.8155562877655029]], [[0.06089203059673309]], [[-0.11099424958229065]], [[-0.11645261943340302]], [[0.1342417150735855]], [[0.27262526750564575]], [[-0.17486652731895447]], [[0.058725565671920776]], [[-0.013198643922805786]], [[-0.029461724683642387]], [[0.3461197316646576]], [[-0.21907103061676025]], [[0.3590005934238434]], [[-0.5513098239898682]], [[-0.07925102114677429]], [[-0.03499540686607361]], [[-0.21200641989707947]], [[-0.40096578001976013]], [[0.027870982885360718]], [[1.2655733823776245]], [[0.0]], [[0.6195987462997437]], [[0.4397546648979187]], [[0.6206019520759583]], [[0.5511082410812378]], [[-0.12041990458965302]], [[0.2167436182498932]], [[0.26388221979141235]], [[0.46753495931625366]], [[0.2131124883890152]], [[0.7579471468925476]], [[0.11505678296089172]], [[0.4574028551578522]], [[0.26381179690361023]], [[1.2247061729431152]], [[0.0]], [[-1.0665342807769775]], [[0.045476362109184265]], [[0.2509281039237976]], [[-0.09717515110969543]], [[-0.3647769093513489]], [[0.17794567346572876]], [[0.6081417798995972]], [[0.2575285732746124]], [[1.233847737312317]], [[0.32589927315711975]], [[0.23246663808822632]], [[-0.03795808553695679]], [[-0.21133442223072052]], [[0.0446239709854126]], [[0.9670358300209045]], [[-0.2170550674200058]], [[-0.4938381016254425]], [[0.009092815220355988]], [[0.03114502690732479]], [[0.37417125701904297]], [[-0.03797981142997742]], [[0.316339910030365]], [[-0.14389732480049133]], [[-0.6169514656066895]], [[0.3610243797302246]], [[-0.20511558651924133]], [[1.0811264514923096]], [[-0.18081621825695038]], [[1.250417947769165]], [[0.49274003505706787]], [[0.03662720322608948]], [[-0.4824070334434509]], [[1.193629503250122]], [[0.0]], [[-0.477722704410553]], [[0.2715688645839691]], [[0.1365576982498169]], [[-0.137520432472229]], [[-0.23846754431724548]], [[0.3024696409702301]], [[-0.009005129337310791]], [[0.025821596384048462]], [[-0.28732362389564514]], [[0.10145334899425507]], [[0.35765257477760315]], [[1.2690656185150146]], [[0.2792135179042816]], [[0.6168732643127441]], [[0.006094411015510559]], [[0.010606765747070312]], [[0.27788427472114563]], [[0.080596923828125]], [[0.2661980986595154]], [[0.33626940846443176]], [[0.7717018723487854]], [[0.5359643697738647]], [[-0.1353054642677307]], [[0.1826767921447754]], [[-0.18791207671165466]], [[0.2534499168395996]], [[0.4827831983566284]], [[0.42408043146133423]], [[0.38516494631767273]], [[0.7074828743934631]], [[1.475264549255371]], [[0.7426522970199585]], [[0.6501669883728027]], [[0.24640892446041107]], [[0.026876553893089294]], [[1.1626602411270142]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.9512454271316528]], [[0.8140400648117065]], [[0.48289477825164795]], [[-0.4462251365184784]], [[-0.3902415335178375]], [[-0.29220250248908997]], [[0.5470491051673889]], [[-0.17562851309776306]], [[-0.4606855809688568]], [[0.9291576743125916]], [[0.09512589871883392]], [[0.041118159890174866]], [[0.023333117365837097]], [[-0.5074400901794434]], [[0.26228681206703186]], [[0.09249386191368103]], [[-0.7861419320106506]], [[-0.4112052321434021]], [[1.1219310760498047]], [[0.0]], [[-1.066534161567688]], [[-0.060649096965789795]], [[-0.1412101686000824]], [[0.45173564553260803]], [[0.22625024616718292]], [[-0.14537674188613892]], [[-0.12077603489160538]], [[-0.10308969765901566]], [[0.24199454486370087]], [[-0.08206461369991302]], [[-0.07668575644493103]], [[0.34650862216949463]], [[0.3285548985004425]], [[0.7753244638442993]], [[0.22319620847702026]], [[0.8157265186309814]], [[0.3527522087097168]], [[-0.01645737886428833]], [[0.9578825235366821]], [[0.6055866479873657]], [[1.0691132545471191]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.951904296875]], [[-0.5408797860145569]], [[0.03564499318599701]], [[-0.08889055997133255]], [[0.218109130859375]], [[0.6466209888458252]], [[-0.20620578527450562]], [[-0.09738065302371979]], [[0.11111147701740265]], [[0.9185736179351807]], [[-0.2269565910100937]], [[-0.2830427587032318]], [[-0.011086568236351013]], [[0.04478982090950012]], [[-0.0020026862621307373]], [[-0.15663355588912964]], [[-0.24748863279819489]], [[-0.012040598317980766]], [[0.25658899545669556]], [[-0.2373858094215393]], [[0.27693742513656616]], [[-0.15217554569244385]], [[1.0313160419464111]], [[0.0]], [[0.841008186340332]], [[0.4625747799873352]], [[0.3267335295677185]], [[-0.17210429906845093]], [[0.0006536245346069336]], [[-0.20045703649520874]], [[0.16133716702461243]], [[0.5810999870300293]], [[-0.0607508048415184]], [[0.13231492042541504]], [[1.1951004266738892]], [[1.1713786125183105]], [[1.7948112487792969]], [[0.09552001953125]], [[0.18224959075450897]], [[1.1667845249176025]], [[1.9757903814315796]], [[1.4361343383789062]], [[1.5810317993164062]], [[1.3416630029678345]], [[1.5381611585617065]], [[1.8619341850280762]], [[1.614748477935791]], [[1.555795431137085]], [[0.6784389019012451]], [[0.39245983958244324]], [[0.9443203210830688]], [[0.4579075574874878]], [[1.1042982339859009]], [[0.4539235830307007]], [[1.0020062923431396]], [[0.0]], [[0.25929173827171326]], [[0.3217158615589142]], [[0.2059689611196518]], [[-0.0731818899512291]], [[0.17755374312400818]], [[0.6842074394226074]], [[1.2209028005599976]], [[-0.23265880346298218]], [[-0.039877451956272125]], [[0.10189187526702881]], [[0.11340349912643433]], [[0.6049075126647949]], [[0.5136160254478455]], [[1.6806869506835938]], [[-0.06331774592399597]], [[-0.06369280815124512]], [[0.009570300579071045]], [[0.265577495098114]], [[0.515476405620575]], [[0.5708661079406738]], [[0.9752065539360046]], [[0.0]], [[-0.11315008997917175]], [[-0.16095426678657532]], [[-0.3282749652862549]], [[-0.3177720904350281]], [[-0.6059970259666443]], [[0.23379439115524292]], [[-0.17375582456588745]], [[-0.5775454044342041]], [[-0.14573033154010773]], [[0.5601860880851746]], [[-0.0920972004532814]], [[-0.15653252601623535]], [[-0.5377776622772217]], [[0.09068655967712402]], [[-0.04677492380142212]], [[-0.8411783576011658]], [[-0.5356755256652832]], [[-0.4109466075897217]], [[1.1436131000518799]], [[-0.1095721498131752]], [[-0.6301063895225525]], [[0.005402585491538048]], [[-0.7302320599555969]], [[-0.435308575630188]], [[0.09492063522338867]], [[-0.6398029327392578]], [[-0.058792173862457275]], [[0.18875887989997864]], [[0.8638976216316223]], [[0.1464506834745407]], [[0.07797078788280487]], [[-0.011987768113613129]], [[0.6735248565673828]], [[0.464102178812027]], [[-0.3594246804714203]], [[1.0232906341552734]], [[0.35377761721611023]], [[1.156757116317749]], [[0.16555973887443542]], [[0.923269510269165]], [[0.0]], [[0.1208658367395401]], [[0.544913649559021]], [[0.31716740131378174]], [[0.557453989982605]], [[0.11222371459007263]], [[0.7301934361457825]], [[0.4455357491970062]], [[0.8850244283676147]], [[0.0]], [[-0.11315008997917175]], [[-0.16095426678657532]], [[-0.3282749652862549]], [[-0.3177720904350281]], [[-0.5267140865325928]], [[-0.24728205800056458]], [[0.30088549852371216]], [[-0.41691118478775024]], [[0.1536356806755066]], [[-0.5227096080780029]], [[0.06357777118682861]], [[-0.05645143985748291]], [[-0.6563234925270081]], [[0.11087767779827118]], [[-0.2927623689174652]], [[0.08948126435279846]], [[0.13347390294075012]], [[-0.4103381633758545]], [[-0.23966796696186066]], [[-0.49286818504333496]], [[0.14554645121097565]], [[-0.02507486194372177]], [[-0.11595361679792404]], [[0.06306737661361694]], [[0.350450336933136]], [[0.33532601594924927]], [[0.7476414442062378]], [[0.3212553858757019]], [[0.7309038639068604]], [[0.6681578755378723]], [[0.7557684183120728]], [[0.36439502239227295]], [[0.852126955986023]], [[0.6199363470077515]], [[0.8192452788352966]], [[0.39606794714927673]], [[0.8534550070762634]], [[0.0]], [[0.17617565393447876]], [[0.3006143569946289]], [[1.0840468406677246]], [[0.42816269397735596]], [[-0.23001065850257874]], [[0.076186403632164]], [[0.573780357837677]], [[0.10939885675907135]], [[1.2651972770690918]], [[-0.35290706157684326]], [[0.17054584622383118]], [[0.17193594574928284]], [[-0.03912505507469177]], [[-0.49655207991600037]], [[0.07355949282646179]], [[-0.06730708479881287]], [[-0.8637893199920654]], [[-0.5559397339820862]], [[-0.2834644913673401]], [[-0.3929232656955719]], [[0.8172214031219482]], [[0.0]], [[-0.5498442649841309]], [[0.038965314626693726]], [[-0.18565353751182556]], [[-0.4804988503456116]], [[0.04583503305912018]], [[-0.05773341283202171]], [[0.2818293571472168]], [[-0.6961516737937927]], [[-0.23935475945472717]], [[-0.24900633096694946]], [[0.053448207676410675]], [[-0.3196462392807007]], [[-0.3989003300666809]], [[0.15521755814552307]], [[0.47332799434661865]], [[0.4283027648925781]], [[0.7893853783607483]], [[0.0]], [[0.004776306450366974]], [[0.10140711069107056]], [[-0.2659136950969696]], [[-0.14012113213539124]], [[-0.061852049082517624]], [[-0.025215832516551018]], [[-0.2948322892189026]], [[0.058745626360177994]], [[-0.25471651554107666]], [[-0.7457590103149414]], [[-0.44445478916168213]], [[-0.04439004510641098]], [[-0.3709873855113983]], [[0.33667293190956116]], [[0.8504806756973267]], [[0.5197789072990417]], [[0.29496529698371887]], [[0.4076220393180847]], [[-0.08973857760429382]], [[0.4408044219017029]], [[0.7491947412490845]], [[0.0]], [[-0.32464179396629333]], [[0.09080950170755386]], [[0.8013594150543213]], [[0.26536911725997925]], [[0.14563438296318054]], [[-0.481759250164032]], [[0.9297176599502563]], [[1.1415600776672363]], [[0.9008225202560425]], [[-0.3081783056259155]], [[-0.07532468438148499]], [[0.15707580745220184]], [[0.42506229877471924]], [[0.9827888011932373]], [[0.22856426239013672]], [[1.1658989191055298]], [[-0.11273989081382751]], [[0.68325275182724]], [[0.04394368827342987]], [[0.35009992122650146]], [[-0.1562873125076294]], [[-0.15445160865783691]], [[-0.7956734895706177]], [[-0.40307140350341797]], [[0.21840617060661316]], [[0.6977721452713013]], [[0.0]], [[0.3792824149131775]], [[0.312516450881958]], [[0.13725832104682922]], [[-0.1360434889793396]], [[0.690003514289856]], [[0.4140024185180664]], [[-0.10026367008686066]], [[0.526725172996521]], [[0.049783676862716675]], [[0.5193147659301758]], [[-0.08001425862312317]], [[0.07231675833463669]], [[-0.11137354373931885]], [[0.49651867151260376]], [[0.3792017698287964]], [[-0.3009105920791626]], [[-0.3405873477458954]], [[0.1602591723203659]], [[1.217315912246704]], [[-0.052775055170059204]], [[0.112929567694664]], [[0.567381739616394]], [[0.6506572961807251]], [[0.0]], [[-0.40511399507522583]], [[-0.279362291097641]], [[0.005045694764703512]], [[-0.11217311769723892]], [[-0.07831791043281555]], [[0.21414679288864136]], [[-0.024253305047750473]], [[-0.5728732943534851]], [[-0.14947181940078735]], [[0.3736484944820404]], [[-0.33354270458221436]], [[-0.143775075674057]], [[-0.2842223644256592]], [[0.06173229217529297]], [[0.625378429889679]], [[0.0]], [[0.23560774326324463]], [[0.3641485273838043]], [[0.2185594141483307]], [[-0.16562432050704956]], [[0.6359331011772156]], [[0.48943838477134705]], [[1.2070369720458984]], [[-0.344470739364624]], [[0.9151856899261475]], [[0.4977725148200989]], [[0.36396604776382446]], [[-0.13828492164611816]], [[-0.05920284986495972]], [[-0.3497496247291565]], [[0.0059883128851652145]], [[-0.2593621015548706]], [[1.1629066467285156]], [[-0.07170817255973816]], [[-0.6537858843803406]], [[0.7174879908561707]], [[0.2960403263568878]], [[0.16199971735477448]], [[0.4822959303855896]], [[-0.07597950100898743]], [[0.13241533935070038]], [[0.6937366724014282]], [[0.1583026945590973]], [[0.2971838712692261]], [[-0.06321695446968079]], [[-0.6236497759819031]], [[0.744832456111908]], [[0.13378414511680603]], [[-0.4044124186038971]], [[0.571250855922699]], [[0.0]], [[0.4832516312599182]], [[0.3120678961277008]], [[0.6722205877304077]], [[0.01573517918586731]], [[0.3635942339897156]], [[-0.2619610130786896]], [[0.12198998034000397]], [[0.27021169662475586]], [[0.3734705150127411]], [[-0.20355316996574402]], [[0.11858158558607101]], [[0.15418395400047302]], [[0.42861616611480713]], [[-0.4675939083099365]], [[0.1726704090833664]], [[0.16776365041732788]], [[0.013816863298416138]], [[0.19495977461338043]], [[0.11905607581138611]], [[0.7245342135429382]], [[0.4242269992828369]], [[0.2969725728034973]], [[0.6257931590080261]], [[0.08186394721269608]], [[0.5428549647331238]], [[0.0]], [[-1.066534161567688]], [[-0.027470752596855164]], [[0.02023223042488098]], [[0.2832428812980652]], [[-0.1661612093448639]], [[0.3003716468811035]], [[0.09028451144695282]], [[0.5463014841079712]], [[-0.1765952706336975]], [[0.16703635454177856]], [[0.23983389139175415]], [[0.04534432291984558]], [[-0.21733292937278748]], [[0.007830530405044556]], [[-0.12327161431312561]], [[-0.018457472324371338]], [[-0.059735968708992004]], [[0.6936874985694885]], [[-0.30524754524230957]], [[-0.052205890417099]], [[0.18236179649829865]], [[0.9390937089920044]], [[0.5690449476242065]], [[-0.16113106906414032]], [[0.37003767490386963]], [[0.13030600547790527]], [[0.5042017102241516]], [[0.0]], [[-0.48161235451698303]], [[-0.2100912630558014]], [[-0.16237103939056396]], [[0.4375467002391815]], [[0.9530802965164185]], [[1.0307316780090332]], [[0.6505331993103027]], [[-0.718806266784668]], [[-1.0125994682312012]], [[-0.4785662889480591]], [[-0.2621329128742218]], [[-0.18756574392318726]], [[-0.06822337210178375]], [[0.05464041233062744]], [[-0.07205605506896973]], [[-0.9305738210678101]], [[-0.8737471103668213]], [[-0.3614579439163208]], [[-0.18506217002868652]], [[0.17570851743221283]], [[0.6492153406143188]], [[0.07164302468299866]], [[1.3609260320663452]], [[0.3019299805164337]], [[0.9115879535675049]], [[-0.14665129780769348]], [[-0.18964453041553497]], [[0.2920822501182556]], [[1.1076669692993164]], [[-0.02723061293363571]], [[-0.009927945211529732]], [[0.2724376916885376]], [[-0.016605235636234283]], [[0.2521257996559143]], [[0.3791298270225525]], [[-0.07206478714942932]], [[-0.09873765707015991]], [[0.4807125926017761]], [[0.2820623517036438]], [[0.47195616364479065]], [[0.0]], [[-0.17544572055339813]], [[0.2733195722103119]], [[0.4451581537723541]], [[0.3258211612701416]], [[0.29856815934181213]], [[0.6985082626342773]], [[-0.6888910531997681]], [[-0.5621716380119324]], [[0.23621220886707306]], [[1.166066288948059]], [[0.047495611011981964]], [[0.4421235918998718]], [[0.21080347895622253]], [[0.41997796297073364]], [[0.03256325423717499]], [[1.2336781024932861]], [[0.010496966540813446]], [[0.031646728515625]], [[1.6481130123138428]], [[-0.1054394543170929]], [[-0.531790018081665]], [[0.2184004932641983]], [[0.19224897027015686]], [[0.5261578559875488]], [[0.26697537302970886]], [[0.5719167590141296]], [[0.4387202560901642]], [[0.0047141313552856445]], [[-0.018753409385681152]], [[0.41749870777130127]], [[0.0]], [[0.2148972898721695]], [[0.7232506275177002]], [[0.5792960524559021]], [[0.6523451209068298]], [[0.22035706043243408]], [[-0.0011551082134246826]], [[0.17849117517471313]], [[-0.12109775096178055]], [[-0.0500074177980423]], [[-0.8489388227462769]], [[0.4182172417640686]], [[-0.05180071294307709]], [[0.1629495769739151]], [[0.4356059432029724]], [[0.15110741555690765]], [[-0.12137217819690704]], [[0.31382375955581665]], [[0.4533895254135132]], [[0.3934013247489929]], [[0.0]], [[-0.05799320712685585]], [[-0.331367552280426]], [[1.0020416975021362]], [[0.7800446152687073]], [[1.6334248781204224]], [[0.21250459551811218]], [[0.5212412476539612]], [[0.23311462998390198]], [[0.5834391713142395]], [[0.8387523889541626]], [[0.3291282653808594]], [[0.0]], [[0.7797319889068604]], [[1.017602801322937]], [[0.13884806632995605]], [[1.1198389530181885]], [[0.5993117094039917]], [[0.7687661051750183]], [[1.1000876426696777]], [[0.056057631969451904]], [[0.974460244178772]], [[0.5875706076622009]], [[1.1822750568389893]], [[0.08022516965866089]], [[0.7242574691772461]], [[0.5768629312515259]], [[0.4121384620666504]], [[0.5567482113838196]], [[0.4737825393676758]], [[0.28273990750312805]], [[0.2600680887699127]], [[-0.2586548924446106]], [[0.2640753984451294]], [[1.0298689603805542]], [[0.7003999948501587]], [[0.5642004609107971]], [[0.572128176689148]], [[0.45346152782440186]], [[0.4742504954338074]], [[0.24538099765777588]], [[0.991611123085022]], [[0.5679771304130554]], [[0.3862731158733368]], [[0.30570122599601746]], [[0.0]], [[0.6469317078590393]], [[0.29704180359840393]], [[0.4235386550426483]], [[0.9388135671615601]], [[0.33432090282440186]], [[0.09112077951431274]], [[-0.08869040012359619]], [[0.028676070272922516]], [[0.20619139075279236]], [[0.7360726594924927]], [[0.26318198442459106]], [[-0.00045652687549591064]], [[-0.058565862476825714]], [[0.28314417600631714]], [[0.6208591461181641]], [[0.2516162097454071]], [[0.2673211097717285]], [[0.9433308243751526]], [[1.3288320302963257]], [[0.9956749081611633]], [[1.2256758213043213]], [[1.353161334991455]], [[0.4190270006656647]], [[0.24102485179901123]], [[0.07722058147192001]], [[0.11283927410840988]], [[0.14654460549354553]], [[0.4827733635902405]], [[0.21685682237148285]], [[0.12188848853111267]], [[-0.15584751963615417]], [[-0.2103465497493744]], [[0.2810533344745636]], [[0.0]], [[0.6404546499252319]], [[0.21291089057922363]], [[0.0]], [[0.11139035224914551]], [[0.5972742438316345]], [[0.4084048867225647]], [[0.9233920574188232]], [[0.7839382886886597]], [[0.20572629570960999]], [[0.03350561857223511]], [[0.1825101524591446]], [[0.9035959839820862]], [[0.2991299629211426]], [[0.5059436559677124]], [[0.09572197496891022]], [[0.3923815190792084]], [[0.06568390130996704]], [[-0.10444244742393494]], [[0.35637369751930237]], [[0.2531294822692871]], [[0.29856669902801514]], [[-0.09154900908470154]], [[-0.05449711158871651]], [[0.29790976643562317]], [[0.11144642531871796]], [[0.3687193989753723]], [[-0.04698193073272705]], [[0.1863633096218109]], [[0.20636743307113647]], [[0.0]], [[-0.033051878213882446]], [[-0.5532650351524353]], [[-0.08783045411109924]], [[0.11678460240364075]], [[0.13887467980384827]], [[0.0]], [[-0.49344801902770996]], [[-0.10985884815454483]], [[-0.04238481819629669]], [[0.10777051746845245]], [[-0.696152925491333]], [[-0.1941315084695816]], [[0.08689038455486298]], [[0.13298958539962769]], [[-0.5677254796028137]], [[0.3641160726547241]], [[0.041772983968257904]], [[0.4534333050251007]], [[0.2614859938621521]], [[0.42441707849502563]], [[0.4680454730987549]], [[-0.7227148413658142]], [[0.15444974601268768]], [[0.21816134452819824]], [[0.2677455246448517]], [[-0.48802608251571655]], [[1.448461890220642]], [[0.17564384639263153]], [[0.5748798251152039]], [[-0.14412151277065277]], [[-0.29923462867736816]], [[-0.6146550178527832]], [[-0.1430886685848236]], [[0.12937329709529877]], [[-0.3186572790145874]], [[0.7911422848701477]], [[0.18672582507133484]], [[0.10832475125789642]], [[0.0]], [[-0.876163125038147]], [[0.29864805936813354]], [[0.1926739513874054]], [[-0.18529394268989563]], [[0.18831031024456024]], [[0.018341630697250366]], [[0.06343735754489899]], [[0.21241849660873413]], [[-0.10585269331932068]], [[0.8559372425079346]], [[0.1069406196475029]], [[0.05941423773765564]], [[0.0]], [[0.1757257729768753]], [[-0.44669485092163086]], [[-0.3695310652256012]], [[0.1976485401391983]], [[1.1971485614776611]], [[-0.1813831478357315]], [[0.23132549226284027]], [[0.13807269930839539]], [[-0.008702099323272705]], [[-0.46939903497695923]], [[-0.49465298652648926]], [[-0.3245835602283478]], [[-0.018979087471961975]], [[0.045902177691459656]], [[0.0]], [[-0.2642901539802551]], [[-0.0676809623837471]], [[-0.019894540309906006]], [[0.0]], [[-1.0665342807769775]], [[-0.5215810537338257]], [[0.2393045276403427]], [[0.3681339621543884]], [[0.08874291926622391]], [[-0.05267152190208435]], [[0.0]], [[0.09026524424552917]], [[0.12051939964294434]], [[0.10489250719547272]], [[-0.13401862978935242]], [[1.3728208541870117]], [[-0.32762956619262695]], [[1.087554931640625]], [[0.4714549779891968]], [[-0.2030167132616043]], [[0.2859475016593933]], [[0.0864572823047638]], [[0.3107685148715973]], [[0.06267917156219482]], [[-0.3751671016216278]], [[-0.08684538304805756]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.5190685987472534]], [[0.17712396383285522]], [[0.8602705597877502]], [[-0.45056113600730896]], [[1.3927586078643799]], [[0.03290781378746033]], [[-0.32245033979415894]], [[-0.014170199632644653]], [[-0.4888889193534851]], [[1.1766279935836792]], [[0.5055966377258301]], [[1.0898205041885376]], [[-0.0117886271327734]], [[-0.04078330099582672]], [[0.10741040110588074]], [[0.04897615313529968]], [[-0.7636213898658752]], [[-0.19351352751255035]], [[-0.3559611439704895]], [[-0.10071312636137009]], [[0.11223437637090683]], [[0.9526091814041138]], [[-0.46735528111457825]], [[-0.1003192663192749]], [[-0.14125077426433563]], [[0.0]], [[-1.0665342807769775]], [[-0.2957737147808075]], [[0.132161945104599]], [[-0.15377062559127808]], [[0.0]], [[0.4832516312599182]], [[-0.6376005411148071]], [[-0.7043893337249756]], [[-0.18852370977401733]], [[-0.2973693609237671]], [[-0.17852070927619934]], [[-0.029672563076019287]], [[-0.15246029198169708]], [[0.11911699175834656]], [[-0.2419472634792328]], [[-0.2768283784389496]], [[0.2343151569366455]], [[-0.5256567597389221]], [[-0.09630903601646423]], [[-0.435665488243103]], [[-0.2521120011806488]], [[0.07137332856655121]], [[-0.21597644686698914]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.3038456439971924]], [[-0.13471880555152893]], [[-0.2448147088289261]], [[-0.3982992172241211]], [[0.7745176553726196]], [[-0.22301633656024933]], [[-0.21912147104740143]], [[0.990646481513977]], [[-0.29159796237945557]], [[-0.2199884057044983]], [[1.0960185527801514]], [[-0.021605879068374634]], [[0.30457741022109985]], [[0.36894890666007996]], [[0.1373639702796936]], [[-0.6925634145736694]], [[0.09793345630168915]], [[-0.09566376358270645]], [[-0.1723560094833374]], [[-0.25491148233413696]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.3063705265522003]], [[-0.2762855887413025]], [[0.8800303339958191]], [[-0.12954315543174744]], [[-0.2628466486930847]], [[0.09855440258979797]], [[1.052108645439148]], [[-0.01922350376844406]], [[-0.426902174949646]], [[0.1567646563053131]], [[0.00279352068901062]], [[-0.7715184092521667]], [[0.0750107616186142]], [[-0.01807367615401745]], [[-0.1587713658809662]], [[-0.33287644386291504]], [[-0.3333529233932495]], [[-0.09575924277305603]], [[-0.26076239347457886]], [[0.0]], [[0.30482298135757446]], [[0.7735462188720703]], [[0.4696606993675232]], [[1.0825275182724]], [[1.181837558746338]], [[0.45805442333221436]], [[-0.16012215614318848]], [[0.03712579607963562]], [[0.04128682613372803]], [[-0.27776384353637695]], [[0.3141905665397644]], [[0.508804202079773]], [[-0.7423726916313171]], [[-0.006258223205804825]], [[-0.34488892555236816]], [[-0.20689812302589417]], [[0.02739184908568859]], [[-0.3107442855834961]], [[0.21742326021194458]], [[0.18332859873771667]], [[-0.05941791087388992]], [[-0.29925069212913513]], [[0.0]], [[-0.0689803957939148]], [[-0.2641478180885315]], [[-0.09434258937835693]], [[-0.03115895390510559]], [[-0.24360385537147522]], [[-0.37487131357192993]], [[0.0]], [[0.4832516312599182]], [[-0.6376005411148071]], [[-0.6106313467025757]], [[-0.4782004952430725]], [[-0.6040186882019043]], [[-0.029840052127838135]], [[-0.12123461067676544]], [[-0.3336063325405121]], [[-0.10240373015403748]], [[-0.25103428959846497]], [[0.31647586822509766]], [[-0.5173830986022949]], [[-0.4095550775527954]], [[-0.18233564496040344]], [[-0.09213888645172119]], [[0.03072439879179001]], [[-0.12732505798339844]], [[1.4617459774017334]], [[-0.23803552985191345]], [[0.033588990569114685]], [[0.5841862559318542]], [[-0.38804373145103455]], [[0.0]], [[0.8658504486083984]], [[0.3764512836933136]], [[0.6865967512130737]], [[-0.334919810295105]], [[0.12660831212997437]], [[0.09244274348020554]], [[0.7169530391693115]], [[0.4261881113052368]], [[0.08679790794849396]], [[0.6157479286193848]], [[0.2366890013217926]], [[0.5717321038246155]], [[-0.03278525546193123]], [[-0.38656705617904663]], [[-0.11686864495277405]], [[1.015868067741394]], [[-0.12556950747966766]], [[0.07509274035692215]], [[0.3898402750492096]], [[0.20625221729278564]], [[1.1012649536132812]], [[0.11077897995710373]], [[1.219609022140503]], [[0.030993342399597168]], [[0.031679511070251465]], [[-0.2713131606578827]], [[0.4462239146232605]], [[0.00635518878698349]], [[-0.48417994379997253]], [[0.42261338233947754]], [[0.05972564220428467]], [[1.2564574480056763]], [[0.6955441236495972]], [[0.30133843421936035]], [[0.2513887286186218]], [[0.05569508671760559]], [[-0.41913092136383057]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.9302722215652466]], [[0.5999708771705627]], [[0.04248502850532532]], [[0.6293555498123169]], [[-0.28096428513526917]], [[-0.29654330015182495]], [[0.5220077633857727]], [[0.11819185316562653]], [[0.12384214997291565]], [[-0.051941901445388794]], [[-0.8026816248893738]], [[-0.24301233887672424]], [[0.058833640068769455]], [[0.5688701272010803]], [[-0.39902210235595703]], [[-0.48889708518981934]], [[0.0]], [[-0.11315010488033295]], [[-0.1609543263912201]], [[-0.3282749652862549]], [[-0.31777238845825195]], [[-0.9512451887130737]], [[0.8140400648117065]], [[0.09425622224807739]], [[-0.05171952396631241]], [[-0.10646263509988785]], [[0.5676667094230652]], [[-0.527319073677063]], [[0.0]], [[-0.11315010488033295]], [[-0.1609543263912201]], [[-0.3282749652862549]], [[-0.31777238845825195]], [[-0.6856017708778381]], [[0.04602855443954468]], [[-0.14865481853485107]], [[-0.17993083596229553]], [[-0.09935116767883301]], [[-0.5138869881629944]], [[-0.5378283858299255]], [[0.0]], [[0.48325157165527344]], [[-0.6376004815101624]], [[-0.6106318235397339]], [[-0.4782007038593292]], [[-0.604019045829773]], [[0.0]], [[0.5754234790802002]], [[0.16264870762825012]], [[-0.025549083948135376]], [[0.061585187911987305]], [[-0.07242476940155029]], [[-0.2587640583515167]], [[-0.556729793548584]], [[-0.04973505437374115]], [[-0.6641082763671875]], [[0.6475027799606323]], [[-0.21925698220729828]], [[0.09447683393955231]], [[0.3079664707183838]], [[-0.17673443257808685]], [[0.1304253786802292]], [[0.20473074913024902]], [[0.005112707614898682]], [[-0.49369877576828003]], [[-0.039500996470451355]], [[0.26003286242485046]], [[-0.15032508969306946]], [[-0.3380119800567627]], [[0.05170126259326935]], [[0.677602231502533]], [[-0.229771688580513]], [[0.21479587256908417]], [[0.5631070137023926]], [[0.4280635118484497]], [[-0.007505424320697784]], [[-0.40160608291625977]], [[0.6788790822029114]], [[0.4560697078704834]], [[0.18035095930099487]], [[0.21402938663959503]], [[0.2712092399597168]], [[-0.1944757103919983]], [[-0.627436101436615]], [[0.0]], [[-0.1916348934173584]], [[-0.33909299969673157]], [[0.6272395253181458]], [[-0.5635571479797363]], [[-0.5866705775260925]], [[0.2791180908679962]], [[0.28444570302963257]], [[1.2913734912872314]], [[-0.3116130828857422]], [[0.013966217637062073]], [[0.2000589370727539]], [[0.08221952617168427]], [[-0.10300754755735397]], [[0.08420887589454651]], [[0.8432508707046509]], [[-0.23928968608379364]], [[0.1848287582397461]], [[0.32064035534858704]], [[-0.41145703196525574]], [[-0.2982003092765808]], [[-0.061122626066207886]], [[-0.6837244033813477]], [[0.0]], [[-0.43540483713150024]], [[0.4454203248023987]], [[-0.36783862113952637]], [[0.3157363533973694]], [[0.24869772791862488]], [[-0.06743203103542328]], [[1.3153772354125977]], [[-0.2525760531425476]], [[-0.01068039983510971]], [[0.07770592719316483]], [[0.10859391838312149]], [[0.029238075017929077]], [[0.03419896960258484]], [[-0.14817003905773163]], [[-0.11431144922971725]], [[0.8004702925682068]], [[0.4796609878540039]], [[-0.4356316924095154]], [[-0.29012352228164673]], [[-0.14012715220451355]], [[-0.7171099185943604]], [[0.0]], [[-0.2441106140613556]], [[0.3646446466445923]], [[0.1878792941570282]], [[-0.03247939050197601]], [[0.5053139925003052]], [[0.29729020595550537]], [[-0.1004885733127594]], [[0.7198711633682251]], [[0.927932858467102]], [[0.781240701675415]], [[0.08733238279819489]], [[-0.4620768427848816]], [[0.2640955448150635]], [[1.0530664920806885]], [[-0.5045868158340454]], [[0.5445475578308105]], [[0.6300709247589111]], [[0.19882932305335999]], [[0.031028151512145996]], [[-0.22785794734954834]], [[0.019913658499717712]], [[-0.11850041151046753]], [[0.6500064134597778]], [[-0.013447344303131104]], [[-0.2073274701833725]], [[-0.38703298568725586]], [[0.12016487121582031]], [[0.6201364994049072]], [[0.4280206561088562]], [[0.5512944459915161]], [[0.3229461908340454]], [[-0.057713210582733154]], [[-0.7604158520698547]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.9512454271316528]], [[0.2119961977005005]], [[-0.5528934001922607]], [[0.15955916047096252]], [[-0.5078181028366089]], [[-0.3778151273727417]], [[0.48542606830596924]], [[-0.13225340843200684]], [[0.9643393158912659]], [[-0.14330489933490753]], [[-0.3781315088272095]], [[0.5797222852706909]], [[0.12295563519001007]], [[-0.44835981726646423]], [[0.05143122375011444]], [[0.049333661794662476]], [[-0.015877634286880493]], [[-0.7726005911827087]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.9512454271316528]], [[0.8140400648117065]], [[-0.25039392709732056]], [[0.15312844514846802]], [[-0.2589261531829834]], [[-0.21403178572654724]], [[-0.09754355996847153]], [[0.18758079409599304]], [[-0.0744859129190445]], [[-0.22102589905261993]], [[-0.15344175696372986]], [[0.9329989552497864]], [[-0.4661511480808258]], [[0.21959248185157776]], [[0.16997681558132172]], [[0.09013441205024719]], [[0.027352124452590942]], [[-0.7458037734031677]], [[-0.8378294706344604]], [[0.0]], [[1.241324782371521]], [[0.7014296054840088]], [[0.5596684217453003]], [[0.21020165085792542]], [[-0.8520441055297852]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.9512454271316528]], [[-0.21725915372371674]], [[-0.4660860598087311]], [[-0.0746268779039383]], [[-0.12038598209619522]], [[-0.41526806354522705]], [[1.0915946960449219]], [[-0.01566436141729355]], [[-0.24185998737812042]], [[-0.3568902313709259]], [[0.12599289417266846]], [[0.3043869435787201]], [[0.01223975419998169]], [[-0.22207708656787872]], [[-0.282082200050354]], [[1.1959686279296875]], [[-0.32948678731918335]], [[0.02581612765789032]], [[0.044028908014297485]], [[0.01561465859413147]], [[-0.8284673094749451]], [[-0.902449369430542]], [[0.0]], [[0.06646053493022919]], [[-0.15605011582374573]], [[-0.27392396330833435]], [[-0.5432469248771667]], [[-0.5637046098709106]], [[-0.47126874327659607]], [[0.09774360060691833]], [[-0.09304067492485046]], [[-0.601668119430542]], [[-0.2071971893310547]], [[-0.6337866187095642]], [[-0.11195410043001175]], [[0.12074890732765198]], [[-0.07962116599082947]], [[-1.0615085363388062]], [[-0.11757148802280426]], [[-0.5059780478477478]], [[-0.005359485745429993]], [[0.09342601895332336]], [[-0.06576231122016907]], [[-0.9264401197433472]], [[0.08340732008218765]], [[-0.43146806955337524]], [[0.1303216814994812]], [[-0.03431764245033264]], [[-0.9472019076347351]], [[0.0]], [[-0.9829387068748474]], [[0.0]], [[-0.11315028369426727]], [[-0.16095438599586487]], [[-0.32827508449554443]], [[-0.3177723288536072]], [[-1.0033020973205566]], [[0.0]], [[-1.0665342807769775]], [[0.0]], [[-0.19736911356449127]], [[-1.104034662246704]], [[0.0]], [[-0.34796926379203796]], [[-0.22989946603775024]], [[-0.3010019063949585]], [[-1.123521089553833]], [[0.0]], [[-1.167125940322876]], [[-0.1531175971031189]], [[-0.003114759922027588]], [[-0.1487293243408203]], [[-0.9422581195831299]], [[-1.162879467010498]], [[0.0]], [[-0.0942484438419342]], [[0.2524263560771942]], [[0.04757276177406311]], [[-0.2218065857887268]], [[-0.22016501426696777]], [[0.2381110042333603]], [[0.49154117703437805]], [[-0.23176664113998413]], [[0.014932923018932343]], [[0.20574456453323364]], [[0.05880388617515564]], [[0.05663985013961792]], [[0.09313377737998962]], [[0.2502615749835968]], [[0.25563809275627136]], [[0.3711467981338501]], [[-0.05284196138381958]], [[0.3666977286338806]], [[0.5287909507751465]], [[0.41367292404174805]], [[0.16226908564567566]], [[-1.2129067182540894]], [[0.0]], [[0.08458003401756287]], [[0.42231953144073486]], [[0.418099045753479]], [[0.667856752872467]], [[0.46536558866500854]], [[0.447201132774353]], [[0.17339403927326202]], [[-0.18910042941570282]], [[-0.10862983018159866]], [[0.09504872560501099]], [[0.012378513813018799]], [[-1.2517427206039429]], [[0.0]], [[-1.2972172498703003]], [[0.0]], [[-0.4780730903148651]], [[0.06099838763475418]], [[-0.3467648923397064]], [[0.07827728986740112]], [[0.12090984731912613]], [[-0.3882978558540344]], [[-0.19174258410930634]], [[1.2729310989379883]], [[0.32328999042510986]], [[-0.050434187054634094]], [[0.4855327010154724]], [[0.3229160010814667]], [[0.09424278140068054]], [[0.4921891689300537]], [[0.03540109843015671]], [[0.38152992725372314]], [[0.4638485610485077]], [[-0.10253235697746277]], [[-0.033400505781173706]], [[-1.3218600749969482]], [[0.0]], [[-0.7644290924072266]], [[0.13238152861595154]], [[-0.03178662061691284]], [[0.2324785590171814]], [[-0.38034865260124207]], [[-0.047109752893447876]], [[-1.4462486505508423]], [[0.03486558794975281]], [[-0.2701641917228699]], [[0.14416125416755676]], [[0.044117048382759094]], [[-0.034205883741378784]], [[-1.455430030822754]], [[0.045743316411972046]], [[0.028347350656986237]], [[0.5249671339988708]], [[0.040274858474731445]], [[-0.04505467414855957]], [[0.12237086892127991]], [[-1.37270987033844]], [[0.0]], [[0.10044354200363159]], [[0.6518823504447937]], [[-0.12838757038116455]], [[0.21294242143630981]], [[-0.10604408383369446]], [[-0.7374320030212402]], [[0.37629950046539307]], [[0.2061893343925476]], [[0.07352998852729797]], [[0.2663666307926178]], [[0.11588333547115326]], [[0.10327041149139404]], [[0.13687780499458313]], [[0.0595514252781868]], [[-0.009213000535964966]], [[-0.07473649084568024]], [[0.13482141494750977]], [[0.07218138873577118]], [[0.2828890085220337]], [[0.5989515781402588]], [[0.6309226155281067]], [[-0.1210927963256836]], [[-0.053398698568344116]], [[-0.3524458408355713]], [[-0.08092346787452698]], [[-0.23167894780635834]], [[-0.5303452014923096]], [[-1.4186824560165405]], [[0.0]], [[-0.7644293308258057]], [[0.13238176703453064]], [[-0.03178660571575165]], [[0.23247864842414856]], [[-0.3803483247756958]], [[-0.04711031913757324]], [[-1.4462485313415527]], [[0.0]], [[-1.4740253686904907]], [[0.0]], [[-0.1294383406639099]], [[-0.9234673976898193]], [[-0.29208123683929443]], [[0.3168879449367523]], [[-0.2587331533432007]], [[-0.3450998067855835]], [[-0.9210726022720337]], [[-0.28276610374450684]], [[-0.39471161365509033]], [[-0.28002044558525085]], [[0.6951402425765991]], [[-0.12371709942817688]], [[-0.18290072679519653]], [[-0.31371670961380005]], [[-1.5445067882537842]], [[0.0]], [[-0.04949557036161423]], [[0.0677909255027771]], [[-0.17038728296756744]], [[-0.1645030379295349]], [[-0.21819227933883667]], [[-1.6618385314941406]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fecd44b2f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import interp_utils\n",
    "import importlib\n",
    "importlib.reload(interp_utils)\n",
    "\n",
    "feature = 69\n",
    "text_list, full_text, token_list, full_token_list, indices = get_feature_datapoints_with_idx(feature, model_activations[LAYER], model.tokenizer, token_amount, dataset, setting=\"uniform\", k=100)\n",
    "interp_utils.visualize_text(text_list, feature, model, None, layer=LAYER, setting=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "We can maybe do a basic K-means on just **layers**'s activations. We use this to get a sense for \"neuron's which fire together, wire together\".\n",
    "Maybe we do not have to use K-Means but can instead build some sort of relationship graph.\n",
    "\n",
    "### Then:\n",
    "- We find the top sentences which activate the clustered neurons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redifine Params for the $``\\text{Idea}\"$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURON = 69\n",
    "LAYER = 0\n",
    "N_RELATED = 10\n",
    "N_CLUSTER_EXAMPLES = 5_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup correlations and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        , -0.30844496,  0.30783449,  0.27606701, -0.23459918,\n",
       "       -0.19587211, -0.19140962, -0.18078335,  0.17494178, -0.17259253])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compute the correlations\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assuming 'activations' is your numpy array of shape (num_samples, num_neurons)\n",
    "# where each row is an activation vector for all neurons\n",
    "\n",
    "def compute_correlations(activations, neuron=NEURON):\n",
    "    num_neurons = activations.shape[1]\n",
    "    neur_acts = activations[:, neuron]\n",
    "    corr_list = np.zeros(num_neurons)\n",
    "    for i in range(num_neurons):\n",
    "      corr, _ = pearsonr(activations[:, i], neur_acts)\n",
    "      corr_list[i] = corr\n",
    "    return corr_list\n",
    "\n",
    "neuron_corrs = compute_correlations(model_activations[LAYER].reshape(-1, model_activations[LAYER].shape[-1]))\n",
    "top_correlated_neurons = np.argsort(np.abs(neuron_corrs))[::-1][:N_RELATED]\n",
    "neuron_corrs[top_correlated_neurons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at constructive interference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, [10, 76, 264, 303, 460, 506])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_related_datapoints(correlated_indices: list[int], layer=LAYER, n_examples=100):\n",
    "\t# TODO: np.abs(*)?\n",
    "\tfeature_activations = np.abs(model_activations[layer][:, :, correlated_indices])\n",
    "\tsummed_along_sentence = feature_activations.sum(axis=1)\n",
    "\tweighted = summed_along_sentence * neuron_corrs[correlated_indices]\n",
    "\tsummed_along_features = weighted.sum(axis=1)\n",
    "\t# Find the input data-points that most activate the neuron\n",
    "\tfound_indices = np.argsort(summed_along_features)[::-1][:n_examples]\n",
    "\treturn found_indices\n",
    "\n",
    "\n",
    "def get_relevant_neurons(correlated_indices: list[int], layer=LAYER, n_examples=100, weight_cutoff=1.2): # TODO: check weight cutoff vis a vis using quantified models\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\tfound_indices = get_top_related_datapoints(correlated_indices, layer=layer, n_examples=n_examples)\n",
    "\n",
    "\tdef get_activated_neurons(layer: int):\n",
    "\t\tneurons = set()\n",
    "\t\tfor i in found_indices:\n",
    "\t\t\tcutoff_n = model_activations[layer][i, :, :] > weight_cutoff\n",
    "\t\t\t_pos_nonzero, neuron_nonzero = np.nonzero(cutoff_n)\n",
    "\t\t\t# print(\"LEN\", neuron_nonzero.shape)\n",
    "\t\t\tneurons.update(neuron_nonzero)\n",
    "\t\treturn list(neurons)\n",
    "\t\n",
    "\tother_layer_neurons = []\n",
    "\tfor i in range(n_layers):\n",
    "\t\t# if i != layer:\n",
    "\t\tother_layer_neurons.append((i, get_activated_neurons(i)))\n",
    "\treturn other_layer_neurons\n",
    "\n",
    "other_neurons = get_relevant_neurons(correlated_indices=top_correlated_neurons, layer=LAYER, n_examples=N_CLUSTER_EXAMPLES, weight_cutoff=2)\n",
    "len(other_neurons[4][1]), [len(i[1]) for i in other_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5} dict_keys([0, 1, 2, 3, 4, 5])\n",
      "[0, 10, 86, 350, 653, 1113]\n",
      "(5000, 1113)\n",
      "(5000,)\n",
      "{0: (0, 10), 1: (10, 86), 2: (86, 350), 3: (350, 653)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_to_n_layers = 4\n",
    "\n",
    "def get_auxiliary_data(correlated_indices: list[int], layer: int, n_examples=N_CLUSTER_EXAMPLES):\n",
    "\t# TODO: this should be a different function!! (UTILS)\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\t# TODO: abs\n",
    "\t# best_feature_activations = np.abs(model_activations[layer][:, :, correlated_indices])\n",
    "\t# # TODO: weight by correlation?!\n",
    "\t# summed_along_sentence_and_correlates = best_feature_activations.sum(axis=1).sum(axis=1)\n",
    "\t# # Find the input data-points that most activate the neuron\n",
    "\t# found_indices = np.argsort(summed_along_sentence_and_correlates)[::-1][:n_examples]\n",
    "\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\tfound_indices = get_top_related_datapoints(correlated_indices, layer=layer, n_examples=n_examples)\n",
    "\n",
    "\tlayer_idx = {i: layer for i, layer in enumerate(\n",
    "\t\tlist({layer for i, (layer, _) in enumerate(other_neurons)})\n",
    "\t)}\n",
    "\n",
    "\tprint(layer_idx, layer_idx.keys())\n",
    "\tlayer_dividers_list = []\n",
    "\ton_layer = -1\n",
    "\n",
    "\ttotal_other_neurons = 0\n",
    "\tfor other_neur in other_neurons:\n",
    "\t\tother_layer, neurons = other_neur\n",
    "\t\tif other_layer <= go_to_n_layers:\n",
    "\t\t\tif other_layer != on_layer:\n",
    "\t\t\t\ton_layer = other_layer\n",
    "\t\t\t\tlayer_dividers_list.append(total_other_neurons)\n",
    "\t\t\ttotal_other_neurons += len(neurons)\n",
    "\tlayer_dividers_list.append(total_other_neurons)\n",
    "\n",
    "\tlayer_dividers = {}\n",
    "\tprint(layer_dividers_list)\n",
    "\t# TODO: with go_to_n_layers there is going to be some overflow problems\n",
    "\tfor i in range(go_to_n_layers):\n",
    "\t\tlayer_dividers[layer_idx[i]] = (layer_dividers_list[i],  layer_dividers_list[i+1])\n",
    "\t\t\t\n",
    "\t# sum([len(i[1]) for i in other_neurons])\n",
    "\n",
    "\n",
    "\tconcatenated = np.zeros((len(found_indices), total_other_neurons))\n",
    "\n",
    "\tcounter = 0\n",
    "\tfor i, other_neur in enumerate(other_neurons):\n",
    "\t\tother_layer, neurons = other_neur\n",
    "\t\tif other_layer <= go_to_n_layers:\n",
    "\t\t\tr = model_activations[other_layer][:, :, neurons][found_indices].sum(axis=1) # Sum over the entire sentence/ text input\n",
    "\t\t\tconcatenated[:, counter:counter+len(neurons)] = r\n",
    "\t\t\tcounter += len(neurons)\n",
    "\t\t\n",
    "\treturn concatenated, found_indices, layer_dividers\n",
    "\n",
    "\n",
    "aux_data, datapoints_used, layer_dividers  = get_auxiliary_data(correlated_indices=top_correlated_neurons, layer=0)\n",
    "# TODO: CONSIDER ONLY USING THE CLOSER LAYERS...\n",
    "print(aux_data.shape), print(datapoints_used.shape), print(layer_dividers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gaussian_mixture_model():\n",
    "\t# TODO:\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with iteration 0\n",
      "Done with iteration 1\n",
      "Done with iteration 2\n",
      "Done with iteration 3\n",
      "Done with iteration 4\n",
      "Done with iteration 5\n",
      "Done with iteration 6\n",
      "Done with iteration 7\n",
      "Done with iteration 8\n",
      "Done with iteration 9\n",
      "Done with iteration 10\n",
      "Done with iteration 11\n",
      "Done with iteration 12\n",
      "Done with iteration 13\n",
      "Done with iteration 14\n",
      "Done with iteration 15\n",
      "Done with iteration 16\n",
      "Done with iteration 17\n",
      "Done with iteration 18\n",
      "Done with iteration 19\n",
      "Done with iteration 20\n",
      "Done with iteration 21\n",
      "Done with iteration 22\n",
      "Done with iteration 23\n",
      "Done with iteration 24\n",
      "Done with iteration 25\n",
      "Done with iteration 26\n",
      "Done with iteration 27\n",
      "Done with iteration 28\n",
      "Done with iteration 29\n",
      "Done with iteration 30\n",
      "Done with iteration 31\n",
      "Done with iteration 32\n",
      "Done with iteration 33\n",
      "Done with iteration 34\n",
      "Done with iteration 35\n",
      "Done with iteration 36\n",
      "Done with iteration 37\n",
      "Done with iteration 38\n",
      "Done with iteration 39\n",
      "Done with iteration 40\n",
      "Done with iteration 41\n",
      "Done with iteration 42\n",
      "Done with iteration 43\n",
      "Done with iteration 44\n",
      "Done with iteration 45\n",
      "Done with iteration 46\n",
      "Done with iteration 47\n",
      "Done with iteration 48\n",
      "Done with iteration 49\n",
      "Done with iteration 50\n",
      "Done with iteration 51\n",
      "Done with iteration 52\n",
      "Done with iteration 53\n",
      "Done with iteration 54\n",
      "Done with iteration 55\n",
      "Done with iteration 56\n",
      "Done with iteration 57\n",
      "Done with iteration 58\n",
      "Done with iteration 59\n",
      "Done with iteration 60\n",
      "Done with iteration 61\n",
      "Done with iteration 62\n",
      "Done with iteration 63\n",
      "Done with iteration 64\n",
      "Done with iteration 65\n",
      "Done with iteration 66\n",
      "Done with iteration 67\n",
      "Done with iteration 68\n",
      "Done with iteration 69\n",
      "Done with iteration 70\n",
      "Done with iteration 71\n",
      "Done with iteration 72\n",
      "Done with iteration 73\n",
      "Done with iteration 74\n",
      "Done with iteration 75\n",
      "Done with iteration 76\n",
      "Done with iteration 77\n",
      "Done with iteration 78\n",
      "Done with iteration 79\n",
      "Done with iteration 80\n",
      "Done with iteration 81\n",
      "Done with iteration 82\n",
      "Done with iteration 83\n",
      "Done with iteration 84\n",
      "Done with iteration 85\n",
      "Done with iteration 86\n",
      "Done with iteration 87\n",
      "Done with iteration 88\n",
      "Done with iteration 89\n",
      "Done with iteration 90\n",
      "Done with iteration 91\n",
      "Done with iteration 92\n",
      "Done with iteration 93\n",
      "Done with iteration 94\n",
      "Done with iteration 95\n",
      "Done with iteration 96\n",
      "Done with iteration 97\n",
      "Done with iteration 98\n",
      "Done with iteration 99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# Cosine similarity function\n",
    "# TODO: yay or no\n",
    "\n",
    "def create_per_layer_weighting():\n",
    "    weighting = np.ones(aux_data.shape[-1])\n",
    "    for curr_layer in layer_dividers:\n",
    "        start, stop = layer_dividers[curr_layer]\n",
    "        # The further from the current layer, the less important\n",
    "        weighting[start:stop] = weighting[start:stop] * (2 ** (-1 * (abs(LAYER - curr_layer))))\n",
    "        # TODO: not just per neuron\n",
    "    return weighting\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def weighted_dot_similarity(weighting, a, b, cutoff=0.2):\n",
    "    a = a * (np.abs(a) > cutoff)\n",
    "    b = b * (np.abs(b) > cutoff)\n",
    "    ab = a * b \n",
    "    weighted = ab * weighting\n",
    "    return np.sum(weighted)\n",
    "    # return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# KMeans with cosine similarity\n",
    "\n",
    "\n",
    "def kmeans_cosine(X, n_clusters: int, iterations=100):\n",
    "    k = n_clusters\n",
    "    # Normalize input data\n",
    "    # TODO: why are we normalizing??\n",
    "    X_normalized = normalize(X, axis=1)\n",
    "    weighting = create_per_layer_weighting()\n",
    "\n",
    "    # Randomly initialize centroids\n",
    "    n_samples, n_features = X_normalized.shape\n",
    "    centroids = X_normalized[np.random.choice(n_samples, k, replace=False)]\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        # Cluster assignment step\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for idx, x in enumerate(X_normalized):\n",
    "            similarities = [weighted_dot_similarity(weighting,\n",
    "                x, centroid) for centroid in centroids]\n",
    "            # similarities = [np.dot(x, centroid) for centroid in centroids]\n",
    "            closest = np.argmax(similarities)\n",
    "            clusters[closest].append(idx)\n",
    "\n",
    "        # Update centroids\n",
    "        # TODO: we maybe able to just **not use** PCA at all here.... slow it may be\n",
    "        new_centroids = []\n",
    "        for cluster in clusters:\n",
    "            if cluster:  # Check if cluster is not empty\n",
    "                new_centroid = np.mean(X_normalized[cluster], axis=0)\n",
    "                new_centroids.append(new_centroid)\n",
    "            else:\n",
    "                # Reinitialize empty clusters\n",
    "                new_centroids.append(np.random.rand(n_features))\n",
    "\n",
    "        new_centroids = np.array(new_centroids)\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        print(\"Done with iteration\", iter)\n",
    "\n",
    "    return centroids, clusters\n",
    "\n",
    "\n",
    "# TODO: no function. Just on global so we can stop middway etc etc\n",
    "# TODO: can we speed this up??? Maybe we use PCA\n",
    "_, cluster_by_idx = kmeans_cosine(aux_data, iterations=100, n_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 23, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 4967]\n",
      "2\n",
      "9\n",
      "13\n",
      "24\n",
      "25\n",
      "27\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# TODO: GMMs\n",
    "print([len(c) for c in cluster_by_idx])\n",
    "for i, c in enumerate(cluster_by_idx):\n",
    "\tif len(c) > 0:\n",
    "\t\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_text_per_cluster(text, cluster_inds: list[int], feature, model, autoencoder, layer, setting=\"dictionary_basis\", max_activation = None):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    if isinstance(feature, int):\n",
    "        feature = [feature]\n",
    "    display_text_list = []\n",
    "    act_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            token = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t are tokens\n",
    "            token = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        for f in feature:\n",
    "            display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            # TODO: right here. MASK by whether a token is in cluster or not\n",
    "            act_list += get_neuron_activation(token, f, model, autoencoder, layer, setting) + [0.0]\n",
    "    act_list = torch.tensor(act_list).reshape(-1,1,1)\n",
    "    if(max_activation is not None):\n",
    "        act_list = torch.clamp(act_list, max=max_activation)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=act_list)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 - 5)*(-u**3 + 0*u**3 + 0*u**3) + (u**3 + 0*u**3 - 2*u**3)*(2*u - 2*u + u) as j*u**2 + v*u + d*u**3\n",
      "(2, 40, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ef05b4db-a2cd\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ef05b4db-a2cd\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"5\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \" Trump\", \" over\", \" rally\", \" remarks\", \":\", \" '\", \"This\", \" is\", \" my\", \" country\", \"'\", \" Pelosi\", \":\", \" Trump\", \" hur\", \"rying\", \" to\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \"\\n\", \"5\", \" -\", \" 5\", \"\\n\", \"5\", \" -\", \" 5\", \")*(-\", \"u\", \"**\", \"3\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \" Trump\", \" over\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \" Trump\", \" over\", \" rally\", \"\\n\", \"5\", \" -\", \" 5\", \")*(-\", \"u\", \"**\", \"3\", \" +\", \" 0\", \"*\", \"u\", \"**\", \"3\", \" +\", \" 0\", \"*\", \"u\", \"**\", \"3\", \")\", \" +\", \" (\", \"u\", \"**\", \"3\", \" +\", \" 0\", \"*\", \"u\", \"**\", \"3\", \" -\", \" 2\", \"*\", \"u\", \"**\", \"3\", \")*(\", \"2\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \"\\n\", \"Four\", \" of\", \" out\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \" Trump\", \" over\", \" rally\", \" remarks\", \":\", \" '\", \"\\n\", \"5\", \" -\", \" 5\", \")*(-\", \"u\", \"**\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \" Trump\", \"\\n\", \"5\", \" -\", \" 5\", \")*(-\", \"u\", \"**\", \"3\", \" +\", \" 0\", \"*\", \"u\", \"**\", \"3\", \" +\", \" 0\", \"*\", \"u\", \"**\", \"3\", \")\", \" +\", \" (\", \"u\", \"**\", \"\\n\", \"5\", \" -\", \" 5\", \")*(-\", \"u\", \"**\", \"3\", \" +\", \" 0\", \"*\", \"u\", \"**\", \"3\", \" +\", \" 0\", \"*\", \"u\", \"**\", \"3\", \")\", \" +\", \" (\", \"u\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \" Trump\", \" over\", \" rally\", \" remarks\", \":\", \" '\", \"This\", \" is\", \" my\", \" country\", \"'\", \" Pelosi\", \":\", \" Trump\", \" hur\", \"\\n\", \"Four\", \"\\n\", \"Four\", \" of\", \" out\", \" every\", \" 10\", \" Americans\", \" believe\", \" President\", \" Trump\", \" Donald\", \" John\", \" Trump\", \"O\", \"mar\", \" fires\", \" back\", \" at\", \" Trump\", \" over\", \" rally\", \" remarks\", \":\", \" '\", \"This\", \" is\", \" my\", \"\\n\"], \"activations\": [[[2.153202772140503]], [[0.0]], [[-0.12172824144363403]], [[0.7179699540138245]], [[0.5863940715789795]], [[0.1945357620716095]], [[0.6660460233688354]], [[0.36121290922164917]], [[0.6554841995239258]], [[0.5840335488319397]], [[0.34858131408691406]], [[0.7302299737930298]], [[0.49955618381500244]], [[0.2371700406074524]], [[0.42052167654037476]], [[0.602689802646637]], [[0.3475961685180664]], [[1.342585563659668]], [[1.2992792129516602]], [[0.2750982642173767]], [[0.9480305910110474]], [[0.8418983817100525]], [[0.37220197916030884]], [[0.7733244299888611]], [[0.5080734491348267]], [[-0.11720170080661774]], [[0.2959270775318146]], [[-0.20873719453811646]], [[0.19322144985198975]], [[0.28939345479011536]], [[0.2978407144546509]], [[0.4685274362564087]], [[0.04135364294052124]], [[0.08907210826873779]], [[0.2990807890892029]], [[1.4351139068603516]], [[0.0]], [[-0.12172806262969971]], [[0.7179700136184692]], [[0.5863940715789795]], [[0.19453582167625427]], [[0.6660457849502563]], [[0.3612130880355835]], [[0.6554843187332153]], [[0.5840336084365845]], [[0.3485810458660126]], [[0.7302299737930298]], [[0.49955636262893677]], [[0.23717007040977478]], [[0.4205215871334076]], [[0.6026896238327026]], [[0.3475962281227112]], [[1.342585802078247]], [[1.2992790937423706]], [[0.0]], [[2.153202533721924]], [[0.643721342086792]], [[1.1775263547897339]], [[0.0]], [[2.153202533721924]], [[0.643721342086792]], [[1.1775263547897339]], [[0.4565430283546448]], [[0.6829096674919128]], [[0.46302729845046997]], [[1.0268313884735107]], [[0.0]], [[-0.12172806262969971]], [[0.7179700136184692]], [[0.5863940715789795]], [[0.19453582167625427]], [[0.6660457849502563]], [[0.3612130880355835]], [[0.6554843187332153]], [[0.5840336084365845]], [[0.3485810458660126]], [[0.7302299737930298]], [[0.49955636262893677]], [[0.23717007040977478]], [[0.4205215871334076]], [[0.6026896238327026]], [[0.3475962281227112]], [[1.342585802078247]], [[1.2992790937423706]], [[0.27509805560112]], [[0.9480301737785339]], [[0.0]], [[-0.12172806262969971]], [[0.7179700136184692]], [[0.5863940715789795]], [[0.19453582167625427]], [[0.6660457849502563]], [[0.3612130880355835]], [[0.6554843187332153]], [[0.5840336084365845]], [[0.3485810458660126]], [[0.7302299737930298]], [[0.49955636262893677]], [[0.23717007040977478]], [[0.4205215871334076]], [[0.6026896238327026]], [[0.3475962281227112]], [[1.342585802078247]], [[1.2992790937423706]], [[0.27509805560112]], [[0.9480301737785339]], [[0.8418982028961182]], [[0.0]], [[2.153202533721924]], [[0.6437216401100159]], [[1.1775256395339966]], [[0.45654329657554626]], [[0.6829094290733337]], [[0.46302729845046997]], [[1.0268311500549316]], [[0.5478433966636658]], [[0.5441939234733582]], [[0.6103899478912354]], [[0.455775648355484]], [[0.4251096546649933]], [[0.8660011291503906]], [[0.47694164514541626]], [[0.43680405616760254]], [[0.5948941707611084]], [[0.43942198157310486]], [[0.42768850922584534]], [[0.8326020240783691]], [[0.30562055110931396]], [[0.27404722571372986]], [[0.06696142256259918]], [[0.1424725353717804]], [[0.2021079659461975]], [[0.7812711000442505]], [[0.4299466609954834]], [[0.3706890344619751]], [[0.548086941242218]], [[0.4115588665008545]], [[0.42114609479904175]], [[0.7805307507514954]], [[0.35308074951171875]], [[0.4043119549751282]], [[0.4986864924430847]], [[0.39880332350730896]], [[0.27572694420814514]], [[0.781461238861084]], [[0.22229745984077454]], [[0.7439563274383545]], [[0.0]], [[-0.1217280924320221]], [[0.7179699540138245]], [[0.5863943099975586]], [[0.19453641772270203]], [[0.6660459041595459]], [[0.0]], [[-0.1217280924320221]], [[0.7179699540138245]], [[0.5863943099975586]], [[0.0]], [[-0.12172806262969971]], [[0.7179700136184692]], [[0.5863940715789795]], [[0.19453582167625427]], [[0.6660457849502563]], [[0.3612130880355835]], [[0.6554843187332153]], [[0.5840336084365845]], [[0.3485810458660126]], [[0.7302299737930298]], [[0.49955636262893677]], [[0.23717007040977478]], [[0.4205215871334076]], [[0.6026896238327026]], [[0.3475962281227112]], [[1.342585802078247]], [[1.2992790937423706]], [[0.27509805560112]], [[0.9480301737785339]], [[0.8418982028961182]], [[0.3722021281719208]], [[0.7733244895935059]], [[0.5080733299255371]], [[0.0]], [[2.153202533721924]], [[0.643721342086792]], [[1.1775263547897339]], [[0.4565430283546448]], [[0.6829096674919128]], [[0.46302729845046997]], [[0.0]], [[-0.12172815203666687]], [[0.7179700136184692]], [[0.5863943099975586]], [[0.1945362687110901]], [[0.6660459041595459]], [[0.3612130284309387]], [[0.6554845571517944]], [[0.5840336680412292]], [[0.34858131408691406]], [[0.7302299737930298]], [[0.4995563328266144]], [[0.23717015981674194]], [[0.42052143812179565]], [[0.6026896834373474]], [[0.3475961983203888]], [[0.0]], [[-0.12172806262969971]], [[0.7179700136184692]], [[0.5863940715789795]], [[0.19453582167625427]], [[0.6660457849502563]], [[0.3612130880355835]], [[0.6554843187332153]], [[0.5840336084365845]], [[0.3485810458660126]], [[0.7302299737930298]], [[0.49955636262893677]], [[0.23717007040977478]], [[0.4205215871334076]], [[0.6026896238327026]], [[0.3475962281227112]], [[1.342585802078247]], [[1.2992790937423706]], [[0.27509805560112]], [[0.0]], [[2.153202533721924]], [[0.6437219381332397]], [[1.177525520324707]], [[0.45654308795928955]], [[0.6829094290733337]], [[0.46302735805511475]], [[1.0268312692642212]], [[0.5478434562683105]], [[0.5441939234733582]], [[0.610389769077301]], [[0.4557756781578064]], [[0.42510926723480225]], [[0.8660012483596802]], [[0.4769413471221924]], [[0.43680405616760254]], [[0.5948941707611084]], [[0.4394218921661377]], [[0.4276881814002991]], [[0.8326021432876587]], [[0.3056204617023468]], [[0.2740470767021179]], [[0.0669613927602768]], [[0.14247234165668488]], [[0.20210778713226318]], [[0.0]], [[2.153202533721924]], [[0.6437219381332397]], [[1.177525520324707]], [[0.45654308795928955]], [[0.6829094290733337]], [[0.46302735805511475]], [[1.0268312692642212]], [[0.5478434562683105]], [[0.5441939234733582]], [[0.610389769077301]], [[0.4557756781578064]], [[0.42510926723480225]], [[0.8660012483596802]], [[0.4769413471221924]], [[0.43680405616760254]], [[0.5948941707611084]], [[0.4394218921661377]], [[0.4276881814002991]], [[0.8326021432876587]], [[0.3056204617023468]], [[0.2740470767021179]], [[0.0669613927602768]], [[0.14247234165668488]], [[0.0]], [[-0.12172806262969971]], [[0.7179700136184692]], [[0.5863940715789795]], [[0.19453582167625427]], [[0.6660457849502563]], [[0.3612130880355835]], [[0.6554843187332153]], [[0.5840336084365845]], [[0.3485810458660126]], [[0.7302299737930298]], [[0.49955636262893677]], [[0.23717007040977478]], [[0.4205215871334076]], [[0.6026896238327026]], [[0.3475962281227112]], [[1.342585802078247]], [[1.2992790937423706]], [[0.27509805560112]], [[0.9480301737785339]], [[0.8418982028961182]], [[0.3722021281719208]], [[0.7733244895935059]], [[0.5080733299255371]], [[-0.11720146238803864]], [[0.2959272861480713]], [[-0.20873726904392242]], [[0.1932213306427002]], [[0.2893936336040497]], [[0.2978406548500061]], [[0.46852758526802063]], [[0.0413534939289093]], [[0.08907222747802734]], [[0.0]], [[-0.12172818183898926]], [[0.0]], [[-0.12172806262969971]], [[0.7179700136184692]], [[0.5863940715789795]], [[0.19453582167625427]], [[0.6660457849502563]], [[0.3612130880355835]], [[0.6554843187332153]], [[0.5840336084365845]], [[0.3485810458660126]], [[0.7302299737930298]], [[0.49955636262893677]], [[0.23717007040977478]], [[0.4205215871334076]], [[0.6026896238327026]], [[0.3475962281227112]], [[1.342585802078247]], [[1.2992790937423706]], [[0.27509805560112]], [[0.9480301737785339]], [[0.8418982028961182]], [[0.3722021281719208]], [[0.7733244895935059]], [[0.5080733299255371]], [[-0.11720146238803864]], [[0.2959272861480713]], [[-0.20873726904392242]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fecd43d81d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: we have to change how ``best'' is done and only visualize within our clusters, not\n",
    "# just sample from the cluster and visualize everything\n",
    "\n",
    "# also, it seems like abs(*) is not really helping??? Idk,\n",
    "\n",
    "cluster_idx = 9\n",
    "\n",
    "cluster_inds = [int(datapoints_used[int(cluster_by_idx[cluster_idx][i])]) for i in range(len(cluster_by_idx[cluster_idx]))]\n",
    "print(dataset[int(cluster_inds[0])]['text'][:100])\n",
    "\n",
    "new_dataset = []\n",
    "for i in cluster_inds:\n",
    "\tnew_dataset.append(dataset[i])\n",
    "# for i in range(len(cluster_by_idx[cluster_idx])):\n",
    "# \tprint(dataset[int(datapoints_used[int(cluster_by_idx[cluster_idx][i])])]['text'][:100])\n",
    "# \tprint(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "activations = model_activations[LAYER][cluster_inds, :, :]\n",
    "# .reshape(-1, model_activations[layer].shape[-1])[cluster, :], 1)\n",
    "print(activations.shape)\n",
    "\n",
    "# TODO: weighted correlations here\n",
    "text_list, full_text, token_list, full_token_list, indices = get_feature_datapoints_with_idx(NEURON, activations, model.tokenizer, token_amount, new_dataset, setting=\"uniform\", k=30)\n",
    "visualize_text_per_cluster(text_list, cluster_inds, NEURON, model, None, layer=LAYER, setting=\"model\")\n",
    "# print(\"\\n\".join(text_list))\n",
    "# TODO: maybe do everything on MLP side where we get only positive activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
