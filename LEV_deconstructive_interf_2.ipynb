{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ad2ecfd158f710eb.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d173f762c78db5b8.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bd49b6b2e3856158.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"JeanKaddour/minipile\"\n",
    "token_amount= 40\n",
    "#TODO: change train[:1000] to train if you want whole dataset\n",
    "# 100_000 datasets\n",
    "# I think that we want to use the full 100_000 at some point...\n",
    "# dataset = load_dataset(dataset_name, split=\"train[:100000]\").map(\n",
    "dataset = load_dataset(dataset_name, split=\"train[:10000]\").map( # 1_000 to get started\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")\n",
    "# TODO: we can maybe make this faster for the larger dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"residual\"\n",
    "\n",
    "def get_cache_name_neurons(layer: int):\n",
    "    if setting == \"residual\":\n",
    "        cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp\":\n",
    "        cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "        neurons = model.cfg.d_mlp\n",
    "    elif setting == \"attention\":\n",
    "        cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp_out\":\n",
    "        cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return cache_name, neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "model.cfg.d_model, n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3cd0dfebf64cfebb6031e5f96cd229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95feab37b9ee499bbc04c42c1bb68e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a42720a8664f308b1110db99398346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9020163eb524cb694a47a60fcecca6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c18a523e9e94e8198b4f91d8073140e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d685d9ee0524e9b8af10837be71c705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: in chunks...\n",
    "# TODO: cache?\n",
    "\n",
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "# MAX_CHUNK_SIZE = 1_000\n",
    "\n",
    "# TODO: move to a separate file or something\n",
    "def get_activations(layer: int):\n",
    "    datapoints = dataset.num_rows\n",
    "    embedding_size = model.cfg.d_model\n",
    "    activations_final = np.memmap(f'layer-{layer}.mymemmap', dtype='float32', mode='w+', shape=(datapoints, token_amount, embedding_size))\n",
    "    batch_size = 32\n",
    "\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        cache_name = get_cache_name_neurons(layer)[0]\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            # print(batch)\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            # print(\"AA\", cache[cache_name].shape)\n",
    "            # batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "\n",
    "            real_batch_size = batch.shape[0]\n",
    "            activations_final[i*batch_size:i*batch_size + real_batch_size, :, :] = cache[cache_name].cpu().numpy()\n",
    "    return activations_final\n",
    "\n",
    "model_activations = [get_activations(layer) for layer in range(n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get activations for a specific feature and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9909, 40, 512), (396360, 512))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_activations[0].shape, model_activations[LAYER].reshape(-1, model_activations[LAYER].shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp_utils import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints_with_idx(feature_index, dictionary_activations, tokenizer, token_amount, dataset, k=10, setting=\"max\"):\n",
    "    if len(dictionary_activations.shape) == 3:\n",
    "        best_feature_activations = dictionary_activations[:, :, feature_index].flatten()\n",
    "    else:\n",
    "        best_feature_activations = dictionary_activations\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        # TODO:! Urrr.... is this backwards? CHECK IF ::-1 is correct but I think that it is\n",
    "        found_indices = np.argsort(best_feature_activations)[::-1][:k]\n",
    "        # found_indices = np.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        # min_value = torch.min(best_feature_activations)\n",
    "        min_value = np.min(best_feature_activations)\n",
    "        max_value = np.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        # TODO: hmm\n",
    "        # np bucketize?\n",
    "        # bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "        bins = np.digitize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in np.unique(bins):\n",
    "            if(bin_idx==0): # Skip the first one. This is below the median\n",
    "                continue\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = np.array(np.nonzero(bins == bin_idx)).squeeze(axis=0)\n",
    "            # print(bin_indices.shape)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = np.flip(np.array(sampled_indices), axis=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    num_datapoints = int(dictionary_activations.shape[0])\n",
    "    datapoint_indices =[np.unravel_index(i, (num_datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list, found_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline before looking at \"deconstructive interference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-33338f44-37f3\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-33338f44-37f3\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"26\", \"\\n\", \"17\", \"\\n\", \"476\", \"\\n\", \"9\", \"46\", \"\\n\", \"6\", \"\\n\", \"18\", \"52\", \"\\n\", \"24\", \"04\", \"\\n\", \"Sen\", \"ate\", \" Republicans\", \" are\", \" pun\", \"ting\", \" two\", \" Obama\", \"Care\", \" bills\", \" into\", \"\\n\", \"Cong\", \"ressional\", \" ins\", \"iders\", \" leading\", \" multiple\", \" probes\", \" into\", \"\\n\", \"45\", \"\\n\", \"A\", \"UST\", \"IN\", \" \\u2013\", \" Just\", \" after\", \" New\", \" Orleans\", \" officially\", \" kicks\", \" off\", \" Carn\", \"ival\", \" season\", \" 2015\", \",\", \" Texas\", \" gay\", \" marriage\", \" and\", \" abortion\", \" activists\", \" will\", \" descend\", \" on\", \"\\n\", \"You\", \" put\", \" hour\", \" upon\", \" gru\", \"eling\", \" hour\", \" into\", \"\\n\", \"Connect\", \"able\", \" sections\", \" provide\", \" a\", \" reach\", \" of\", \" up\", \" to\", \" 10\", \" feet\", \"!\", \"\\\\newline\", \"\\\\newline\", \"Light\", \"weight\", \",\", \" re\", \"-\", \"charge\", \"able\", \" Extended\", \" Re\", \"ach\", \" Tr\", \"immer\", \" features\", \" a\", \" 180\", \"-\", \"degree\", \" piv\", \"oting\", \" head\", \" that\", \" locks\", \" into\", \"\\n\", \"Ray\", \"mond\", \" H\", \".\", \" B\", \"res\", \"cia\", \" and\", \" Edward\", \" J\", \".\", \" Oh\", \"anian\", \",\", \" both\", \" of\", \" Albany\", \" Law\", \" School\", \",\", \" have\", \" posted\", \" on\", \"\\n\", \"The\", \" bizarre\", \" encounter\", \"\\u2014\", \"fil\", \"med\", \" and\", \" posted\", \" to\", \"\\n\", \"National\", \" Law\", \"yers\", \" Guild\", \" -\", \" Center\", \" on\", \" American\", \"-\", \"Islam\", \"ic\", \" Relations\", \"http\", \"://\", \"www\", \".\", \"nl\", \"g\", \".\", \"org\", \"/\", \"tax\", \"onomy\", \"/\", \"term\", \"/\", \"414\", \"\\\\newline\", \"en\", \"Letter\", \" to\", \"\\n\", \"36\", \"60\", \"56\", \"13\", \",\", \" -\", \"36\", \"60\", \"56\", \"12\", \",\", \" -\", \"36\", \"60\", \"56\", \"11\", \"?\", \"\\\\newline\", \"h\", \" -\", \" 36\", \"60\", \"56\", \"14\", \"\\n\", \"2\", \"\\n\", \"Over\", \" forty\", \" years\", \" since\", \" the\", \" declaration\", \" of\", \" the\", \" War\", \" on\", \"\\n\", \"Way\", \"ne\", \" Ro\", \"oney\", \"\\u2019\", \"s\", \" first\", \" game\", \" as\", \" permanent\", \" England\", \" captain\", \" is\", \" unlikely\", \" to\", \"\\n\", \"De\", \"fect\", \"s\", \" in\", \" liver\", \" and\", \" muscle\", \" glycogen\", \" metabolism\", \" in\", \" neonatal\", \" and\", \" adult\", \" New\", \" Zealand\", \" obese\", \" mice\", \".\", \"\\\\newline\", \"Imp\", \"aired\", \" glycogen\", \" synthesis\", \" is\", \" present\", \" in\", \" subjects\", \" at\", \" risk\", \" for\", \"\\n\", \"I\", \" know\", \" people\", \" will\", \" say\", \" it\", \" is\", \" early\", \" days\", \" and\", \" all\", \" that\", \" but\", \",\", \" I\", \" really\", \" feel\", \" from\", \" here\", \" on\", \"\\n\", \"Various\", \" rat\", \" strains\", \" were\", \" studied\", \" on\", \" different\", \" diets\", \" to\", \" check\", \" on\", \"\\n\", \"iron\", \" and\", \" vitamin\", \" supplements\", \"?\", \"\\\\newline\", \"\\\\newline\", \"Is\", \" anyone\", \"'s\", \" premature\", \" baby\", \" out\", \" there\", \" on\", \"\\n\", \"Effect\", \" of\", \" man\", \"n\", \"itol\", \" on\", \" cerebro\", \"spinal\", \" fluid\", \" dynamics\", \" and\", \" brain\", \" tissue\", \" edema\", \".\", \"\\\\newline\", \"Mann\", \"itol\", \" is\", \" used\", \" widely\", \" to\", \"\\n\", \"Over\", \" 2\", \" million\", \" people\", \" have\", \" fled\", \" Syria\", \" since\", \" the\", \" beginning\", \" of\", \" the\", \" conflict\", \" in\", \" 2011\", \",\", \" making\", \" this\", \" one\", \" of\", \" the\", \" largest\", \" refugee\", \" ex\", \"od\", \"uses\", \" in\", \"\\n\", \"In\", \" addition\", \" to\", \"\\n\", \"A\", \" functional\", \" relationship\", \" between\", \" delayed\", \" hypersensitivity\", \" and\", \" antibacterial\", \" immunity\", \".\", \"\\\\newline\", \"The\", \" injection\", \" of\", \" living\", \" L\", \"ister\", \"ia\", \" Mon\", \"ocyt\", \"ogenes\", \" into\", \" the\", \" site\", \" of\", \" a\", \" delayed\", \" hypersensitivity\", \" reaction\", \" in\", \"\\n\", \".\", \"TH\", \" repro\", \"cmd\", \" 8\", \" \\\"\", \"May\", \" 2012\", \"\\\"\", \"\\\\newline\", \".\\\\\\\"\", \" ================================================================\", \"====\", \"\\\\newline\", \".\\\\\\\"\", \" Copyright\", \" 2012\", \" Daniel\", \" P\", \"oc\", \"ock\", \".\", \"  \", \"All\", \" rights\", \" reserved\", \".\", \"\\\\newline\", \".\\\\\\\"\", \" \", \"\\\\newline\", \".\\\\\\\"\", \" Redist\", \"ribution\", \" and\", \" use\", \" in\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"blue\", \"bird\", \"js\", \" cor\", \"out\", \"ine\", \" error\", \" handling\", \" (\", \"browser\", \")\", \"\\\\newline\", \"\\\\newline\", \"How\", \" am\", \" I\", \" supposed\", \" handle\", \" errors\", \" in\", \"\\n\", \"When\", \" wealthy\", \" people\", \" esp\", \"ouse\", \" left\", \"-\", \"wing\", \" causes\", \",\", \" such\", \" as\", \" redistribution\", \" of\", \" wealth\", \",\", \" those\", \" on\", \" the\", \" right\", \" often\", \" label\", \" them\", \" hyp\", \"oc\", \"rites\", \".\", \" \\u201c\", \"If\", \" you\", \" are\", \" so\", \" concerned\", \" about\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Free\", \"ing\", \" a\", \" generators\", \" resources\", \" after\", \" iter\", \"ating\", \" only\", \" over\", \" a\", \" part\", \" of\", \" the\", \" sequence\", \"\\\\newline\", \"\\\\newline\", \"I\", \" would\", \" like\", \" to\", \" parse\", \" a\", \" string\", \" line\", \" by\", \" line\", \" and\", \" provide\", \" a\", \" generator\", \" for\", \"\\n\", \"2\", \",\", \" 263\", \",\", \" 78\", \"007\", \"\\\\newline\", \"What\", \" are\", \" the\", \" prime\", \" factors\", \" of\", \" 95\", \"43\", \"64\", \"90\", \"?\", \"\\\\newline\", \"2\", \",\", \" 5\", \",\", \" 421\", \",\", \" 226\", \"69\", \"\\\\newline\", \"List\", \" the\", \" prime\", \" factors\", \" of\", \" 44\", \"386\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Compar\", \"ing\", \" a\", \" sequence\", \" of\", \" numbers\", \" in\", \" a\", \" python\", \" list\", \"\\\\newline\", \"\\\\newline\", \"I\", \" would\", \" like\", \" to\", \" make\", \" a\", \" loop\", \" to\", \" take\", \" a\", \" number\", \",\", \" compare\", \" with\", \"\\n\", \"\\\\newline\", \"745\", \" A\", \".\", \"2\", \"d\", \" 95\", \" (\", \"2000\", \")\", \"\\\\newline\", \"K\", \"arin\", \" SH\", \"ER\", \"MAN\", \",\", \" Appellant\", \",\", \"\\\\newline\", \"v\", \".\", \"\\\\newline\", \"C\", \"ITY\", \" OF\", \" PH\", \"IL\", \"AD\", \"EL\", \"PH\", \"IA\", \".\", \"\\\\newline\", \"Common\", \"wealth\", \" Court\", \" of\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" use\", \" string\", \" replace\", \" in\", \"\\n\", \"Apple\", \" Creek\", \",\", \" Wisconsin\", \"\\\\newline\", \"\\\\newline\", \"Apple\", \" Creek\", \" is\", \" an\", \" un\", \"incorporated\", \" community\", \" located\", \" in\", \" the\", \" towns\", \" of\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"The\", \" electronic\", \" structure\", \" of\", \" the\", \" rational\", \" approxim\", \"ants\", \" 1\", \"/\", \"1\", \"\\n\", \"You\", \" can\", \" also\", \" pick\", \" up\", \" Char\", \"-\", \"Gr\", \"iller\", \" Ak\", \"orn\", \" Kam\", \"ado\", \"\\n\", \"Sign\", \" up\", \" to\", \" receive\", \" free\", \" email\", \" alerts\", \" when\", \" patent\", \" applications\", \" with\", \" chosen\", \"\\n\", \"T\", \"ed\", \" Cruz\", \"\\n\", \"Inter\", \"action\", \" of\", \" lysine\", \" with\", \" i\", \"odic\", \" acid\", \".\", \"\\\\newline\", \"A\", \" new\", \" compound\", \" in\", \" the\", \" Lys\", \"-\", \"H\", \"IO\", \"(\", \"3\", \"\\n\", \"Application\", \" of\", \" high\", \" resolution\", \" melt\", \" (\", \"HR\", \"M\", \")\", \" analysis\", \" for\", \" duplex\", \" detection\", \" of\", \" Mac\", \"ro\", \"\\n\", \"Gen\", \"etic\", \" factors\", \" in\", \" seizures\", \":\", \" a\", \" population\", \"-\", \"based\", \" study\", \" of\", \" 47\", \",\", \"626\", \" US\", \",\", \" Norwegian\", \" and\", \" Danish\", \" twin\", \" pairs\", \".\", \"\\\\newline\", \"The\", \" purpose\", \" of\", \"\\n\", \"419\", \" F\", \".\", \"2\", \"d\", \" 12\", \"82\", \"\\\\newline\", \"73\", \" L\", \".\", \"R\", \".\", \"R\", \".\", \"M\", \".\", \" (\", \"B\", \"NA\", \")\", \" 2\", \"199\", \"\\\\newline\", \"N\", \"ATIONAL\", \" LAB\", \"OR\", \" REL\", \"ATIONS\", \" BO\", \"ARD\", \",\", \" Petitioner\", \",\", \"v\", \".\", \"TE\", \"AM\", \"ST\", \"\\n\", \"\\u201c\", \"My\", \" body\", \" belongs\", \" to\", \" me\", \",\\u201d\", \" said\", \" Amin\", \"a\", \",\", \" an\", \" ex\", \"-\", \"F\", \"emen\", \" member\", \" who\", \" posed\", \" to\", \"pless\", \" on\", \" the\", \" web\", \".\", \" The\", \" S\", \"ousse\", \" pro\", \"stitutes\", \" seem\", \" to\", \" be\", \" following\", \"\\n\", \"Article\", \" content\", \"\\\\newline\", \"\\n\", \"W\", \"oman\", \" arrested\", \" after\", \" Charlotte\", \" airport\", \" bomb\", \" claim\", \"\\\\newline\", \"\\\\newline\", \"A\", \" Virginia\", \" woman\", \" faces\", \" federal\", \"\\n\", \"Non\", \"-\", \"al\", \"phan\", \"um\", \"eric\", \"\\n\", \"ive\", \" g\", \".\", \"\\\\newline\", \"3\", \"\\\\newline\", \"Rearrange\", \" (\", \"3\", \" -\", \" 7\", \" +\", \" 3\", \")*(\", \"3\", \"*\", \"u\", \" -\", \" 2\", \"*\", \"u\", \" +\", \" 0\", \"*\", \"u\", \")\", \"\\n\", \"Evidence\", \" Acc\", \"um\", \"ulation\", \" and\", \" Flow\", \" of\", \" Control\", \" in\", \" a\", \" Hier\", \"arch\", \"ical\", \" Sp\", \"atial\", \" Reason\", \"ing\", \" System\", \"\\\\newline\", \"\\\\newline\", \"K\", \".\", \" M\", \".\", \" And\", \"ress\", \",\", \" A\", \"vi\", \" K\", \"\\n\", \"2015\", \" CE\", \"A\", \" W\", \"inner\", \"\\n\", \"How\", \" to\", \" be\", \" wrong\", \" (\", \"for\", \" beg\", \"inners\", \")\", \"\\\\newline\", \"\\\\newline\", \"Being\", \" wrong\", \" is\", \" quite\", \" easy\", \",\", \" though\", \" it\", \" requires\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"My\", \"SQL\", \" Parent\", \" Sub\", \"-\", \"level\", \" Table\", \"\\n\", \"COMM\", \"ER\", \"CIAL\", \" TI\", \"\\\\newline\", \"\\n\", \"SC\", \"O\", \"OP\", \" \\u2013\", \" New\", \" Pig\", \" In\", \" Town\", \":\", \" \\u201c\", \"I\", \"b\", \"\\u00e9\", \"rico\", \"\\u201d\", \" Ar\", \"rives\", \" in\", \" Atlanta\", \"\\\\newline\", \"\\\\newline\", \"*\", \"Editor\", \"\\u2019\", \"s\", \" Note\", \" 12\", \"/\", \"18\", \"/\", \"09\", \":\", \" The\", \" original\", \" version\", \" of\", \" this\", \"\\n\", \"................................\", \"................................\", \"................\", \" \", \"\\n\", \"HA\", \" H\", \"3\", \" H\", \"4\", \" CA\", \" SK\", \" S\", \"5\", \" C\", \"5\", \" S\", \"6\", \" C\", \"4\", \" D\", \"5\", \" H\", \"7\", \" H\", \"J\", \" HQ\", \"\\\\newline\", \"D\", \"\\n\", \"contract\", \" C\", \" {\", \"\\\\newline\", \"\\n\", \"City\", \" Council\", \" appro\", \"ves\", \" talk\", \" for\", \" fire\", \" station\", \" renov\", \"ations\", \"\\\\newline\", \"\\\\newline\", \"Related\", \" Story\", \"\\\\newline\", \"\\\\newline\", \"COL\", \"UM\", \"B\", \"IA\", \" -\", \" The\", \" Columbia\", \" City\", \" Council\", \" is\", \" planning\", \" renov\", \"ations\", \" for\", \" Columbia\", \" fire\", \"\\n\", \"Effects\", \" of\", \" a\", \" soy\", \"-\", \"based\", \" dietary\", \" supplement\", \" compared\", \" with\", \" low\", \"-\", \"dose\", \" hormone\", \" therapy\", \" on\", \" the\", \" u\", \"rogen\", \"ital\", \" system\", \":\", \" a\", \" randomized\", \",\", \" double\", \"-\", \"blind\", \",\", \" controlled\", \" clinical\", \" trial\", \".\", \"\\\\newline\", \"This\", \" study\", \" aims\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Object\", \" #\", \" has\", \" no\", \" method\", \" '\", \"done\", \"'\", \"\\\\newline\", \"\\\\newline\", \"I\", \" was\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Im\", \"prove\", \" performance\", \" of\", \" COUNT\", \"/\", \"GROUP\", \"-\", \"BY\", \" in\", \" large\", \" Post\", \"gres\", \"SQL\", \" table\", \"?\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" running\", \" Post\", \"gres\", \"SQL\", \" 9\", \".\", \"2\", \" and\", \" have\", \" a\", \" 12\", \" column\", \" relation\", \"\\n\", \"Don\", \"'t\", \" Sit\", \" Stand\", \"-\", \"Out\", \"\\\\newline\", \"\\\\newline\", \"Man\", \" holds\", \" woman\", \" in\", \" choke\", \"-\", \"hold\", \" to\", \" rob\", \" on\", \" CCT\", \"V\", \"\\\\newline\", \"\\\\newline\", \"CCT\", \"V\", \" shows\", \" a\", \" man\", \" seen\", \" following\", \" woman\", \" walking\", \" down\", \" a\", \" narrow\", \" lane\", \".\", \" He\", \"\\n\", \"A\", \" housing\", \" development\", \" and\", \" soccer\", \" stadium\", \" (\", \"right\", \")\", \" would\", \" be\", \" built\", \" alongside\", \" the\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" detect\", \" if\", \" fetch\", \" cursor\", \" returned\", \" no\", \"\\n\", \"\\\\newline\", \"\\n\", \"The\", \" Cit\", \"adel\", \"\\u2019\", \"s\", \" leading\", \" tack\", \"ler\", \",\", \" linebacker\", \" Carl\", \" Robinson\", \",\", \" will\", \" miss\", \" the\", \" rest\", \" of\", \" the\", \" football\", \" season\", \" after\", \" tearing\", \" the\", \" anterior\", \" cruc\", \"iate\", \" ligament\", \" in\", \" his\", \" knee\", \"\\n\", \"Doctor\", \" Who\", \" (\", \"series\", \" 1\", \")\", \"\\\\newline\", \"\\\\newline\", \"The\", \" first\", \" series\", \" of\", \" the\", \" 2005\", \" revival\", \" of\", \" the\", \" British\", \"\\n\", \"Random\", \"ized\", \" trial\", \" of\", \" stent\", \" placed\", \" above\", \" and\", \" across\", \" the\", \" sph\", \"inct\", \"er\", \" of\", \" Odd\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"V\", \"\\n\", \"The\", \" Cannon\", \" County\", \" Commission\", \" will\", \" consider\", \" seven\", \" candidates\", \" for\", \" appointment\", \" to\", \" the\", \" Cannon\", \" County\", \" Industrial\", \" Development\", \" Board\", \" when\", \" it\", \" meets\", \" Tuesday\", \" night\", \" (\", \"June\", \" 12\", \").\", \"\\\\newline\", \"\\\\newline\", \"The\", \"\\n\", \"LOCK\", \" ON\", \" The\", \" New\", \" World\", \" L\", \"OCK\", \" ON\", \" The\", \" New\", \" World\", \"\\\\newline\", \"Have\", \" confidence\", \" and\", \" jump\", \" into\", \" the\", \" new\", \" world\", \" Jump\", \" into\", \"\\\\newline\", \"LOCK\", \" ON\", \" The\", \" New\", \" World\", \" L\", \"OCK\", \" ON\", \" The\", \" New\", \" World\", \"\\n\", \"Sur\", \"fer\", \" Stri\", \"pe\", \" Back\", \"pack\", \" by\", \" Room\", \" It\", \" Up\", \"\\\\newline\", \"\\\\newline\", \"Item\", \"#\", \" TC\", \"6\", \"184\", \"AN\", \"\\\\newline\", \"\\\\newline\", \"$\", \"30\", \".\", \"99\", \"\\\\newline\", \"\\\\newline\", \"Product\", \" Description\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Adding\", \" an\", \" click\", \" event\", \" listener\", \" to\", \" an\", \" element\", \" in\", \" an\", \" electron\", \" app\", \"\\\\newline\", \"\\\\newline\", \"Error\", \":\", \" document\", \" is\", \" not\", \" defined\", \" when\", \" starting\", \" up\", \" the\", \" electron\", \" app\", \"\\\\newline\", \"I\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"wx\", \"Python\", \" GUI\", \" killed\", \" by\", \" ent\", \"h\", \"ought\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" writing\", \" a\", \" wx\", \"Python\", \" GUI\", \".\", \"  \", \"For\", \" certain\", \"\\n\", \"H\", \"ugh\", \" Alexander\", \" Poll\", \"ock\", \"\\\\newline\", \"\\\\newline\", \"Lie\", \"utenant\", \"\\n\", \"Pl\", \"ant\", \"ron\", \"ics\", \" H\", \"81\", \"N\", \"/\", \"RL\", \"\\\\newline\", \"\\\\newline\", \"The\", \" Plant\", \"ron\", \"ics\", \" H\", \"81\", \"N\", \"/\", \"RL\", \" has\", \" been\", \" discontinued\", \".\", \" No\", \"\\n\", \"H\", \"em\", \"oglobin\", \" enhances\", \" the\", \" production\", \" of\", \" tissue\", \" factor\", \" by\", \" endothelial\", \" cells\", \" in\", \" response\", \" to\", \" bacterial\", \" end\", \"otoxin\", \".\", \"\\\\newline\", \"Human\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Is\", \" this\", \" algorithm\", \" reversible\", \"?\", \"\\\\newline\", \"\\\\newline\", \"I\", \"\\n\", \"Indust\", \"rial\", \" Sh\", \"red\", \"der\", \" |\", \"\\\\newline\", \"\\\\newline\", \"How\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \"\\n\", \"Effect\", \" of\", \" aging\", \" on\", \" the\", \" buff\", \"ering\", \" capacity\", \" of\", \" fast\", \"-\", \"tw\", \"itch\", \" skeletal\", \" muscle\", \".\", \"\\\\newline\", \"The\", \"\\n\", \"'\", \"A\", \" Bad\", \" M\", \"oms\", \" Christmas\", \"'\", \" Review\", \":\", \" Is\", \" The\", \" Movie\", \" Worth\", \" Going\", \" to\", \" See\", \"?\", \"\\\\newline\", \"\\\\newline\", \"Just\", \" in\", \" time\", \" for\", \" the\", \" holidays\", \",\", \" A\", \" Bad\", \"\\n\", \"The\", \"\\n\", \"1\", \"-\", \"800\", \"-\", \"Flow\", \"ers\", \"\\u00ae\", \" Cup\", \"cake\", \" In\", \" Bloom\", \"\\u2122\", \"\\\\newline\", \"\\\\newline\", \"En\", \"large\", \"\\\\newline\", \"\\\\newline\", \"EX\", \"CLUS\", \"IVE\", \" Sweet\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Display\", \"\\n\", \"Guest\", \" Post\", \":\", \" G\", \"UN\", \"S\", \":\", \" The\", \" Good\", \"\\n\", \"Poly\", \"ploid\", \"y\", \" in\", \" sp\", \"itz\", \" ne\", \"vi\", \":\", \" a\", \" not\", \" uncommon\", \" k\", \"ary\", \"otypic\", \" abnormality\", \" identifiable\", \" by\", \" fluorescence\", \" in\", \" situ\", \" hybridization\", \".\", \"\\\\newline\", \"Flu\", \"\\n\", \"Hum\", \"mus\", \" +\", \" Sp\", \"ag\", \"hetti\", \" Squ\", \"ash\", \"\\\\newline\", \"\\\\newline\", \"Sometimes\", \"\\n\", \"Size\", \"\\\\newline\", \"\\\\newline\", \"Col\", \"our\", \"\\\\newline\", \"\\\\newline\", \"Fit\", \"\\\\newline\", \"\\\\newline\", \"Pattern\", \"\\\\newline\", \"\\\\newline\", \"Co\", \"-\", \"ords\", \"\\\\newline\", \"\\\\newline\", \"Sometimes\", \"\\n\", \"The\", \" Good\", \"\\n\", \"Sym\", \"_\", \"type\", \":\", \" module\", \"\\\\newline\", \"Sym\", \"_\", \"name\", \":\", \" top\", \"\\\\newline\", \"Sym\", \"_\", \"lin\", \"eno\", \":\", \" 0\", \"\\\\newline\", \"Sym\", \"\\n\", \"Main\", \" menu\", \"\\\\newline\", \"\\\\newline\", \"Alright\", \"\\n\", \"Sym\", \"_\", \"type\", \":\", \" module\", \"\\\\newline\", \"Sym\", \"\\n\", \"Look\", \"\\n\", \"\\\"\", \"Hi\", \" Frank\", \"ie\", \".\\\"\", \" \\\"\", \"Hello\", \".\\\"\", \" \\\"\", \"Get\", \" in\", \" here\", \".\\\"\", \" \\\"\", \"Nothing\", \"\\n\", \"Introduction\", \"\\\\newline\", \"============\", \"\\\\newline\", \"\\\\newline\", \"Sym\", \"\\n\"], \"activations\": [[[2.2343437671661377]], [[0.0]], [[2.184044599533081]], [[0.0]], [[2.148494243621826]], [[0.0]], [[2.0593743324279785]], [[2.1060314178466797]], [[0.0]], [[2.0608129501342773]], [[0.0]], [[1.9599461555480957]], [[2.0052571296691895]], [[0.0]], [[1.9040392637252808]], [[1.9951276779174805]], [[0.0]], [[1.3036795854568481]], [[1.376867651939392]], [[0.7137495279312134]], [[0.5546756386756897]], [[0.5752054452896118]], [[1.008055567741394]], [[0.42405182123184204]], [[0.2854943871498108]], [[0.22707462310791016]], [[0.6698073148727417]], [[1.9238401651382446]], [[0.0]], [[-0.6913985013961792]], [[0.216981902718544]], [[0.13889187574386597]], [[0.1211627721786499]], [[0.8768267035484314]], [[0.45496445894241333]], [[0.4626604914665222]], [[1.920097827911377]], [[0.0]], [[1.8516335487365723]], [[0.0]], [[-0.05799330398440361]], [[0.17183640599250793]], [[0.8994936347007751]], [[0.6421279311180115]], [[-0.23516245186328888]], [[0.6122263669967651]], [[-0.25474536418914795]], [[-0.019444316625595093]], [[0.7311921119689941]], [[0.5468838214874268]], [[1.0691089630126953]], [[0.3854823410511017]], [[0.6503748893737793]], [[0.35192209482192993]], [[1.0072487592697144]], [[0.10881701111793518]], [[0.3986135721206665]], [[0.4640757739543915]], [[0.6167465448379517]], [[0.31814858317375183]], [[0.22738027572631836]], [[0.5536953210830688]], [[0.4391400218009949]], [[0.7659496068954468]], [[1.8063106536865234]], [[0.0]], [[-0.3105677366256714]], [[0.7118270397186279]], [[-0.012382417917251587]], [[1.4661245346069336]], [[-0.32806822657585144]], [[0.5228796005249023]], [[0.08103631436824799]], [[1.8034155368804932]], [[0.0]], [[0.023299939930438995]], [[0.5880186557769775]], [[0.28106552362442017]], [[0.6427156329154968]], [[0.02713356912136078]], [[0.3349977433681488]], [[0.6444743275642395]], [[0.8332961797714233]], [[1.2526874542236328]], [[0.8074029684066772]], [[0.44593068957328796]], [[0.3930172920227051]], [[-0.049355268478393555]], [[-0.08657336235046387]], [[-0.7237496376037598]], [[-0.33825936913490295]], [[-0.4334896206855774]], [[0.014875873923301697]], [[0.28843608498573303]], [[0.15542756021022797]], [[0.2896011471748352]], [[0.035149529576301575]], [[0.34109094738960266]], [[0.5277290344238281]], [[-0.21991822123527527]], [[0.0771513506770134]], [[0.04799991101026535]], [[0.1272173970937729]], [[0.218753382563591]], [[0.4639046788215637]], [[0.030256837606430054]], [[-0.04893985390663147]], [[0.4798569083213806]], [[0.10985502600669861]], [[0.3078121244907379]], [[-0.05672477185726166]], [[1.7381625175476074]], [[0.0]], [[-0.07107207924127579]], [[0.8739544749259949]], [[0.22080859541893005]], [[-0.12752705812454224]], [[-0.10505156219005585]], [[0.10118946433067322]], [[0.48399102687835693]], [[0.1579301953315735]], [[0.13819727301597595]], [[0.31046462059020996]], [[0.07051509618759155]], [[-0.4720686674118042]], [[0.21812698245048523]], [[-0.2967222332954407]], [[0.10232595354318619]], [[0.6258584856987]], [[0.228074848651886]], [[-0.24802830815315247]], [[0.08043506741523743]], [[-0.16869862377643585]], [[0.3278646767139435]], [[0.9423952698707581]], [[1.691223382949829]], [[0.0]], [[-1.0665342807769775]], [[-0.8568718433380127]], [[-0.027937613427639008]], [[0.5368711948394775]], [[-0.2690664529800415]], [[0.42716556787490845]], [[-0.07614950835704803]], [[0.7927001714706421]], [[1.6703410148620605]], [[0.0]], [[0.1933794766664505]], [[-0.1558862328529358]], [[0.4939875304698944]], [[-0.11951148509979248]], [[0.3908090591430664]], [[0.6261817216873169]], [[1.5845916271209717]], [[0.49139833450317383]], [[0.41343748569488525]], [[0.08015154302120209]], [[0.342405766248703]], [[0.10642600059509277]], [[0.3996715843677521]], [[0.30369997024536133]], [[0.3675960600376129]], [[-0.05261644721031189]], [[0.34134137630462646]], [[0.1053038239479065]], [[-0.0313892662525177]], [[0.2435959279537201]], [[0.17614996433258057]], [[0.30461400747299194]], [[0.397568017244339]], [[0.2139807641506195]], [[-0.03787599503993988]], [[0.0764266848564148]], [[0.7133617401123047]], [[0.4235849976539612]], [[0.1975659877061844]], [[0.19157688319683075]], [[1.6188867092132568]], [[0.0]], [[1.9262802600860596]], [[1.7208997011184692]], [[1.6853983402252197]], [[1.7840001583099365]], [[0.07510772347450256]], [[0.15811441838741302]], [[0.5708094239234924]], [[1.0308641195297241]], [[1.316933274269104]], [[1.5898393392562866]], [[0.03966778516769409]], [[0.14559845626354218]], [[0.5655690431594849]], [[1.0372676849365234]], [[1.3107842206954956]], [[1.7405052185058594]], [[0.49151238799095154]], [[0.14869722723960876]], [[-0.026903465390205383]], [[0.022241167724132538]], [[0.34739944338798523]], [[1.004876732826233]], [[1.252166509628296]], [[1.5781736373901367]], [[0.0]], [[1.5496571063995361]], [[0.0]], [[-0.29256418347358704]], [[-0.04315955936908722]], [[0.010546743869781494]], [[1.0611064434051514]], [[-0.4286940395832062]], [[0.21246469020843506]], [[0.9342072606086731]], [[-0.3502987325191498]], [[-0.35849669575691223]], [[1.4953052997589111]], [[0.0]], [[0.28292030096054077]], [[0.8047626614570618]], [[0.7508432865142822]], [[0.3195980489253998]], [[0.38786858320236206]], [[0.13406404852867126]], [[0.4344450831413269]], [[0.2939348518848419]], [[1.0731184482574463]], [[0.05730816721916199]], [[0.4781170189380646]], [[0.6372054219245911]], [[0.45682552456855774]], [[0.17421233654022217]], [[1.45539391040802]], [[0.0]], [[0.5639767050743103]], [[0.016626209020614624]], [[0.35363081097602844]], [[1.0731596946716309]], [[-0.5700565576553345]], [[0.176537424325943]], [[-0.08755384385585785]], [[-0.3466038405895233]], [[0.2653511166572571]], [[1.2967638969421387]], [[-0.2603575587272644]], [[0.17492468655109406]], [[-0.14182814955711365]], [[-0.3504580855369568]], [[0.4779667258262634]], [[0.3022400736808777]], [[-0.1819850504398346]], [[-0.1258358657360077]], [[0.04592791199684143]], [[-0.7805923223495483]], [[0.20561911165714264]], [[-0.5715514421463013]], [[0.3520492613315582]], [[0.17195934057235718]], [[0.006339050829410553]], [[1.08196222782135]], [[-0.3370642364025116]], [[0.9821033477783203]], [[0.11727556586265564]], [[1.4437317848205566]], [[0.0]], [[-0.699069082736969]], [[-0.18693611025810242]], [[-0.5364629626274109]], [[0.14456243813037872]], [[0.1617470532655716]], [[-0.0688147097826004]], [[0.34414732456207275]], [[0.1687784492969513]], [[-0.16728635132312775]], [[0.20041143894195557]], [[0.006272077560424805]], [[0.34412357211112976]], [[-0.029354892671108246]], [[-0.3295768201351166]], [[-0.7370242476463318]], [[-0.42780405282974243]], [[-0.07185757160186768]], [[0.9708295464515686]], [[0.2174237072467804]], [[1.3864349126815796]], [[0.0]], [[-0.7654981017112732]], [[-0.6130460500717163]], [[-0.3162536919116974]], [[-0.02497648447751999]], [[-0.0827813595533371]], [[1.3017125129699707]], [[-0.5257193446159363]], [[-0.10061462223529816]], [[1.3318912982940674]], [[0.08421733975410461]], [[1.3417619466781616]], [[0.0]], [[0.07934466004371643]], [[-0.1298740953207016]], [[-0.04306681454181671]], [[0.058391273021698]], [[0.1899005025625229]], [[-0.030261993408203125]], [[-0.10128843784332275]], [[-0.6397790908813477]], [[-0.6246263980865479]], [[-0.24870531260967255]], [[0.10959139466285706]], [[-0.3324185609817505]], [[0.5449141263961792]], [[0.15940245985984802]], [[1.3192278146743774]], [[0.0]], [[-0.42055654525756836]], [[0.35655099153518677]], [[-0.6565908789634705]], [[0.18005689978599548]], [[0.3741348385810852]], [[1.4416828155517578]], [[0.20561590790748596]], [[0.260701060295105]], [[-0.05207018554210663]], [[0.16261200606822968]], [[0.19182132184505463]], [[-0.6342364549636841]], [[-0.0737198144197464]], [[0.18439669907093048]], [[-0.25120681524276733]], [[-0.07001587748527527]], [[-0.4703921377658844]], [[0.29504144191741943]], [[0.11856546998023987]], [[0.7290949821472168]], [[0.23896284401416779]], [[1.290712833404541]], [[0.0]], [[-0.2925642728805542]], [[0.23939037322998047]], [[0.23712220788002014]], [[-0.22269728779792786]], [[0.6151397824287415]], [[0.4507831335067749]], [[0.3376694321632385]], [[0.8991042971611023]], [[-0.34533804655075073]], [[0.6764543056488037]], [[0.8766351938247681]], [[-0.2848910391330719]], [[-0.0012314915657043457]], [[1.2310550212860107]], [[0.4675142765045166]], [[-0.02307000756263733]], [[0.5781747102737427]], [[0.18588906526565552]], [[0.4454955458641052]], [[0.7441965341567993]], [[-0.3029293119907379]], [[-0.09760427474975586]], [[0.08018317818641663]], [[-0.13692495226860046]], [[0.05352240800857544]], [[0.5514487028121948]], [[1.2461811304092407]], [[0.0]], [[-0.19163495302200317]], [[-0.35697630047798157]], [[1.2142385244369507]], [[0.0]], [[-0.05799330398440361]], [[-0.2374381124973297]], [[0.18326324224472046]], [[1.4318912029266357]], [[0.30294784903526306]], [[0.22643715143203735]], [[0.1568925380706787]], [[0.12869788706302643]], [[-0.09138473868370056]], [[-0.18432027101516724]], [[0.0010839402675628662]], [[-0.8246352672576904]], [[-0.03519987314939499]], [[0.57510906457901]], [[-0.07211005687713623]], [[-0.13688209652900696]], [[-0.008483104407787323]], [[0.36498498916625977]], [[-0.2941170632839203]], [[-0.09053584933280945]], [[0.20518271625041962]], [[1.416516661643982]], [[-0.11899691820144653]], [[0.07693344354629517]], [[0.7368562817573547]], [[-0.06611574441194534]], [[0.14778776466846466]], [[0.052792519330978394]], [[0.204931378364563]], [[1.1438050270080566]], [[0.0]], [[-0.727714478969574]], [[-0.383788526058197]], [[-0.19520828127861023]], [[0.018199577927589417]], [[0.5464725494384766]], [[0.3037576675415039]], [[0.01887412741780281]], [[0.0975361168384552]], [[0.21947839856147766]], [[0.23609298467636108]], [[0.1268543154001236]], [[0.4017164707183838]], [[0.27927225828170776]], [[0.10172790288925171]], [[0.07272517681121826]], [[-0.45325833559036255]], [[-0.17277014255523682]], [[-0.2220882922410965]], [[0.29378053545951843]], [[0.6081231236457825]], [[0.157919779419899]], [[-0.08617506176233292]], [[-0.1422165185213089]], [[-0.7971256971359253]], [[-0.5953343510627747]], [[0.04428310692310333]], [[-0.0018624179065227509]], [[0.1589052677154541]], [[0.0759856104850769]], [[-0.0975460559129715]], [[0.14629113674163818]], [[0.08364702761173248]], [[-0.07755309343338013]], [[-0.28741955757141113]], [[-0.11705747991800308]], [[0.4970269203186035]], [[1.14061439037323]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.796299159526825]], [[-0.3900328576564789]], [[-0.5442185997962952]], [[-0.20833870768547058]], [[0.2930155396461487]], [[0.027538858354091644]], [[-0.18953396379947662]], [[0.37199732661247253]], [[0.24572069942951202]], [[-0.2108260989189148]], [[0.0075022876262664795]], [[0.09473773837089539]], [[0.029049724340438843]], [[-0.8805203437805176]], [[-0.324255108833313]], [[-0.5497370958328247]], [[-0.20829203724861145]], [[0.003537047654390335]], [[-0.5409907698631287]], [[1.0761710405349731]], [[0.0]], [[-0.3128051161766052]], [[-0.22984683513641357]], [[-0.34320738911628723]], [[0.8092300891876221]], [[0.6676427125930786]], [[0.8589556813240051]], [[0.5790478587150574]], [[0.19146302342414856]], [[0.37580910325050354]], [[-0.10761383175849915]], [[0.11370401084423065]], [[0.43649622797966003]], [[0.2041310966014862]], [[0.6222655177116394]], [[-0.16422715783119202]], [[-0.13850361108779907]], [[-0.49042847752571106]], [[1.4117039442062378]], [[-0.05162695050239563]], [[0.014737933874130249]], [[0.19206780195236206]], [[0.21012181043624878]], [[-0.01938474178314209]], [[0.6193463206291199]], [[0.7306689023971558]], [[0.453869104385376]], [[-0.06555349379777908]], [[0.34497520327568054]], [[-0.49827250838279724]], [[-0.3265199661254883]], [[0.25007888674736023]], [[0.3538440465927124]], [[0.028665393590927124]], [[1.0393955707550049]], [[0.0]], [[-0.11315008997917175]], [[-0.16095426678657532]], [[-0.3282749652862549]], [[-0.3177720904350281]], [[-0.4134664833545685]], [[-0.03634387254714966]], [[-0.2694092392921448]], [[-0.10858270525932312]], [[-0.07432067394256592]], [[0.6620912551879883]], [[0.05639538913965225]], [[0.28079092502593994]], [[-0.15781408548355103]], [[0.5514935255050659]], [[0.11690063774585724]], [[-0.09792909026145935]], [[0.6827457547187805]], [[-0.3544144332408905]], [[-0.13916750252246857]], [[0.261338472366333]], [[0.045774638652801514]], [[-0.7777875661849976]], [[-0.023974059149622917]], [[0.21327397227287292]], [[1.0056664943695068]], [[0.16745740175247192]], [[-0.04744354635477066]], [[-0.7605755925178528]], [[-0.07242119312286377]], [[0.6399481892585754]], [[0.00267079658806324]], [[0.060411594808101654]], [[0.08638769388198853]], [[-0.1084856167435646]], [[-0.2213836908340454]], [[1.023350477218628]], [[0.0]], [[1.5496571063995361]], [[-0.38071417808532715]], [[0.7563424110412598]], [[-0.22813206911087036]], [[0.7224588394165039]], [[1.4714934825897217]], [[0.46879518032073975]], [[-0.3756678104400635]], [[-0.11765022575855255]], [[-0.40293511748313904]], [[-0.19433975219726562]], [[0.05164080858230591]], [[0.7382893562316895]], [[0.7917900681495667]], [[1.1650645732879639]], [[1.0605154037475586]], [[1.4320166110992432]], [[0.3033543825149536]], [[0.13023138046264648]], [[0.3654361963272095]], [[-0.26824015378952026]], [[0.5677164793014526]], [[-0.23348623514175415]], [[0.23955954611301422]], [[-0.21888422966003418]], [[0.3466799855232239]], [[0.9845575094223022]], [[0.4583415389060974]], [[-0.23985567688941956]], [[-0.39898714423179626]], [[-0.07328058034181595]], [[0.06961771845817566]], [[0.7268073558807373]], [[0.614418625831604]], [[0.9691444635391235]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.5079324245452881]], [[0.09730774164199829]], [[-0.353637158870697]], [[-0.37178659439086914]], [[0.4084976017475128]], [[-0.3721335232257843]], [[0.9425449371337891]], [[0.020954474806785583]], [[-0.9453222155570984]], [[-0.18629318475723267]], [[0.1798437237739563]], [[-0.00911375880241394]], [[-0.8255904316902161]], [[-0.05874473601579666]], [[0.1156500056385994]], [[0.9243748188018799]], [[0.5780667662620544]], [[-0.08424077183008194]], [[-0.5397225022315979]], [[1.0979551076889038]], [[0.29582178592681885]], [[-0.01679038256406784]], [[-0.5653218030929565]], [[-0.34226444363594055]], [[-0.20995362102985382]], [[0.929825484752655]], [[0.0]], [[-0.3604313135147095]], [[0.010250508785247803]], [[0.11872775852680206]], [[-0.26937729120254517]], [[0.4001900255680084]], [[0.1425682008266449]], [[0.9801902770996094]], [[0.3741716146469116]], [[0.5340651869773865]], [[0.21751686930656433]], [[0.1538671851158142]], [[0.048931896686553955]], [[-0.24824701249599457]], [[-0.5995016098022461]], [[0.6509782671928406]], [[0.2905616760253906]], [[-0.30587238073349]], [[-0.4098564684391022]], [[-0.20092549920082092]], [[0.11852651834487915]], [[-0.37121647596359253]], [[0.029985982924699783]], [[0.11013832688331604]], [[0.05018891394138336]], [[0.28550130128860474]], [[0.6394749879837036]], [[-0.24460911750793457]], [[0.19657552242279053]], [[0.5007413625717163]], [[0.5385401248931885]], [[0.31981444358825684]], [[0.18737609684467316]], [[-0.11863396316766739]], [[0.16110137104988098]], [[-0.6752376556396484]], [[-0.40251949429512024]], [[0.07183155417442322]], [[0.8784987926483154]], [[0.0]], [[-0.11315010488033295]], [[-0.1609543263912201]], [[-0.3282749652862549]], [[-0.31777238845825195]], [[-0.9512451887130737]], [[0.8140400648117065]], [[0.7801598906517029]], [[-0.713151216506958]], [[0.08844996243715286]], [[0.8330716490745544]], [[0.0]], [[0.38223063945770264]], [[0.6027705073356628]], [[-0.1738353669643402]], [[0.6671399474143982]], [[0.2153649628162384]], [[0.02387174963951111]], [[-0.056267209351062775]], [[0.42538315057754517]], [[0.20534354448318481]], [[-0.01933671161532402]], [[-0.041382476687431335]], [[0.7483434677124023]], [[0.8728368878364563]], [[0.4573923349380493]], [[1.2282264232635498]], [[0.0355551540851593]], [[0.2322767674922943]], [[0.8152039051055908]], [[0.0]], [[-0.06898057460784912]], [[-0.26414787769317627]], [[-0.09434255957603455]], [[-0.031158894300460815]], [[-0.24360400438308716]], [[-1.136207103729248]], [[-0.5413668751716614]], [[-0.4126295745372772]], [[0.39716005325317383]], [[-0.6964476108551025]], [[-0.4541332721710205]], [[0.11295747756958008]], [[0.08543666452169418]], [[0.39666396379470825]], [[0.1837373971939087]], [[0.7649198770523071]], [[0.0]], [[-0.3105677366256714]], [[0.014104247093200684]], [[0.23308345675468445]], [[-0.004987016320228577]], [[0.8549686670303345]], [[-0.30693936347961426]], [[0.08410126715898514]], [[-0.3510409891605377]], [[0.134494349360466]], [[0.0475386306643486]], [[0.07656895369291306]], [[-0.5514096021652222]], [[0.7277817726135254]], [[0.0]], [[-0.05342969298362732]], [[1.4177099466323853]], [[1.3146297931671143]], [[0.8638426065444946]], [[0.8163484334945679]], [[0.14458858966827393]], [[-0.11956512928009033]], [[0.48980018496513367]], [[0.516909658908844]], [[0.6814024448394775]], [[1.3275721073150635]], [[0.6873700022697449]], [[0.0]], [[0.16650845110416412]], [[0.8041629791259766]], [[0.6476759910583496]], [[0.0]], [[-0.4594281017780304]], [[-0.4845380187034607]], [[0.7137141227722168]], [[0.15826886892318726]], [[1.0144367218017578]], [[-0.2584265470504761]], [[-0.02972852997481823]], [[0.4402448832988739]], [[-0.3241170048713684]], [[-0.10922417044639587]], [[-0.32383403182029724]], [[-0.7341262102127075]], [[-0.4937652349472046]], [[0.9778881072998047]], [[-0.171769917011261]], [[-0.40199244022369385]], [[0.013027295470237732]], [[-0.3090282678604126]], [[-0.13517126441001892]], [[0.12470582127571106]], [[0.6114869117736816]], [[0.0]], [[0.00889846682548523]], [[0.5588130950927734]], [[-0.1363888680934906]], [[-0.03192073106765747]], [[0.4155000150203705]], [[0.44168901443481445]], [[-0.21298885345458984]], [[0.14981016516685486]], [[0.17611408233642578]], [[-0.010582253336906433]], [[1.0312907695770264]], [[-0.030670523643493652]], [[0.03693251311779022]], [[0.7753551006317139]], [[0.05426138639450073]], [[0.5865367650985718]], [[0.0]], [[-0.5001564025878906]], [[0.20903724431991577]], [[0.3041883409023285]], [[1.3190078735351562]], [[0.15218690037727356]], [[0.4938928484916687]], [[-0.15267032384872437]], [[-0.020821914076805115]], [[0.3747648596763611]], [[0.045277468860149384]], [[-0.046699583530426025]], [[0.7117951512336731]], [[0.8217848539352417]], [[-0.1218542754650116]], [[0.33866503834724426]], [[0.10072115063667297]], [[-0.24189013242721558]], [[0.5787758827209473]], [[-0.03307060897350311]], [[0.41714704036712646]], [[-0.37214696407318115]], [[0.08847188949584961]], [[-0.15607595443725586]], [[0.0021996796131134033]], [[-0.8506637811660767]], [[-0.2400752156972885]], [[0.5350857377052307]], [[0.0]], [[1.7316527366638184]], [[0.581296443939209]], [[-0.017055701464414597]], [[0.6157527565956116]], [[0.3585219979286194]], [[0.6546610593795776]], [[1.1544431447982788]], [[0.6695016622543335]], [[0.8417515754699707]], [[0.17799809575080872]], [[0.04952271655201912]], [[-0.21989378333091736]], [[-0.04490731284022331]], [[-0.26556169986724854]], [[-0.08951718360185623]], [[-0.14371171593666077]], [[-0.17284700274467468]], [[-0.09444339573383331]], [[-0.27003592252731323]], [[0.3572055995464325]], [[0.044518742710351944]], [[0.23518486320972443]], [[0.9718228578567505]], [[0.43303680419921875]], [[-0.31566938757896423]], [[0.17244590818881989]], [[-0.08103007078170776]], [[0.5408035516738892]], [[0.33185458183288574]], [[0.19469013810157776]], [[0.36508816480636597]], [[0.6144629716873169]], [[-0.2335413098335266]], [[-0.4097669720649719]], [[-0.21218258142471313]], [[-0.24709710478782654]], [[0.05948271229863167]], [[0.45287543535232544]], [[0.6682496070861816]], [[0.49441444873809814]], [[0.0]], [[0.12881150841712952]], [[-0.7947476506233215]], [[-0.3188750445842743]], [[0.11393912136554718]], [[1.3199100494384766]], [[-0.16208302974700928]], [[0.1816013753414154]], [[-0.017661213874816895]], [[-0.17112338542938232]], [[0.4573964774608612]], [[-0.17831356823444366]], [[-0.27781784534454346]], [[-0.35041162371635437]], [[0.2335047423839569]], [[-0.0690300464630127]], [[-0.05490840971469879]], [[0.2547853887081146]], [[0.0959344208240509]], [[0.3693144619464874]], [[1.6089164018630981]], [[0.5112176537513733]], [[1.4636921882629395]], [[-0.047355085611343384]], [[0.31500834226608276]], [[-0.12980729341506958]], [[-0.6498106718063354]], [[-0.22965842485427856]], [[-0.05245561897754669]], [[0.0910363420844078]], [[-0.1073010116815567]], [[-0.35983210802078247]], [[1.254394292831421]], [[0.49804234504699707]], [[0.45579397678375244]], [[0.0]], [[0.2946045994758606]], [[0.6497512459754944]], [[0.41264042258262634]], [[0.0]], [[-0.03295360133051872]], [[0.16916945576667786]], [[1.2035757303237915]], [[1.4836820363998413]], [[1.1920442581176758]], [[1.0053913593292236]], [[0.4050026834011078]], [[0.9723747968673706]], [[0.5568276643753052]], [[0.23834389448165894]], [[0.0203530415892601]], [[0.5349107384681702]], [[-0.06149032711982727]], [[0.3706359267234802]], [[0.3844894766807556]], [[0.0]], [[-0.057323332875967026]], [[0.11412560194730759]], [[0.05909888446331024]], [[-0.5891069173812866]], [[0.082852803170681]], [[0.33287882804870605]], [[0.0]], [[0.1614362597465515]], [[0.15787158906459808]], [[-0.31408369541168213]], [[-0.25765368342399597]], [[0.24572014808654785]], [[0.11250641942024231]], [[0.28824907541275024]], [[0.11767889559268951]], [[0.5673509836196899]], [[0.389626145362854]], [[0.41749507188796997]], [[0.4349355101585388]], [[0.6786625385284424]], [[0.24660331010818481]], [[0.9505979418754578]], [[0.6797901391983032]], [[0.40239816904067993]], [[0.2976232171058655]], [[0.38382089138031006]], [[0.6296992897987366]], [[0.37522369623184204]], [[0.2586270570755005]], [[0.4345242977142334]], [[0.6449612379074097]], [[0.3717727065086365]], [[0.29078251123428345]], [[0.0]], [[0.8022786378860474]], [[0.5149170160293579]], [[0.15205031633377075]], [[0.18343406915664673]], [[0.16532425582408905]], [[-0.07225526869297028]], [[0.5965778231620789]], [[-0.3188122510910034]], [[1.067376971244812]], [[0.24412287771701813]], [[-0.42495113611221313]], [[0.25291162729263306]], [[-0.12012922763824463]], [[0.12686607241630554]], [[-0.006837144494056702]], [[-0.3211015462875366]], [[0.29377228021621704]], [[-0.10514526069164276]], [[0.35787421464920044]], [[0.12163087725639343]], [[0.013692855834960938]], [[-0.12439128756523132]], [[-0.2447313815355301]], [[-0.057623326778411865]], [[-0.4762793481349945]], [[-0.2812701165676117]], [[-0.42735305428504944]], [[-0.1545150876045227]], [[-0.10367973893880844]], [[0.253781795501709]], [[0.0]], [[0.9776767492294312]], [[0.7026943564414978]], [[0.5705486536026001]], [[0.1681108921766281]], [[0.21763411164283752]], [[0.0]], [[-0.7110655307769775]], [[0.9296396970748901]], [[0.35848015546798706]], [[-0.20471999049186707]], [[0.2182052880525589]], [[0.7619836330413818]], [[-0.3581494987010956]], [[-0.1055198535323143]], [[0.16509684920310974]], [[0.045583575963974]], [[-0.08026197552680969]], [[-0.34375572204589844]], [[-0.3982025980949402]], [[0.07954403758049011]], [[-0.2183184027671814]], [[-0.4729945659637451]], [[-0.4210657775402069]], [[-0.13987964391708374]], [[-0.24487720429897308]], [[0.20651280879974365]], [[0.0]], [[-0.11315010488033295]], [[-0.1609543263912201]], [[-0.3282749652862549]], [[-0.31777238845825195]], [[-0.9374517202377319]], [[-0.3428511619567871]], [[-0.28336191177368164]], [[0.12051688134670258]], [[0.07930190116167068]], [[-0.40386465191841125]], [[0.14418667554855347]], [[0.0]], [[0.031837642192840576]], [[1.0529160499572754]], [[0.7711299657821655]], [[0.35414209961891174]], [[0.11186400055885315]], [[0.0]], [[-0.023187894374132156]], [[0.238935187458992]], [[0.6853532195091248]], [[0.5103268623352051]], [[-0.49783065915107727]], [[-0.33688580989837646]], [[0.16273602843284607]], [[-0.22427071630954742]], [[0.23408222198486328]], [[0.2686402201652527]], [[-0.6320198774337769]], [[0.05532529205083847]], [[0.5662599802017212]], [[0.6021165251731873]], [[0.34941044449806213]], [[0.38066366314888]], [[0.12637391686439514]], [[1.1266701221466064]], [[0.5373649597167969]], [[0.2819841504096985]], [[0.05031418800354004]], [[0.10576624423265457]], [[-0.3016849160194397]], [[0.3517270088195801]], [[-0.14232686161994934]], [[-0.27967554330825806]], [[0.42351678013801575]], [[0.14032024145126343]], [[1.0307812690734863]], [[0.2122645080089569]], [[1.3739655017852783]], [[0.5119943618774414]], [[-0.600796103477478]], [[-0.22424104809761047]], [[-0.09473507106304169]], [[0.655664324760437]], [[0.06081312894821167]], [[0.0]], [[0.7960686683654785]], [[0.7960686683654785]], [[0.5520715713500977]], [[0.6406410336494446]], [[0.0]], [[1.0330991744995117]], [[0.3737676441669464]], [[0.8921016454696655]], [[0.6391639113426208]], [[1.29434335231781]], [[0.6513622403144836]], [[0.06935390084981918]], [[0.20246629416942596]], [[1.2937214374542236]], [[0.718341588973999]], [[1.4998316764831543]], [[0.46306008100509644]], [[1.3230040073394775]], [[0.7699365615844727]], [[1.3524764776229858]], [[0.6564604043960571]], [[1.568375587463379]], [[0.5576674342155457]], [[1.3892461061477661]], [[0.5932751297950745]], [[0.34680071473121643]], [[-0.04132641851902008]], [[0.3727830648422241]], [[-0.021540435031056404]], [[0.0]], [[0.452517569065094]], [[0.6263946294784546]], [[0.5889568328857422]], [[-0.04172646999359131]], [[0.0]], [[0.4824884235858917]], [[1.0355868339538574]], [[1.419384479522705]], [[0.9502400159835815]], [[0.8395729064941406]], [[1.3033256530761719]], [[0.13765007257461548]], [[0.6667629480361938]], [[0.6631640791893005]], [[0.540960967540741]], [[0.4676339030265808]], [[0.1265542209148407]], [[-0.2311754822731018]], [[0.21341043710708618]], [[0.3679603338241577]], [[0.10579225420951843]], [[0.1971644014120102]], [[0.05215232074260712]], [[0.10187400877475739]], [[-0.034290239214897156]], [[0.21464532613754272]], [[-0.8114128708839417]], [[-0.09662067890167236]], [[0.11758382618427277]], [[0.6601836681365967]], [[0.2729540765285492]], [[0.3849763572216034]], [[0.2529616355895996]], [[0.36917340755462646]], [[1.2279659509658813]], [[0.3146701455116272]], [[-0.07737797498703003]], [[0.0]], [[-0.4354049563407898]], [[0.44542041420936584]], [[-0.28728771209716797]], [[-0.3518978953361511]], [[0.032865092158317566]], [[-0.2921414375305176]], [[-0.382750928401947]], [[-0.21899010241031647]], [[0.2791826128959656]], [[0.9839840531349182]], [[-0.1933235377073288]], [[0.07105156034231186]], [[-0.3380427360534668]], [[-0.15534184873104095]], [[0.07100920379161835]], [[1.4663928747177124]], [[-0.1300894021987915]], [[-0.15452872216701508]], [[0.005013197660446167]], [[-0.15900161862373352]], [[0.06647910177707672]], [[0.28058093786239624]], [[-0.23876355588436127]], [[-0.04042112082242966]], [[-0.1975880116224289]], [[-0.6787051558494568]], [[0.13516321778297424]], [[-0.4317144453525543]], [[-0.4290750026702881]], [[-0.15725117921829224]], [[-0.6298832297325134]], [[-0.12716078758239746]], [[-0.18449059128761292]], [[-0.020811796188354492]], [[-0.536555826663971]], [[-0.43103379011154175]], [[-0.12481728196144104]], [[0.0]], [[-0.11315010488033295]], [[-0.1609543263912201]], [[-0.3282749652862549]], [[-0.31777238845825195]], [[-0.7419538497924805]], [[0.14831307530403137]], [[0.3024778366088867]], [[-0.41605865955352783]], [[-0.26547375321388245]], [[0.23282332718372345]], [[0.28575775027275085]], [[0.0672922134399414]], [[0.059631526470184326]], [[-0.030843377113342285]], [[-0.7912506461143494]], [[-0.16750191152095795]], [[0.0]], [[-0.11315008997917175]], [[-0.16095426678657532]], [[-0.3282749652862549]], [[-0.3177720904350281]], [[-0.9733485579490662]], [[0.02936244010925293]], [[0.08824093639850616]], [[0.6879583597183228]], [[-0.21625015139579773]], [[-0.10612770915031433]], [[-0.23831790685653687]], [[0.06699373573064804]], [[0.4065092206001282]], [[0.8230069875717163]], [[-0.2515701651573181]], [[0.38024935126304626]], [[-0.4209858775138855]], [[-0.033383943140506744]], [[-0.10093601047992706]], [[-0.01672413945198059]], [[0.0845748782157898]], [[0.015689074993133545]], [[-0.7478513717651367]], [[-0.49544525146484375]], [[0.07910948991775513]], [[-0.02390676736831665]], [[-0.4759201407432556]], [[-0.1672412008047104]], [[0.6018441319465637]], [[0.18139412999153137]], [[0.595363199710846]], [[0.08278381079435349]], [[0.18361912667751312]], [[0.05994249880313873]], [[0.24054446816444397]], [[-0.1893429309129715]], [[-0.2136859893798828]], [[0.0]], [[0.12714944779872894]], [[-0.01925620436668396]], [[-0.19624993205070496]], [[0.018088817596435547]], [[0.23240229487419128]], [[-0.10378531366586685]], [[0.04543358087539673]], [[-0.09424805641174316]], [[-0.7046951055526733]], [[0.24988318979740143]], [[-0.4546252191066742]], [[1.0492074489593506]], [[-0.25605422258377075]], [[0.20205533504486084]], [[0.11501529812812805]], [[1.2727776765823364]], [[0.29358142614364624]], [[1.353863000869751]], [[0.017481207847595215]], [[0.2111583799123764]], [[0.20459133386611938]], [[0.08451449871063232]], [[-0.1836555302143097]], [[-0.010104358196258545]], [[-0.33065110445022583]], [[0.016784176230430603]], [[-0.41288214921951294]], [[0.04476539045572281]], [[0.5117027759552002]], [[-0.3300248086452484]], [[0.021877914667129517]], [[0.8406099081039429]], [[0.15210281312465668]], [[-0.04390980303287506]], [[0.29022687673568726]], [[-0.14317923784255981]], [[-0.24703490734100342]], [[0.0]], [[-0.05799320712685585]], [[0.24575816094875336]], [[0.5965346693992615]], [[0.2226973921060562]], [[-0.16057631373405457]], [[0.22203321754932404]], [[0.4776015877723694]], [[-0.28859826922416687]], [[0.13733914494514465]], [[0.2434794008731842]], [[0.3045167922973633]], [[0.7421807050704956]], [[0.9348829388618469]], [[-0.2783264219760895]], [[0.0]], [[-0.11315010488033295]], [[-0.1609543263912201]], [[-0.3282749652862549]], [[-0.31777238845825195]], [[-0.9512451887130737]], [[0.8140400648117065]], [[0.23142653703689575]], [[0.11743898689746857]], [[-0.44475090503692627]], [[-0.2209387719631195]], [[0.2391762137413025]], [[-0.3206444978713989]], [[0.0]], [[-0.36043113470077515]], [[0.0]], [[-1.066534161567688]], [[-0.027470752596855164]], [[0.02023223042488098]], [[0.2832428812980652]], [[-0.1661612093448639]], [[0.3003716468811035]], [[0.09028451144695282]], [[0.5463014841079712]], [[-0.1765952706336975]], [[0.16703635454177856]], [[0.23983389139175415]], [[0.04534432291984558]], [[-0.21733292937278748]], [[0.007830530405044556]], [[-0.12327161431312561]], [[-0.018457472324371338]], [[-0.059735968708992004]], [[0.6936874985694885]], [[-0.30524754524230957]], [[-0.052205890417099]], [[0.18236179649829865]], [[0.9390937089920044]], [[0.5690449476242065]], [[-0.16113106906414032]], [[0.37003767490386963]], [[0.13030600547790527]], [[0.5042017102241516]], [[0.07933744788169861]], [[1.2332053184509277]], [[0.3251088261604309]], [[-0.38055795431137085]], [[0.0]], [[-0.6773255467414856]], [[-0.22867101430892944]], [[0.19836296141147614]], [[-0.4232253432273865]], [[0.8106046915054321]], [[0.3533034324645996]], [[0.11625102162361145]], [[-0.058600038290023804]], [[-0.8842848539352417]], [[-0.345967561006546]], [[-0.35072439908981323]], [[0.5517101883888245]], [[-0.4938175082206726]], [[0.023124784231185913]], [[0.14745938777923584]], [[0.7690843343734741]], [[-0.4131433069705963]], [[-0.4225785732269287]], [[0.0]], [[-0.731295108795166]], [[0.4441191852092743]], [[0.20889487862586975]], [[0.8977372646331787]], [[0.1405838429927826]], [[0.6345008611679077]], [[0.8172546029090881]], [[0.3523743450641632]], [[0.7763689756393433]], [[-0.21998398005962372]], [[0.08323846012353897]], [[0.33303606510162354]], [[0.4579277038574219]], [[0.6363968849182129]], [[-0.45560476183891296]], [[0.0]], [[-0.11315028369426727]], [[-0.16095438599586487]], [[-0.32827508449554443]], [[-0.3177723288536072]], [[-0.529922604560852]], [[0.0]], [[-1.066534161567688]], [[-0.5851780772209167]], [[0.3962814509868622]], [[0.5803866386413574]], [[0.3763306140899658]], [[0.48112180829048157]], [[0.5241620540618896]], [[1.0836513042449951]], [[1.351349949836731]], [[0.33747512102127075]], [[1.4698292016983032]], [[0.012372463941574097]], [[-0.11451583355665207]], [[0.4033947288990021]], [[-0.05037367343902588]], [[0.7058545351028442]], [[0.3594871461391449]], [[0.4230770766735077]], [[0.12856660783290863]], [[0.7447234392166138]], [[0.6356953382492065]], [[0.33998602628707886]], [[0.6247168779373169]], [[0.4965200424194336]], [[0.7384845614433289]], [[-0.03203488886356354]], [[0.24804756045341492]], [[0.23278769850730896]], [[-0.5653969645500183]], [[0.0]], [[0.30930739641189575]], [[1.1614680290222168]], [[-0.753530740737915]], [[-0.8058607578277588]], [[-0.5123547911643982]], [[-0.0023801028728485107]], [[0.2303498238325119]], [[1.0094623565673828]], [[-0.692616879940033]], [[-0.7655304670333862]], [[-0.5487551689147949]], [[0.05997574329376221]], [[-0.21552284061908722]], [[0.03140267729759216]], [[0.03487621992826462]], [[0.017446368932724]], [[1.4577025175094604]], [[-0.28375089168548584]], [[-0.5946189165115356]], [[-0.5059369802474976]], [[-0.17201869189739227]], [[1.4480812549591064]], [[0.29443567991256714]], [[-0.3482522964477539]], [[0.8728926181793213]], [[-0.6324983835220337]], [[-0.797914981842041]], [[-0.6117783784866333]], [[-0.19315484166145325]], [[0.10120971500873566]], [[0.8365878462791443]], [[-0.6505427360534668]], [[-0.7659753561019897]], [[-0.5856896042823792]], [[0.0]], [[0.2592160999774933]], [[0.5369706153869629]], [[-0.34582799673080444]], [[0.5824804306030273]], [[0.305808961391449]], [[0.21218439936637878]], [[0.8034956455230713]], [[0.02160218358039856]], [[-0.2487664669752121]], [[0.3228818476200104]], [[0.0651414692401886]], [[-0.09635975956916809]], [[-0.6609635353088379]], [[0.3470608592033386]], [[0.22532004117965698]], [[0.6987475156784058]], [[0.9251765012741089]], [[0.8448216915130615]], [[0.23144039511680603]], [[0.04675164818763733]], [[0.059643715620040894]], [[0.48410725593566895]], [[0.00394156388938427]], [[0.8305365443229675]], [[0.24964779615402222]], [[0.11492675542831421]], [[-0.8445067405700684]], [[-0.6252620816230774]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.27908721566200256]], [[-0.3671243488788605]], [[-0.47215136885643005]], [[-0.7158184051513672]], [[-0.19425120949745178]], [[1.2247954607009888]], [[-0.013942133635282516]], [[-0.14106100797653198]], [[0.9093194603919983]], [[0.12945282459259033]], [[-0.20122575759887695]], [[-0.17746363580226898]], [[0.26588860154151917]], [[0.07390877604484558]], [[-0.6906601190567017]], [[0.017271578311920166]], [[-0.2896979749202728]], [[0.015753023326396942]], [[0.21305015683174133]], [[-0.012752145528793335]], [[0.04067487269639969]], [[0.3505897521972656]], [[0.9551622867584229]], [[-0.1917133927345276]], [[-0.23689314723014832]], [[-0.1334993988275528]], [[0.3399430513381958]], [[-0.6657558679580688]], [[0.0]], [[-0.11315015703439713]], [[-0.1609542965888977]], [[-0.3282751142978668]], [[-0.31777223944664]], [[-0.2824150323867798]], [[-0.5607280731201172]], [[-0.4906551241874695]], [[0.45876216888427734]], [[0.926975429058075]], [[0.2029646933078766]], [[0.5284624099731445]], [[-0.013833284378051758]], [[0.25343942642211914]], [[0.06054458022117615]], [[-0.7310883402824402]], [[-0.4135691523551941]], [[0.03787913918495178]], [[-0.020616523921489716]], [[-0.1142754852771759]], [[-0.44350335001945496]], [[-0.5399858951568604]], [[-0.3144012689590454]], [[-0.18478243052959442]], [[-0.2042461335659027]], [[-0.7022596597671509]], [[0.0]], [[-0.14490917325019836]], [[0.3314008116722107]], [[0.7867857217788696]], [[0.5876423120498657]], [[0.620116651058197]], [[0.19211861491203308]], [[-0.009836852550506592]], [[-0.4230690598487854]], [[-0.7379380464553833]], [[0.0]], [[0.1375666856765747]], [[0.6353720426559448]], [[0.4700920581817627]], [[0.14931932091712952]], [[0.30280008912086487]], [[0.7388156056404114]], [[0.11288943886756897]], [[0.039810553193092346]], [[0.05847454071044922]], [[-0.005357414484024048]], [[-0.12242528796195984]], [[-0.8713736534118652]], [[-0.3044588267803192]], [[-0.06848016381263733]], [[-0.24044570326805115]], [[-0.071443110704422]], [[0.5155761241912842]], [[-0.09812270104885101]], [[-0.17268621921539307]], [[-0.1696205735206604]], [[0.38444462418556213]], [[0.24981693923473358]], [[0.7193297147750854]], [[-0.11510118842124939]], [[-0.7840006351470947]], [[0.0]], [[-0.14490914344787598]], [[-0.05254645645618439]], [[-0.15935999155044556]], [[0.6615417003631592]], [[-0.3528995215892792]], [[0.4589889943599701]], [[0.7638841867446899]], [[-0.24448949098587036]], [[0.1378220170736313]], [[0.8461609482765198]], [[0.18596899509429932]], [[-0.003473252058029175]], [[1.0490444898605347]], [[0.22683870792388916]], [[1.360251784324646]], [[-0.09278947114944458]], [[0.2360815703868866]], [[0.09654878079891205]], [[-0.19556564092636108]], [[-0.03596112132072449]], [[-0.8261128664016724]], [[0.0]], [[-0.11315010488033295]], [[-0.1609543263912201]], [[-0.3282749652862549]], [[-0.31777238845825195]], [[-0.7146422863006592]], [[-0.4033220410346985]], [[-0.21737483143806458]], [[-0.7726327180862427]], [[-0.132472962141037]], [[-0.13411039113998413]], [[-0.09890490770339966]], [[-0.8544692993164062]], [[0.0]], [[-0.09607023000717163]], [[0.21899060904979706]], [[-0.4768722653388977]], [[0.5753240585327148]], [[0.43746355175971985]], [[0.5968379974365234]], [[-0.022030413150787354]], [[-0.05192756652832031]], [[-0.9026421308517456]], [[0.0]], [[-0.11315028369426727]], [[-0.16095438599586487]], [[-0.32827508449554443]], [[-0.3177723288536072]], [[-0.9512455463409424]], [[0.0]], [[-0.42055654525756836]], [[0.35655099153518677]], [[-0.2844376564025879]], [[1.4177498817443848]], [[-0.3223458230495453]], [[-0.4145963788032532]], [[0.09985243529081345]], [[-0.02019847184419632]], [[0.6754383444786072]], [[-0.4675690829753876]], [[0.182845339179039]], [[-0.3147426247596741]], [[-0.5194505453109741]], [[-0.46940988302230835]], [[-0.2944973111152649]], [[-0.2869111895561218]], [[-0.12561652064323425]], [[-0.9639712572097778]], [[0.0]], [[-0.2803013026714325]], [[-0.166399285197258]], [[-0.9880329370498657]], [[-0.15695331990718842]], [[-0.1467410922050476]], [[0.43475374579429626]], [[0.047951504588127136]], [[0.11279746145009995]], [[0.3667134642601013]], [[-0.32575905323028564]], [[-0.6564083099365234]], [[-0.3278626799583435]], [[0.41829538345336914]], [[0.3904649317264557]], [[1.1170563697814941]], [[0.04232005774974823]], [[0.15974092483520508]], [[0.11538240313529968]], [[0.0638439953327179]], [[-0.638331413269043]], [[0.8555197715759277]], [[-0.027505243197083473]], [[1.0118578672409058]], [[-0.33231106400489807]], [[0.05989847332239151]], [[-0.21887603402137756]], [[-0.059493571519851685]], [[-1.0239616632461548]], [[0.0]], [[-1.0665342807769775]], [[0.0]], [[1.7576959133148193]], [[0.5796656608581543]], [[0.8983779549598694]], [[0.5753260254859924]], [[-0.18142415583133698]], [[0.10947409272193909]], [[0.3820715844631195]], [[0.45679599046707153]], [[-0.0524497926235199]], [[0.3022816777229309]], [[-0.19969217479228973]], [[0.4068624675273895]], [[0.11536455154418945]], [[-0.04280629754066467]], [[-0.28849783539772034]], [[-0.33576908707618713]], [[0.13708123564720154]], [[-0.04049304127693176]], [[-0.4595101475715637]], [[0.2616889774799347]], [[-0.031557388603687286]], [[-1.0878924131393433]], [[0.0]], [[-0.11315028369426727]], [[-0.16095438599586487]], [[-0.32827508449554443]], [[-0.3177723288536072]], [[-1.1342887878417969]], [[0.0]], [[-0.13128556311130524]], [[0.17863816022872925]], [[0.3236575722694397]], [[0.1613031029701233]], [[0.18894675374031067]], [[0.4179760217666626]], [[0.24228152632713318]], [[-0.7367035150527954]], [[-1.190734624862671]], [[0.0]], [[0.023166432976722717]], [[0.15978536009788513]], [[0.14305379986763]], [[0.8463718891143799]], [[0.0900263637304306]], [[0.34279897809028625]], [[0.4296039938926697]], [[0.26967334747314453]], [[0.32204774022102356]], [[-0.2553279995918274]], [[0.3001866340637207]], [[-0.352938175201416]], [[-0.2362695336341858]], [[0.0633399486541748]], [[-0.13130074739456177]], [[-0.630977988243103]], [[-0.15828420221805573]], [[0.6365653872489929]], [[-0.2005089372396469]], [[1.1002817153930664]], [[0.06884901225566864]], [[0.22697702050209045]], [[-0.14318937063217163]], [[-0.05641022324562073]], [[-1.1960253715515137]], [[0.0]], [[-0.5948150753974915]], [[-0.015451949089765549]], [[0.41560128331184387]], [[0.04535946995019913]], [[0.3976423144340515]], [[0.3443436920642853]], [[0.2303028404712677]], [[0.3749541640281677]], [[0.05363166332244873]], [[-0.10203158855438232]], [[-1.2533457279205322]], [[0.0]], [[0.2182128131389618]], [[-0.10007086396217346]], [[-0.23984602093696594]], [[-0.8369513750076294]], [[0.1860112100839615]], [[0.10934129357337952]], [[-0.07059022784233093]], [[-0.6021264791488647]], [[-0.06961938738822937]], [[-0.1122303307056427]], [[-1.036680817604065]], [[0.00869324803352356]], [[-0.09628388285636902]], [[-0.12073838710784912]], [[0.03981913626194]], [[-0.31774207949638367]], [[0.03793427348136902]], [[-0.08028331398963928]], [[-1.3098976612091064]], [[0.0]], [[-1.0665342807769775]], [[-1.3463276624679565]], [[0.0]], [[-0.7644290924072266]], [[0.13238152861595154]], [[-0.03178662061691284]], [[0.2324785590171814]], [[-0.38034865260124207]], [[-0.047109752893447876]], [[-1.4462486505508423]], [[0.03486558794975281]], [[-0.2701641917228699]], [[0.14416125416755676]], [[0.044117048382759094]], [[-0.034205883741378784]], [[-1.455430030822754]], [[0.045743316411972046]], [[0.028347350656986237]], [[0.5249671339988708]], [[0.040274858474731445]], [[-0.04505467414855957]], [[0.12237086892127991]], [[-1.37270987033844]], [[0.0]], [[-0.4777224063873291]], [[-0.008904270827770233]], [[-0.06744605302810669]], [[-0.254372239112854]], [[-1.4030426740646362]], [[0.0]], [[-0.7644293308258057]], [[0.13238176703453064]], [[-0.03178660571575165]], [[0.23247864842414856]], [[-0.3803483247756958]], [[-0.04711031913757324]], [[-1.4462485313415527]], [[0.0]], [[-1.4740253686904907]], [[0.0]], [[-0.1294383406639099]], [[-0.9234673976898193]], [[-0.29208123683929443]], [[0.3168879449367523]], [[-0.2587331533432007]], [[-0.3450998067855835]], [[-0.9210726022720337]], [[-0.28276610374450684]], [[-0.39471161365509033]], [[-0.28002044558525085]], [[0.6951402425765991]], [[-0.12371709942817688]], [[-0.18290072679519653]], [[-0.31371670961380005]], [[-1.5445067882537842]], [[0.0]], [[-0.04949557036161423]], [[0.0677909255027771]], [[-0.17038728296756744]], [[-0.1645030379295349]], [[-0.21819227933883667]], [[-1.6618385314941406]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fb26a69af10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import interp_utils\n",
    "import importlib\n",
    "importlib.reload(interp_utils)\n",
    "\n",
    "feature = 69\n",
    "text_list, full_text, token_list, full_token_list, indices = get_feature_datapoints_with_idx(feature, model_activations[LAYER], model.tokenizer, token_amount, dataset, setting=\"uniform\", k=100)\n",
    "interp_utils.visualize_text(text_list, feature, model, None, layer=LAYER, setting=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "We can maybe do a basic K-means on just **layers**'s activations. We use this to get a sense for \"neuron's which fire together, wire together\".\n",
    "Maybe we do not have to use K-Means but can instead build some sort of relationship graph.\n",
    "\n",
    "### Then:\n",
    "- We find the top sentences which activate the clustered neurons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redifine Params for the $``\\text{Idea}\"$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURON = 69\n",
    "LAYER = 0\n",
    "N_RELATED = 10\n",
    "N_CLUSTER_EXAMPLES = 5_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup correlations and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Compute the correlations\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Assuming 'activations' is your numpy array of shape (num_samples, num_neurons)\n",
    "# where each row is an activation vector for all neurons\n",
    "\n",
    "def compute_correlations(activations, neuron=NEURON):\n",
    "    num_neurons = activations.shape[1]\n",
    "    neur_acts = activations[:, neuron]\n",
    "    corr_list = np.zeros(num_neurons)\n",
    "    for i in range(num_neurons):\n",
    "      corr, _ = pearsonr(activations[:, i], neur_acts)\n",
    "      corr_list[i] = corr\n",
    "    return corr_list\n",
    "\n",
    "neuron_corrs = compute_correlations(model_activations[LAYER].reshape(-1, model_activations[LAYER].shape[-1]))\n",
    "top_correlated_neurons = np.argsort(np.abs(neuron_corrs))[::-1][:N_RELATED]\n",
    "neuron_corrs[top_correlated_neurons]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at constructive interference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, [10, 84, 274, 314, 462, 508])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_related_datapoints(correlated_indices: list[int], layer=LAYER, n_examples=100):\n",
    "\t# TODO: np.abs(*)?\n",
    "\tfeature_activations = np.abs(model_activations[layer][:, :, correlated_indices])\n",
    "\tsummed_along_sentence = feature_activations.sum(axis=1)\n",
    "\tweighted = summed_along_sentence * neuron_corrs[correlated_indices]\n",
    "\tsummed_along_features = weighted.sum(axis=1)\n",
    "\t# Find the input data-points that most activate the neuron\n",
    "\tfound_indices = np.argsort(summed_along_features)[::-1][:n_examples]\n",
    "\treturn found_indices\n",
    "\n",
    "\n",
    "def get_relevant_neurons(correlated_indices: list[int], layer=LAYER, n_examples=100, weight_cutoff=1.2): # TODO: check weight cutoff vis a vis using quantified models\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\tfound_indices = get_top_related_datapoints(correlated_indices, layer=layer, n_examples=n_examples)\n",
    "\n",
    "\tdef get_activated_neurons(layer: int):\n",
    "\t\tneurons = set()\n",
    "\t\tfor i in found_indices:\n",
    "\t\t\tcutoff_n = model_activations[layer][i, :, :] > weight_cutoff\n",
    "\t\t\t_pos_nonzero, neuron_nonzero = np.nonzero(cutoff_n)\n",
    "\t\t\t# print(\"LEN\", neuron_nonzero.shape)\n",
    "\t\t\tneurons.update(neuron_nonzero)\n",
    "\t\treturn list(neurons)\n",
    "\t\n",
    "\tother_layer_neurons = []\n",
    "\tfor i in range(n_layers):\n",
    "\t\t# if i != layer:\n",
    "\t\tother_layer_neurons.append((i, get_activated_neurons(i)))\n",
    "\treturn other_layer_neurons\n",
    "\n",
    "other_neurons = get_relevant_neurons(correlated_indices=top_correlated_neurons, layer=LAYER, n_examples=N_CLUSTER_EXAMPLES, weight_cutoff=2)\n",
    "len(other_neurons[4][1]), [len(i[1]) for i in other_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5} dict_keys([0, 1, 2, 3, 4, 5])\n",
      "[0, 10, 94, 368, 682, 1144]\n",
      "(5000, 1144)\n",
      "(5000,)\n",
      "{0: (0, 10), 1: (10, 94), 2: (94, 368), 3: (368, 682)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_to_n_layers = 4\n",
    "\n",
    "def get_auxiliary_data(correlated_indices: list[int], layer: int, n_examples=N_CLUSTER_EXAMPLES):\n",
    "\t# TODO: this should be a different function!! (UTILS)\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\t# TODO: abs\n",
    "\t# best_feature_activations = np.abs(model_activations[layer][:, :, correlated_indices])\n",
    "\t# # TODO: weight by correlation?!\n",
    "\t# summed_along_sentence_and_correlates = best_feature_activations.sum(axis=1).sum(axis=1)\n",
    "\t# # Find the input data-points that most activate the neuron\n",
    "\t# found_indices = np.argsort(summed_along_sentence_and_correlates)[::-1][:n_examples]\n",
    "\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\tfound_indices = get_top_related_datapoints(correlated_indices, layer=layer, n_examples=n_examples)\n",
    "\n",
    "\tlayer_idx = {i: layer for i, layer in enumerate(\n",
    "\t\tlist({layer for i, (layer, _) in enumerate(other_neurons)})\n",
    "\t)}\n",
    "\n",
    "\tprint(layer_idx, layer_idx.keys())\n",
    "\tlayer_dividers_list = []\n",
    "\ton_layer = -1\n",
    "\n",
    "\ttotal_other_neurons = 0\n",
    "\tfor other_neur in other_neurons:\n",
    "\t\tother_layer, neurons = other_neur\n",
    "\t\tif other_layer <= go_to_n_layers:\n",
    "\t\t\tif other_layer != on_layer:\n",
    "\t\t\t\ton_layer = other_layer\n",
    "\t\t\t\tlayer_dividers_list.append(total_other_neurons)\n",
    "\t\t\ttotal_other_neurons += len(neurons)\n",
    "\tlayer_dividers_list.append(total_other_neurons)\n",
    "\n",
    "\tlayer_dividers = {}\n",
    "\tprint(layer_dividers_list)\n",
    "\t# TODO: with go_to_n_layers there is going to be some overflow problems\n",
    "\tfor i in range(go_to_n_layers):\n",
    "\t\tlayer_dividers[layer_idx[i]] = (layer_dividers_list[i],  layer_dividers_list[i+1])\n",
    "\t\t\t\n",
    "\t# sum([len(i[1]) for i in other_neurons])\n",
    "\n",
    "\n",
    "\tconcatenated = np.zeros((len(found_indices), total_other_neurons))\n",
    "\n",
    "\tcounter = 0\n",
    "\tfor i, other_neur in enumerate(other_neurons):\n",
    "\t\tother_layer, neurons = other_neur\n",
    "\t\tif other_layer <= go_to_n_layers:\n",
    "\t\t\tr = model_activations[other_layer][:, :, neurons][found_indices].sum(axis=1) # Sum over the entire sentence/ text input\n",
    "\t\t\tconcatenated[:, counter:counter+len(neurons)] = r\n",
    "\t\t\tcounter += len(neurons)\n",
    "\t\t\n",
    "\treturn concatenated, found_indices, layer_dividers\n",
    "\n",
    "\n",
    "aux_data, datapoints_used, layer_dividers  = get_auxiliary_data(correlated_indices=top_correlated_neurons, layer=0)\n",
    "# TODO: CONSIDER ONLY USING THE CLOSER LAYERS...\n",
    "print(aux_data.shape), print(datapoints_used.shape), print(layer_dividers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gaussian_mixture_model():\n",
    "\t# TODO:\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with iteration 0\n",
      "Done with iteration 1\n",
      "Done with iteration 2\n",
      "Done with iteration 3\n",
      "Done with iteration 4\n",
      "Done with iteration 5\n",
      "Done with iteration 6\n",
      "Done with iteration 7\n",
      "Done with iteration 8\n",
      "Done with iteration 9\n",
      "Done with iteration 10\n",
      "Done with iteration 11\n",
      "Done with iteration 12\n",
      "Done with iteration 13\n",
      "Done with iteration 14\n",
      "Done with iteration 15\n",
      "Done with iteration 16\n",
      "Done with iteration 17\n",
      "Done with iteration 18\n",
      "Done with iteration 19\n",
      "Done with iteration 20\n",
      "Done with iteration 21\n",
      "Done with iteration 22\n",
      "Done with iteration 23\n",
      "Done with iteration 24\n",
      "Done with iteration 25\n",
      "Done with iteration 26\n",
      "Done with iteration 27\n",
      "Done with iteration 28\n",
      "Done with iteration 29\n",
      "Done with iteration 30\n",
      "Done with iteration 31\n",
      "Done with iteration 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# Cosine similarity function\n",
    "# TODO: yay or no\n",
    "\n",
    "def create_per_layer_weighting():\n",
    "    weighting = np.ones(aux_data.shape[-1])\n",
    "    for curr_layer in layer_dividers:\n",
    "        start, stop = layer_dividers[curr_layer]\n",
    "        # The further from the current layer, the less important\n",
    "        weighting[start:stop] = weighting[start:stop] * (2 ** (-1 * (abs(LAYER - curr_layer))))\n",
    "        # TODO: not just per neuron\n",
    "    return weighting\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def weighted_dot_similarity(weighting, a, b):\n",
    "    ab = a * b \n",
    "    weighted = ab * weighting\n",
    "    return np.sum(weighted)\n",
    "    # return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# KMeans with cosine similarity\n",
    "\n",
    "\n",
    "def kmeans_cosine(X, n_clusters: int, iterations=100):\n",
    "    k = n_clusters\n",
    "    # Normalize input data\n",
    "    # TODO: why are we normalizing??\n",
    "    X_normalized = normalize(X, axis=1)\n",
    "    weighting = create_per_layer_weighting()\n",
    "\n",
    "    # Randomly initialize centroids\n",
    "    n_samples, n_features = X_normalized.shape\n",
    "    centroids = X_normalized[np.random.choice(n_samples, k, replace=False)]\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        # Cluster assignment step\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for idx, x in enumerate(X_normalized):\n",
    "            similarities = [weighted_dot_similarity(weighting,\n",
    "                x, centroid) for centroid in centroids]\n",
    "            # similarities = [np.dot(x, centroid) for centroid in centroids]\n",
    "            closest = np.argmax(similarities)\n",
    "            clusters[closest].append(idx)\n",
    "\n",
    "        # Update centroids\n",
    "        # TODO: we maybe able to just **not use** PCA at all here.... slow it may be\n",
    "        new_centroids = []\n",
    "        for cluster in clusters:\n",
    "            if cluster:  # Check if cluster is not empty\n",
    "                new_centroid = np.mean(X_normalized[cluster], axis=0)\n",
    "                new_centroids.append(new_centroid)\n",
    "            else:\n",
    "                # Reinitialize empty clusters\n",
    "                new_centroids.append(np.random.rand(n_features))\n",
    "\n",
    "        new_centroids = np.array(new_centroids)\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        print(\"Done with iteration\", iter)\n",
    "\n",
    "    return centroids, clusters\n",
    "\n",
    "\n",
    "# TODO: no function. Just on global so we can stop middway etc etc\n",
    "# TODO: can we speed this up??? Maybe we use PCA\n",
    "_, cluster_by_idx = kmeans_cosine(aux_data, iterations=400, n_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 4, 244, 378, 4, 20, 1843, 28, 23, 32, 36, 1, 11, 33, 41, 699, 85, 87, 37, 15, 5, 103, 300, 812, 9, 22, 33, 67, 12, 4]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# TODO: GMMs\n",
    "print([len(c) for c in cluster_by_idx])\n",
    "for i, c in enumerate(cluster_by_idx):\n",
    "\tif len(c) > 0:\n",
    "\t\tprint(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\n",
      "(12, 40, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-459dabdf-23c5\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-459dabdf-23c5\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"#\", \"!/\", \"usr\", \"/\", \"bin\", \"/\", \"env\", \" bash\", \"\\\\newline\", \"\\\\newline\", \"#\", \" This\", \" Source\", \" Code\", \" Form\", \" is\", \" subject\", \" to\", \"\\n\", \"<!\", \"DOCTYPE\", \" html\", \" PUBLIC\", \" \\\"-\", \"//\", \"W\", \"3\", \"C\", \"//\", \"DTD\", \" X\", \"HTML\", \" 1\", \".\", \"0\", \" Trans\", \"itional\", \"//\", \"EN\", \"\\\"\", \" \\\"\", \"http\", \"://\", \"www\", \".\", \"w\", \"3\", \".\", \"org\", \"/\", \"TR\", \"/\", \"xhtml\", \"1\", \"\\n\", \"/*\", \"\\\\newline\", \"Copyright\", \" 2017\", \" The\", \" Kubernetes\", \" Authors\", \".\", \"\\\\newline\", \"\\\\newline\", \"Lic\", \"ensed\", \" under\", \" the\", \" Apache\", \" License\", \",\", \" Version\", \" 2\", \".\", \"0\", \" (\", \"the\", \" \\\"\", \"License\", \"\\\");\", \"\\\\newline\", \"you\", \" may\", \" not\", \" use\", \" this\", \" file\", \" except\", \" in\", \" compliance\", \" with\", \"\\n\", \"//\", \"\\\\newline\", \"//\", \"     \", \"Generated\", \" by\", \" class\", \"-\", \"dump\", \" 3\", \".\", \"5\", \"\\n\", \"/*\", \"\\\\newline\", \"Copyright\", \" 2017\", \" The\", \" Kubernetes\", \" Authors\", \".\", \"\\\\newline\", \"\\\\newline\", \"Lic\", \"ensed\", \" under\", \" the\", \" Apache\", \" License\", \",\", \" Version\", \" 2\", \".\", \"0\", \" (\", \"the\", \" \\\"\", \"License\", \"\\\");\", \"\\\\newline\", \"you\", \" may\", \" not\", \" use\", \" this\", \" file\", \" except\", \" in\", \"\\n\", \"/**\", \"\\\\newline\", \" *\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \" Foundation\", \" (\", \"AS\", \"F\", \")\", \" under\", \" one\", \"\\\\newline\", \" *\", \" or\", \" more\", \" contributor\", \" license\", \" agreements\", \".\", \"  \", \"See\", \" the\", \" NOTICE\", \" file\", \"\\\\newline\", \" *\", \" distributed\", \" with\", \"\\n\", \"<!\", \"DOCTYPE\", \" HTML\", \" PUBLIC\", \" \\\"-\", \"//\", \"W\", \"3\", \"C\", \"//\", \"DTD\", \" HTML\", \" 4\", \"\\n\", \"package\", \" ens\", \"\\\\newline\", \"\\\\newline\", \"//\", \"Lic\", \"ensed\", \" under\", \" the\", \" Apache\", \" License\", \",\", \" Version\", \" 2\", \".\", \"0\", \" (\", \"the\", \" \\\"\", \"License\", \"\\\");\", \"\\\\newline\", \"//\", \"you\", \" may\", \" not\", \" use\", \"\\n\", \"<!\", \"DOCTYPE\", \" HTML\", \" PUBLIC\", \" \\\"-\", \"//\", \"W\", \"3\", \"C\", \"\\n\", \"<!\", \"DOCTYPE\", \" HTML\", \" PUBLIC\", \" \\\"-\", \"//\", \"W\", \"3\", \"C\", \"//\", \"DTD\", \" HTML\", \" 4\", \".\", \"01\", \" Trans\", \"itional\", \"//\", \"EN\", \"\\n\", \"<!\", \"DOCTYPE\", \" HTML\", \" PUBLIC\", \" \\\"-\", \"//\", \"W\", \"3\", \"C\", \"//\", \"DTD\", \" HTML\", \"\\n\", \"/**\", \"\\\\newline\", \" *\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \" Foundation\", \" (\", \"AS\", \"F\", \")\", \" under\", \" one\", \"\\\\newline\", \" *\", \" or\", \" more\", \" contributor\", \" license\", \" agreements\", \".\", \"  \", \"See\", \" the\", \" NOTICE\", \" file\", \"\\\\newline\", \"\\n\", \"/*\", \"\\\\newline\", \"Copyright\", \" 2017\", \" The\", \" Kubernetes\", \" Authors\", \".\", \"\\\\newline\", \"\\\\newline\", \"Lic\", \"ensed\", \" under\", \" the\", \" Apache\", \" License\", \",\", \" Version\", \" 2\", \".\", \"0\", \" (\", \"the\", \" \\\"\", \"License\", \"\\\");\", \"\\\\newline\", \"you\", \" may\", \" not\", \" use\", \" this\", \" file\", \" except\", \"\\n\", \"/**\", \"\\\\newline\", \" *\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \" Foundation\", \" (\", \"AS\", \"F\", \")\", \" under\", \" one\", \"\\\\newline\", \" *\", \" or\", \" more\", \"\\n\", \"//\", \"\\\\newline\", \"//\", \"     \", \"Generated\", \" by\", \" class\", \"-\", \"dump\", \" 3\", \".\", \"5\", \" (\", \"\\n\", \"/**\", \"\\\\newline\", \"\\n\", \"/**\", \"\\\\newline\", \" *\", \" Licensed\", \" to\", \" the\", \" Apache\", \" Software\", \" Foundation\", \" (\", \"AS\", \"F\", \")\", \" under\", \" one\", \"\\\\newline\", \" *\", \" or\", \" more\", \" contributor\", \" license\", \" agreements\", \".\", \"  \", \"\\n\", \"//\", \"\\\\newline\", \"//\", \"     \", \"Generated\", \" by\", \" class\", \"-\", \"dump\", \" 3\", \".\", \"5\", \" (\", \"64\", \" bit\", \")\", \" (\", \"Debug\", \" version\", \" compiled\", \" Sep\", \" 17\", \" 2017\", \" 16\", \":\", \"24\", \":\", \"48\", \").\", \"\\\\newline\", \"//\", \"\\\\newline\", \"//\", \"\\n\", \"/*\", \"\\\\newline\", \"Copyright\", \" The\", \" Kubernetes\", \" Authors\", \".\", \"\\\\newline\", \"\\\\newline\", \"Lic\", \"ensed\", \" under\", \" the\", \" Apache\", \" License\", \",\", \" Version\", \" 2\", \".\", \"0\", \" (\", \"the\", \" \\\"\", \"License\", \"\\\");\", \"\\\\newline\", \"you\", \" may\", \" not\", \" use\", \" this\", \" file\", \" except\", \" in\", \" compliance\", \" with\", \" the\", \"\\n\", \"#\", \"!/\", \"usr\", \"/\", \"bin\", \"/\", \"env\", \" bash\", \"\\\\newline\", \"\\\\newline\", \"#\", \" This\", \" Source\", \" Code\", \" Form\", \" is\", \" subject\", \" to\", \" the\", \" terms\", \" of\", \" the\", \" Mozilla\", \" Public\", \"\\\\newline\", \"#\", \" License\", \",\", \" v\", \".\", \" 2\", \".\", \"0\", \".\", \" If\", \" a\", \"\\n\", \"package\", \"\\n\", \"#\", \" Copyright\", \"\\n\", \"/*\", \"\\\\newline\", \"Copyright\", \" The\", \" Kubernetes\", \" Authors\", \".\", \"\\\\newline\", \"\\\\newline\", \"Lic\", \"ensed\", \" under\", \" the\", \" Apache\", \" License\", \"\\n\", \"#\", \"!/\", \"usr\", \"\\n\", \"#\", \"!/\", \"\\n\", \"package\", \" ens\", \"\\\\newline\", \"\\\\newline\", \"//\", \"Lic\", \"ensed\", \" under\", \" the\", \" Apache\", \" License\", \",\", \" Version\", \" 2\", \".\", \"0\", \" (\", \"the\", \" \\\"\", \"License\", \"\\n\"], \"activations\": [[[0.4832516312599182]], [[-0.6376005411148071]], [[-0.6106313467025757]], [[-0.4782004952430725]], [[-0.6040186882019043]], [[-0.029840052127838135]], [[-0.12123461067676544]], [[-0.3336063325405121]], [[-0.10240373015403748]], [[-0.25103428959846497]], [[0.31647586822509766]], [[-0.5173830986022949]], [[-0.4095550775527954]], [[-0.18233564496040344]], [[-0.09213888645172119]], [[0.03072439879179001]], [[-0.12732505798339844]], [[1.4617459774017334]], [[0.0]], [[0.2562711536884308]], [[-0.493869811296463]], [[0.28486618399620056]], [[0.18591433763504028]], [[0.3393917679786682]], [[0.46257108449935913]], [[-0.026622343808412552]], [[0.6857113242149353]], [[0.6304638981819153]], [[0.6438699960708618]], [[0.2491878718137741]], [[0.09937208890914917]], [[0.2808031141757965]], [[0.5993789434432983]], [[0.2868089973926544]], [[0.6731844544410706]], [[0.3907521367073059]], [[-0.010251373052597046]], [[0.48686838150024414]], [[0.5618587732315063]], [[0.0590168833732605]], [[0.18184226751327515]], [[0.07820297032594681]], [[-0.036265186965465546]], [[0.06898549199104309]], [[-0.10220692306756973]], [[-0.239542156457901]], [[0.8213714957237244]], [[0.04753800109028816]], [[0.25379014015197754]], [[0.1720004379749298]], [[0.6004196405410767]], [[0.07579168677330017]], [[-0.18521445989608765]], [[1.2365702390670776]], [[0.0]], [[0.02927614375948906]], [[-0.207588791847229]], [[-0.46909669041633606]], [[0.20497243106365204]], [[-0.5271516442298889]], [[-0.13618028163909912]], [[-0.12047363817691803]], [[-0.15043410658836365]], [[-0.03638666868209839]], [[-0.021837592124938965]], [[-0.054881297051906586]], [[0.08063963800668716]], [[0.821397066116333]], [[-0.2892272174358368]], [[-0.10309699177742004]], [[-0.45937976241111755]], [[-0.21723726391792297]], [[0.013052580878138542]], [[0.30628466606140137]], [[0.23414099216461182]], [[0.6076910495758057]], [[0.2820069491863251]], [[-0.4098966121673584]], [[0.22609084844589233]], [[-1.0151547193527222]], [[0.27764561772346497]], [[0.06321513652801514]], [[-0.4418628215789795]], [[0.3491783142089844]], [[0.4588908851146698]], [[0.783194363117218]], [[0.2631046772003174]], [[0.1652563512325287]], [[0.307696670293808]], [[1.004929780960083]], [[0.4229191839694977]], [[1.2105953693389893]], [[0.0]], [[0.11805032938718796]], [[-0.3014727234840393]], [[-0.09759730100631714]], [[-0.0946575403213501]], [[-0.22382745146751404]], [[0.6647236347198486]], [[-0.23265820741653442]], [[-0.012247443199157715]], [[0.08092561364173889]], [[0.40635767579078674]], [[0.11074677109718323]], [[1.0668118000030518]], [[0.0]], [[0.02927614375948906]], [[-0.207588791847229]], [[-0.46909669041633606]], [[0.20497243106365204]], [[-0.5271516442298889]], [[-0.13618028163909912]], [[-0.12047363817691803]], [[-0.15043410658836365]], [[-0.03638666868209839]], [[-0.021837592124938965]], [[-0.054881297051906586]], [[0.08063963800668716]], [[0.821397066116333]], [[-0.2892272174358368]], [[-0.10309699177742004]], [[-0.45937976241111755]], [[-0.21723726391792297]], [[0.013052580878138542]], [[0.30628466606140137]], [[0.23414099216461182]], [[0.6076910495758057]], [[0.2820069491863251]], [[-0.4098966121673584]], [[0.22609084844589233]], [[-1.0151547193527222]], [[0.27764561772346497]], [[0.06321513652801514]], [[-0.4418628215789795]], [[0.3491783142089844]], [[0.4588908851146698]], [[0.783194363117218]], [[0.2631046772003174]], [[0.1652563512325287]], [[0.307696670293808]], [[1.004929780960083]], [[0.0]], [[0.4518899917602539]], [[0.1182088851928711]], [[0.2577619254589081]], [[0.1808459311723709]], [[0.9650787115097046]], [[-0.3471846878528595]], [[-0.06658041477203369]], [[-0.39219599962234497]], [[0.2690187990665436]], [[0.4237286448478699]], [[0.17613816261291504]], [[0.5255664587020874]], [[0.1529812216758728]], [[0.7945734262466431]], [[0.41204819083213806]], [[0.4444792866706848]], [[0.2886945605278015]], [[-0.40598028898239136]], [[0.27889275550842285]], [[0.17988801002502441]], [[-0.12285372614860535]], [[0.4396767020225525]], [[-0.016371728852391243]], [[0.010367587208747864]], [[-0.08369356393814087]], [[-0.29876869916915894]], [[0.3615081310272217]], [[0.15536120533943176]], [[0.4572314918041229]], [[0.31705912947654724]], [[0.3036300539970398]], [[0.929162859916687]], [[0.0]], [[0.25627121329307556]], [[-0.49386972188949585]], [[0.33172154426574707]], [[0.23592576384544373]], [[0.41729557514190674]], [[0.4929910898208618]], [[0.054257385432720184]], [[0.7305365800857544]], [[0.6635230183601379]], [[0.6676384210586548]], [[0.2593008577823639]], [[0.49273931980133057]], [[0.8166210651397705]], [[0.0]], [[-0.31944748759269714]], [[-0.3893623352050781]], [[-0.0706494152545929]], [[-0.2426307499408722]], [[-0.25242650508880615]], [[-0.31275713443756104]], [[-0.034305039793252945]], [[0.7856411933898926]], [[-0.40288686752319336]], [[-0.16131192445755005]], [[-0.4947929084300995]], [[-0.27302780747413635]], [[0.02113521657884121]], [[0.2666087746620178]], [[0.13472099602222443]], [[0.5987979173660278]], [[0.24381442368030548]], [[-0.401953786611557]], [[0.18335314095020294]], [[-1.0406076908111572]], [[0.2291356921195984]], [[0.022507280111312866]], [[-0.0772656798362732]], [[-0.4440244436264038]], [[0.3429003953933716]], [[0.4452158510684967]], [[0.7789401412010193]], [[0.0]], [[0.25627121329307556]], [[-0.49386972188949585]], [[0.33172154426574707]], [[0.23592576384544373]], [[0.41729557514190674]], [[0.4929910898208618]], [[0.054257385432720184]], [[0.7305365800857544]], [[0.6635230183601379]], [[0.0]], [[0.25627121329307556]], [[-0.49386993050575256]], [[0.33172130584716797]], [[0.23592609167099]], [[0.41729575395584106]], [[0.49299126863479614]], [[0.05425715446472168]], [[0.7305368185043335]], [[0.6635231375694275]], [[0.6676380634307861]], [[0.25930076837539673]], [[0.49273908138275146]], [[0.8166210651397705]], [[0.2230111062526703]], [[1.1403913497924805]], [[0.4584178924560547]], [[0.11142268776893616]], [[0.49735596776008606]], [[0.6143140196800232]], [[0.0]], [[0.25627121329307556]], [[-0.49386972188949585]], [[0.33172154426574707]], [[0.23592576384544373]], [[0.41729557514190674]], [[0.4929910898208618]], [[0.054257385432720184]], [[0.7305365800857544]], [[0.6635230183601379]], [[0.6676384210586548]], [[0.2593008577823639]], [[0.49273931980133057]], [[0.0]], [[0.4518899917602539]], [[0.1182088851928711]], [[0.2577619254589081]], [[0.1808459311723709]], [[0.9650787115097046]], [[-0.3471846878528595]], [[-0.06658041477203369]], [[-0.39219599962234497]], [[0.2690187990665436]], [[0.4237286448478699]], [[0.17613816261291504]], [[0.5255664587020874]], [[0.1529812216758728]], [[0.7945734262466431]], [[0.41204819083213806]], [[0.4444792866706848]], [[0.2886945605278015]], [[-0.40598028898239136]], [[0.27889275550842285]], [[0.17988801002502441]], [[-0.12285372614860535]], [[0.4396767020225525]], [[-0.016371728852391243]], [[0.010367587208747864]], [[-0.08369356393814087]], [[-0.29876869916915894]], [[0.3615081310272217]], [[0.15536120533943176]], [[0.4572314918041229]], [[0.0]], [[0.02927614375948906]], [[-0.207588791847229]], [[-0.46909669041633606]], [[0.20497243106365204]], [[-0.5271516442298889]], [[-0.13618028163909912]], [[-0.12047363817691803]], [[-0.15043410658836365]], [[-0.03638666868209839]], [[-0.021837592124938965]], [[-0.054881297051906586]], [[0.08063963800668716]], [[0.821397066116333]], [[-0.2892272174358368]], [[-0.10309699177742004]], [[-0.45937976241111755]], [[-0.21723726391792297]], [[0.013052580878138542]], [[0.30628466606140137]], [[0.23414099216461182]], [[0.6076910495758057]], [[0.2820069491863251]], [[-0.4098966121673584]], [[0.22609084844589233]], [[-1.0151547193527222]], [[0.27764561772346497]], [[0.06321513652801514]], [[-0.4418628215789795]], [[0.3491783142089844]], [[0.4588908851146698]], [[0.783194363117218]], [[0.2631046772003174]], [[0.1652563512325287]], [[0.307696670293808]], [[0.0]], [[0.4518899917602539]], [[0.1182088851928711]], [[0.2577619254589081]], [[0.1808459311723709]], [[0.9650787115097046]], [[-0.3471846878528595]], [[-0.06658041477203369]], [[-0.39219599962234497]], [[0.2690187990665436]], [[0.4237286448478699]], [[0.17613816261291504]], [[0.5255664587020874]], [[0.1529812216758728]], [[0.7945734262466431]], [[0.41204819083213806]], [[0.4444792866706848]], [[0.2886945605278015]], [[-0.40598028898239136]], [[0.27889275550842285]], [[0.0]], [[0.11805032938718796]], [[-0.3014727234840393]], [[-0.09759730100631714]], [[-0.0946575403213501]], [[-0.22382745146751404]], [[0.6647236347198486]], [[-0.23265820741653442]], [[-0.012247443199157715]], [[0.08092561364173889]], [[0.40635767579078674]], [[0.11074677109718323]], [[1.0668118000030518]], [[0.20760437846183777]], [[0.0]], [[0.45188993215560913]], [[0.11820900440216064]], [[0.0]], [[0.4518899917602539]], [[0.1182088851928711]], [[0.2577619254589081]], [[0.1808459311723709]], [[0.9650787115097046]], [[-0.3471846878528595]], [[-0.06658041477203369]], [[-0.39219599962234497]], [[0.2690187990665436]], [[0.4237286448478699]], [[0.17613816261291504]], [[0.5255664587020874]], [[0.1529812216758728]], [[0.7945734262466431]], [[0.41204819083213806]], [[0.4444792866706848]], [[0.2886945605278015]], [[-0.40598028898239136]], [[0.27889275550842285]], [[0.17988801002502441]], [[-0.12285372614860535]], [[0.4396767020225525]], [[-0.016371728852391243]], [[0.010367587208747864]], [[0.0]], [[0.11805025488138199]], [[-0.30147242546081543]], [[-0.09759743511676788]], [[-0.09465736150741577]], [[-0.22382740676403046]], [[0.6647236347198486]], [[-0.23265784978866577]], [[-0.01224730908870697]], [[0.0809256061911583]], [[0.4063575863838196]], [[0.11074595898389816]], [[1.0668126344680786]], [[0.2076048105955124]], [[0.4485456347465515]], [[-0.09240922331809998]], [[0.16210734844207764]], [[0.1538073569536209]], [[-0.5297178626060486]], [[-0.19893468916416168]], [[0.7662999033927917]], [[0.34537696838378906]], [[0.7707970142364502]], [[0.6783275604248047]], [[0.92841637134552]], [[0.7740908265113831]], [[1.0168577432632446]], [[0.7211850881576538]], [[1.1262872219085693]], [[-0.23226185142993927]], [[-0.018478572368621826]], [[-0.03884120285511017]], [[-0.04075431823730469]], [[-0.05243588984012604]], [[0.0]], [[0.02927614375948906]], [[-0.207588791847229]], [[-0.46909669041633606]], [[-0.6201865673065186]], [[-0.1912490725517273]], [[-0.1318238526582718]], [[-0.1694195568561554]], [[-0.07242429256439209]], [[-0.04868346452713013]], [[-0.07213639467954636]], [[0.06593091040849686]], [[0.8244079947471619]], [[-0.294813871383667]], [[-0.10321664810180664]], [[-0.4664539098739624]], [[-0.2115551233291626]], [[0.014342917129397392]], [[0.3134367763996124]], [[0.23369663953781128]], [[0.6028156876564026]], [[0.281014084815979]], [[-0.4072875380516052]], [[0.2300298810005188]], [[-1.013325572013855]], [[0.2794623374938965]], [[0.06224435567855835]], [[-0.4411531686782837]], [[0.35835134983062744]], [[0.4656013250350952]], [[0.7880018353462219]], [[0.2677903175354004]], [[0.1697154939174652]], [[0.3170787990093231]], [[1.0075384378433228]], [[0.426312118768692]], [[1.212092638015747]], [[-0.1913868486881256]], [[0.0]], [[0.483251690864563]], [[-0.6376004815101624]], [[-0.6106314659118652]], [[-0.4782004654407501]], [[-0.6040186882019043]], [[-0.029839888215065002]], [[-0.12123478949069977]], [[-0.33360639214515686]], [[-0.10240375995635986]], [[-0.25103437900543213]], [[0.3164758086204529]], [[-0.5173831582069397]], [[-0.4095548391342163]], [[-0.18233561515808105]], [[-0.09213876724243164]], [[0.030724257230758667]], [[-0.12732508778572083]], [[1.4617464542388916]], [[-0.2380354404449463]], [[0.033589258790016174]], [[0.5841864943504333]], [[-0.38804376125335693]], [[0.6447216272354126]], [[0.21483340859413147]], [[0.1466239094734192]], [[0.44915270805358887]], [[-0.3020666241645813]], [[-0.2516481280326843]], [[-0.43893906474113464]], [[0.1561286449432373]], [[0.1726912260055542]], [[0.09942669421434402]], [[0.5014031529426575]], [[0.1313703954219818]], [[-0.47611188888549805]], [[-0.2700062394142151]], [[0.0]], [[-0.31944751739501953]], [[0.0]], [[0.48325157165527344]], [[-0.4467289447784424]], [[0.0]], [[0.029275942593812943]], [[-0.2075890302658081]], [[-0.4690970182418823]], [[-0.6201865673065186]], [[-0.1912488341331482]], [[-0.13182362914085388]], [[-0.16941915452480316]], [[-0.07242435216903687]], [[-0.0486835241317749]], [[-0.07213661074638367]], [[0.06593132764101028]], [[0.8244085907936096]], [[-0.29481396079063416]], [[-0.10321671515703201]], [[-0.46645405888557434]], [[0.0]], [[0.48325157165527344]], [[-0.6376004815101624]], [[-0.6106318235397339]], [[0.0]], [[0.48325157165527344]], [[-0.6376004815101624]], [[0.0]], [[-0.31944748759269714]], [[-0.3893623352050781]], [[-0.0706494152545929]], [[-0.2426307499408722]], [[-0.25242650508880615]], [[-0.31275713443756104]], [[-0.034305039793252945]], [[0.7856411933898926]], [[-0.40288686752319336]], [[-0.16131192445755005]], [[-0.4947929084300995]], [[-0.27302780747413635]], [[0.02113521657884121]], [[0.2666087746620178]], [[0.13472099602222443]], [[0.5987979173660278]], [[0.24381442368030548]], [[-0.401953786611557]], [[0.18335314095020294]], [[-1.0406076908111572]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fb176b91c10>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: we have to change how ``best'' is done and only visualize within our clusters, not\n",
    "# just sample from the cluster and visualize everything\n",
    "\n",
    "# also, it seems like abs(*) is not really helping??? Idk,\n",
    "\n",
    "cluster_idx = 0\n",
    "\n",
    "cluster_inds = [int(datapoints_used[int(cluster_by_idx[cluster_idx][i])]) for i in range(len(cluster_by_idx[cluster_idx]))]\n",
    "print(dataset[int(cluster_inds[0])]['text'][:100])\n",
    "\n",
    "new_dataset = []\n",
    "for i in cluster_inds:\n",
    "\tnew_dataset.append(dataset[i])\n",
    "# for i in range(len(cluster_by_idx[cluster_idx])):\n",
    "# \tprint(dataset[int(datapoints_used[int(cluster_by_idx[cluster_idx][i])])]['text'][:100])\n",
    "# \tprint(\"+++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "activations = model_activations[LAYER][cluster_inds, :, :]\n",
    "# .reshape(-1, model_activations[layer].shape[-1])[cluster, :], 1)\n",
    "print(activations.shape)\n",
    "\n",
    "# TODO: weighted correlations here\n",
    "text_list, full_text, token_list, full_token_list, indices = get_feature_datapoints_with_idx(NEURON, activations, model.tokenizer, token_amount, new_dataset, setting=\"uniform\", k=30)\n",
    "interp_utils.visualize_text(text_list, feature, model, None, layer=LAYER, setting=\"model\")\n",
    "# TODO: maybe do everything on MLP side where we get only positive activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
