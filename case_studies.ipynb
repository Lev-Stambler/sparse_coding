{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchorse/miniconda3/envs/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "import pickle\n",
    "\n",
    "# Define the autoencoder so pickle knows how to serialize it. \n",
    "# Later, we should actually save as a state_dict instead of a dumb pickle\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components, t_type=torch.float32, l1_coef=0.0):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Only defining the decoder layer, encoder will share its weights\n",
    "        self.decoder = nn.Linear(n_dict_components, activation_size, bias=True)\n",
    "        # Create a bias layer\n",
    "        self.encoder_bias= nn.Parameter(torch.zeros(n_dict_components))\n",
    "\n",
    "        \n",
    "        # Initialize the decoder weights orthogonally\n",
    "        nn.init.orthogonal_(self.decoder.weight)\n",
    "        self.decoder = self.decoder.to(t_type)\n",
    "\n",
    "        # Encoder is a Sequential with the ReLU activation\n",
    "        # No need to define a Linear layer for the encoder as its weights are tied with the decoder\n",
    "        self.encoder = nn.Sequential(nn.ReLU()).to(t_type)\n",
    "\n",
    "        self.l1_coef = l1_coef\n",
    "        self.activation_size = activation_size\n",
    "        self.n_dict_components = n_dict_components\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.encoder(x @ self.decoder.weight + self.encoder_bias)\n",
    "        # Apply unit norm constraint to the decoder weights\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "\n",
    "        # Decoding step as before\n",
    "        x_hat = self.decoder(c)\n",
    "        return x_hat, c\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n",
      "torch.Size([512, 512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([4096, 512])\n",
      "torch.Size([8192, 512])\n",
      "len of autoencoders:  1\n",
      "smaller_dict.shape:  torch.Size([4096, 512])\n",
      "larger_dict.shape:  torch.Size([8192, 512])\n"
     ]
    }
   ],
   "source": [
    "filename = \"ae4.pkl\"\n",
    "layer = 4\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Load the pickle file\n",
    "with open(filename, 'rb') as file:\n",
    "    autoencoders = pickle.load(file)\n",
    "\n",
    "# Index for l1 value, usually only 1 value is available\n",
    "l1_index = 0\n",
    "dictionaries = [autoencoder.decoder.weight.data.T for autoencoder in autoencoders[l1_index]]\n",
    "for d in dictionaries:\n",
    "    print(d.shape)\n",
    "print(\"len of autoencoders: \", len(autoencoders))\n",
    "dict_index = 3\n",
    "smaller_dict, larger_dict = dictionaries[dict_index], dictionaries[dict_index+1]\n",
    "smaller_auto_encoder, larger_auto_encoder = autoencoders[l1_index][dict_index], autoencoders[l1_index][dict_index+1]\n",
    "print(\"smaller_dict.shape: \", smaller_dict.shape)\n",
    "print(\"larger_dict.shape: \", larger_dict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# of features above 0.9:', 2717)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "#Dictionary Comparison\n",
    "smaller_dict_features, _ = smaller_dict.shape\n",
    "larger_dict_features, _ = larger_dict.shape\n",
    "larger_dict = larger_dict.to(device)\n",
    "# Hungary algorithm\n",
    "# Calculate all cosine similarities and store in a 2D array\n",
    "cos_sims = np.zeros((smaller_dict_features, larger_dict_features))\n",
    "for idx, vector in enumerate(smaller_dict):\n",
    "    cos_sims[idx] = torch.nn.functional.cosine_similarity(vector.to(device), larger_dict, dim=1).cpu().numpy()\n",
    "# Convert to a minimization problem\n",
    "cos_sims = 1 - cos_sims\n",
    "# Use the Hungarian algorithm to solve the assignment problem\n",
    "row_ind, col_ind = linear_sum_assignment(cos_sims)\n",
    "# Retrieve the max cosine similarities and corresponding indices\n",
    "max_cosine_similarities = 1 - cos_sims[row_ind, col_ind]\n",
    "\n",
    "# Get the indices of the max cosine similarities in descending order\n",
    "max_indices = np.argsort(max_cosine_similarities)[::-1].copy()\n",
    "print((\"# of features above 0.9:\", (max_cosine_similarities > .9).sum()))\n",
    "# Plot histogram of max_cosine_similarities\n",
    "plt.hist(max_cosine_similarities, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading metadata: 100%|██████████| 921/921 [00:00<00:00, 8.20MB/s]\n",
      "Downloading readme: 100%|██████████| 373/373 [00:00<00:00, 4.32MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 31.72 MiB, generated: 58.43 MiB, post-processed: Unknown size, total: 90.15 MiB) to /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 33.3M/33.3M [00:00<00:00, 56.5MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1694.67it/s]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount= 40\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:07<00:00, 20.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "# neurons = model.W_in.shape[-1]\n",
    "neurons = model.cfg.d_model\n",
    "datapoints = dataset.num_rows\n",
    "batch_size = 64\n",
    "neuron_activations = torch.zeros((datapoints*token_amount, neurons))\n",
    "dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features))\n",
    "smaller_auto_encoder = smaller_auto_encoder.to(device)\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        _, cache = model.run_with_cache(batch.to(device))\n",
    "        batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "        neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "        reconstruction, batched_dictionary_activations = smaller_auto_encoder(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints(feature_index, dictionary_activations, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    datapoint_indices =[np.unravel_index(i, (datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(model.tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = model.tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list\n",
    "\n",
    "def get_neuron_activation(token, feature, model, setting=\"dictionary_basis\"):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "        neuron_act_batch = cache[cache_name]\n",
    "        if setting==\"dictionary_basis\":\n",
    "            _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "            return act[0, :, feature].tolist()\n",
    "        else: # neuron/residual basis\n",
    "            return neuron_act_batch[0, :, feature].tolist()\n",
    "\n",
    "def ablate_text(text, feature, model, setting=\"plot\"):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    display_text_list = []\n",
    "    activation_list = []\n",
    "    for t in text:\n",
    "        # Convert text into tokens\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t equals tokens\n",
    "            tokens = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        seq_size = tokens.shape[1]\n",
    "        if(seq_size == 1): # If the text is a single token, we can't ablate it\n",
    "            continue\n",
    "        original = get_neuron_activation(tokens, feature, model)[-1]\n",
    "        changed_activations = torch.zeros(seq_size, device=device).cpu()\n",
    "        for i in range(seq_size):\n",
    "            # Remove the i'th token from the input\n",
    "            ablated_tokens = torch.cat((tokens[:,:i], tokens[:,i+1:]), dim=1)\n",
    "            changed_activations[i] += get_neuron_activation(ablated_tokens, feature, model)[-1]\n",
    "        changed_activations -= original\n",
    "        display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        activation_list += changed_activations.tolist() + [0.0]\n",
    "    activation_list = torch.tensor(activation_list).reshape(-1,1,1)\n",
    "    if setting == \"plot\":\n",
    "        return text_neuron_activations(tokens=display_text_list, activations=activation_list)\n",
    "    else:\n",
    "        return display_text_list, activation_list\n",
    "def visualize_text(text, feature, model, setting=\"dictionary_basis\", max_activation = None):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    if isinstance(feature, int):\n",
    "        feature = [feature]\n",
    "    display_text_list = []\n",
    "    act_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            token = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t are tokens\n",
    "            token = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        for f in feature:\n",
    "            display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            act_list += get_neuron_activation(token, f, model, setting) + [0.0]\n",
    "    act_list = torch.tensor(act_list).reshape(-1,1,1)\n",
    "    if(max_activation is not None):\n",
    "        act_list = torch.clamp(act_list, max=max_activation)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=act_list)\n",
    "# Ablate the feature direction of the tokens\n",
    "# token_list is a list of tokens, convert to tensor of shape (batch_size, seq_len)\n",
    "from einops import rearrange\n",
    "def ablate_feature_direction(tokens, feature, model, autoencoder, entire_feature_direction=False):\n",
    "    def mlp_ablation_hook(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "\n",
    "        # Run through the autoencoder\n",
    "        _, act = autoencoder(int_val)\n",
    "        feature_to_ablate = feature # TODO: bring this out of the function\n",
    "\n",
    "        # Subtract value with feature direction*act_of_feature\n",
    "        feature_direction = torch.outer(act[:, feature_to_ablate].squeeze(), autoencoder.decoder.weight[:, feature_to_ablate].squeeze())\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value -= feature_direction\n",
    "        return value\n",
    "    \n",
    "    def ablated_this_feature_dir(value, hook):\n",
    "        value -= feature\n",
    "        return value\n",
    "    if(entire_feature_direction):\n",
    "        return model.run_with_hooks(tokens, \n",
    "            fwd_hooks=[(\n",
    "                cache_name, \n",
    "                ablated_this_feature_dir\n",
    "                )]\n",
    "            )\n",
    "    else:\n",
    "        return model.run_with_hooks(tokens, \n",
    "            fwd_hooks=[(\n",
    "                cache_name, \n",
    "                mlp_ablation_hook\n",
    "                )]\n",
    "            )\n",
    "\n",
    "def add_feature_direction(tokens, feature, model, autoencoder, scalar=1.0):\n",
    "    def residual_add_hook(value, hook):\n",
    "        feature_direction = autoencoder.decoder.weight[:, feature].squeeze()\n",
    "        value += scalar*feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name,\n",
    "            residual_add_hook\n",
    "            )]\n",
    "        )\n",
    "def ablate_feature_direction_display(text, features=None, setting=\"true_tokens\", verbose=False, entire_feature_direction=False):\n",
    "\n",
    "    if features==None:\n",
    "        features = torch.tensor([best_feature])\n",
    "    if isinstance(features, int):\n",
    "        features = torch.tensor([features])\n",
    "    if isinstance(features, list):\n",
    "        features = torch.tensor(features)\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    text_list = []\n",
    "    logit_list = []\n",
    "    for t in text:\n",
    "        tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        with torch.no_grad():\n",
    "            original_logits = model(tokens).log_softmax(-1).cpu()\n",
    "            ablated_logits = ablate_feature_direction(tokens, features, model, smaller_auto_encoder, entire_feature_direction).log_softmax(-1).cpu()\n",
    "        diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "        tokens = tokens.cpu()\n",
    "        if setting == \"true_tokens\":\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            gather_tokens = rearrange(tokens[:,1:], \"b s -> b s 1\") # TODO: verify this is correct\n",
    "            # Gather the logits for the true tokens\n",
    "            diff = rearrange(diff_logits[:, :-1].gather(-1,gather_tokens), \"b s n -> (b s n)\")\n",
    "        elif setting == \"max\":\n",
    "            # Negate the diff_logits to see which tokens have the largest effect on the neuron\n",
    "            val, ind = (-1*diff_logits).max(-1)\n",
    "            diff = rearrange(val[:, :-1], \"b s -> (b s)\")\n",
    "            diff*= -1 # Negate the values gathered\n",
    "            split_text = model.to_str_tokens(ind, prepend_bos=False)\n",
    "            gather_tokens = rearrange(ind[:,1:], \"1 s -> 1 s 1\")\n",
    "        split_text = split_text[1:] # Remove the first token since we're not predicting it\n",
    "        if(verbose):\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            orig = rearrange(original_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            ablated = rearrange(ablated_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            logit_list += orig.tolist() + [0.0]\n",
    "            logit_list += ablated.tolist() + [0.0]\n",
    "        text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        logit_list += diff.tolist() + [0.0]\n",
    "    logit_list = torch.tensor(logit_list).reshape(-1,1,1)\n",
    "    if verbose:\n",
    "        print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
    "    return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
    "def generate_text(input_text, num_tokens, model, autoencoder, feature, temperature=0.7, setting=\"add\", scalar=1.0):\n",
    "    # Convert input text to tokens\n",
    "    input_ids = model.tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        # Generate logits\n",
    "        with torch.no_grad():\n",
    "            if(setting==\"add\"):\n",
    "                logits = add_feature_direction(input_ids, feature, model, autoencoder, scalar=scalar)\n",
    "            else:\n",
    "                logits = model(input_ids)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        predicted_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append predicted token to input_ids\n",
    "        input_ids = torch.cat((input_ids, predicted_token), dim=-1)\n",
    "\n",
    "    # Decode the tokens to text\n",
    "    output_text = model.tokenizer.decode(input_ids[0])\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Logit Lens\n",
    "def logit_lens(model, best_feature, smaller_dict, layer):\n",
    "    with torch.no_grad():\n",
    "        # There are never-used tokens, which have high norm. We want to ignore these.\n",
    "        bad_ind = (model.W_U.norm(dim=0) > 20)\n",
    "        feature_direction = smaller_dict[best_feature].to(device)\n",
    "        # feature_direction = torch.matmul(feature_direction, model.W_out[layer]) # if MLP\n",
    "        logits = torch.matmul(feature_direction, model.W_U).cpu()\n",
    "    # Don't include bad indices\n",
    "    logits[bad_ind] = -1000\n",
    "    topk_values, topk_indices = torch.topk(logits, 20)\n",
    "    top_text = model.to_str_tokens(topk_indices)\n",
    "    print(f\"{top_text}\")\n",
    "    print(topk_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: [2.71, 2.58, 2.32, 1.45, 1.39, 1.38, 1.15, 0.93, 0.42, 0.19]\n",
      "Feature_ids [3119, 2118, 3920, 3608, 3146, 50, 1978, 2016, 3190, 3025]\n"
     ]
    }
   ],
   "source": [
    "# t = \"いさんさん��に\"\n",
    "# t = \"我们一起去玩吧\"\n",
    "# t = \"for i in range\"\n",
    "t = \" I don't know about Dave'\"\n",
    "split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "token = model.to_tokens(t, prepend_bos=False)\n",
    "_, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "neuron_act_batch = cache[cache_name]\n",
    "_, act = smaller_auto_encoder(neuron_act_batch)\n",
    "v, i = act[0, -1, :].topk(10)\n",
    "\n",
    "print(\"Activations:\",[round(val,2) for val in v.tolist()])\n",
    "print(\"Feature_ids\", i.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: -1.8950436\n",
      "Feature index: 2118\n",
      "MCS: 0.9875518083572388\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-14cc2f6b-c387\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-14cc2f6b-c387\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"When\", \" firef\", \"ighter\", \" Randy\", \" Bog\", \"art\", \" per\", \"ishes\", \" in\", \" the\", \" ar\", \"son\", \" fire\", \" at\", \" an\", \" abandoned\", \" brewery\", \" known\", \" as\", \" the\", \" Brew\", \"master\", \"\\u2019\", \"s\", \" Wid\", \"ow\", \",\", \" Death\", \" Bog\", \"art\", \" and\", \" his\", \" girlfriend\", \" W\", \"ren\", \" Morgan\", \" travel\", \" to\", \" St\", \".\", \"\\n\", \"As\", \" Texas\", \" continues\", \" discour\", \"aging\", \" women\", \" from\", \" having\", \" abortions\", \" by\", \" making\", \" them\", \" jump\", \" through\", \" ho\", \"ops\", \",\", \" volunteer\", \"-\", \"led\", \" groups\", \" are\", \" spring\", \"ing\", \" into\", \" action\", \" to\", \" help\", \" women\", \" navigate\", \" the\", \" state\", \"\\u2019\", \"s\", \" unjust\", \" system\", \".\", \" This\", \" summer\", \",\", \"\\n\", \"M\", \"ario\", \" K\", \"art\", \" Arc\", \"ade\", \" GP\", \" DX\", \" USA\", \"/\", \"Europe\", \" Version\", \" Gets\", \" It\", \"\\u2019\", \"s\", \" First\", \" Major\", \" Update\", \" arc\", \"ade\", \"hero\", \"\\\\newline\", \"\\\\newline\", \"When\", \" it\", \" was\", \" first\", \" announced\", \",\", \" a\", \" unique\", \" aspect\", \" of\", \" Band\", \"ai\", \" Nam\", \"co\", \"\\u2019\", \"s\", \"\\n\", \"E\", \"conomic\", \" and\", \" Environmental\", \" Role\", \" of\", \" Wet\", \"lands\", \"\\\\newline\", \"\\\\newline\", \"Inter\", \"view\", \" with\", \" Nick\", \" Davidson\", \",\", \" Rams\", \"ar\", \" Convention\", \"\\u2019\", \"s\", \" Deputy\", \" Secretary\", \" General\", \" at\", \" CBD\", \",\", \" COP\", \"11\", \".\", \" The\", \" key\", \" role\", \" that\", \" rapidly\", \" diminishing\", \" wet\", \"lands\", \" play\", \" in\", \"\\n\", \"Lab\", \"our\", \"\\u2019\", \"s\", \" annual\", \" conference\", \" has\", \" voted\", \" to\", \" reject\", \" a\", \" bid\", \" to\", \" commit\", \" the\", \" party\", \" to\", \" campaigning\", \" for\", \" Rem\", \"ain\", \" in\", \" any\", \" future\", \" EU\", \" referendum\", \".\", \"\\\\newline\", \"\\\\newline\", \"The\", \" result\", \" bol\", \"sters\", \" the\", \" position\", \" of\", \" leader\", \" Jeremy\", \" Corbyn\", \",\", \"\\n\", \"Take\", \" 8\", \"%\", \" of\", \" your\", \" gross\", \" income\", \",\", \" and\", \" that\", \"\\u2019\", \"s\", \" your\", \" tax\", \" cut\", \".\", \" Take\", \" 1\", \".\", \"5\", \"%\", \" of\", \" the\", \" equity\", \" in\", \" your\", \" house\", \" and\", \" that\", \"\\u2019\", \"s\", \" the\", \" additional\", \" tax\", \" to\", \" pay\", \".\", \" This\", \" gives\", \" you\", \"\\n\", \"For\", \" those\", \" that\", \" missed\", \" it\", \" last\", \" night\", \",\", \" there\", \" was\", \" an\", \" incident\", \" during\", \" the\", \" game\", \" where\", \" J\", \"off\", \"rey\", \" L\", \"up\", \"ul\", \" tried\", \" to\", \" throw\", \" an\", \" elbow\", \" in\", \" Hen\", \"rik\", \" Sed\", \"in\", \"\\u2019\", \"s\", \" face\", \",\", \" missed\", \",\", \" and\", \" ended\", \"\\n\", \"Month\", \"ly\", \" Archives\", \":\", \" March\", \" 2014\", \"\\\\newline\", \"\\\\newline\", \"Ph\", \"ar\", \"rell\", \" Williams\", \" will\", \" replace\", \" judge\", \" C\", \"ee\", \" Lo\", \" Green\", \" on\", \" The\", \" Voice\", \" for\", \" it\", \"\\u2019\", \"s\", \" upcoming\", \" seventh\", \" season\", \".\", \" Williams\", \" was\", \" behind\", \" the\", \" two\", \" tracks\", \" that\", \" were\", \" each\", \" dubbed\", \"\\n\", \"If\", \" you\", \" are\", \" like\", \" the\", \" rest\", \" of\", \" our\", \" user\", \" community\", \",\", \" your\", \" IT\", \" team\", \" is\", \" busy\", \".\", \" With\", \" pressure\", \" to\", \" deliver\", \" on\", \"-\", \"time\", \" projects\", \",\", \" you\", \" don\", \"\\u2019\", \"t\", \" have\", \" a\", \" lot\", \" of\", \" time\", \" to\", \" spend\", \" making\", \" your\", \" management\", \"\\n\", \"Sim\", \"pl\", \"en\", \"ote\", \"\\\\newline\", \"\\\\newline\", \"Back\", \" in\", \" November\", \",\", \" I\", \" held\", \" up\", \" Apple\", \"\\u2019\", \"s\", \" own\", \" Notes\", \" app\", \" as\", \" a\", \" great\", \" example\", \" of\", \" iPhone\", \" software\", \" design\", \".\", \" I\", \" wrote\", \":\", \"\\\\newline\", \"\\\\newline\", \"I\", \"\\u2019\", \"ve\", \" looked\", \" at\", \" several\", \" note\", \"\\n\", \" PM\", \"\\\\newline\", \"How\", \" many\", \" minutes\", \" are\", \" there\", \" between\", \" 9\", \":\", \"57\", \" PM\", \" and\", \" 8\", \":\", \"28\", \" AM\", \"?\", \"\\\\newline\", \"6\", \"31\", \"\\\\newline\", \"What\", \" is\", \" 6\", \"34\", \" minutes\", \" after\", \" 3\", \":\", \"55\", \" AM\", \"?\", \"\\\\newline\", \"2\", \":\", \"29\", \" PM\", \"\\\\newline\", \"How\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.56967830657959]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[5.044563293457031]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.859464168548584]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.415582656860352]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.1410722732543945]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.092848062515259]], [[0.01955556869506836]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.001559019088745]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.2911603450775146]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.1467015743255615]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.285308599472046]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.623786211013794]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.03339886665344238]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[4.711648941040039]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.015518903732299805]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fde03e46fe0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "best_feature = int(max_indices[N])\n",
    "# best_feature = 2122     \n",
    "best_feature = 2118\n",
    "print(\"bias:\", smaller_auto_encoder.encoder_bias.detach().cpu().numpy()[best_feature])\n",
    "print(f\"Feature index: {best_feature}\")\n",
    "print(f\"MCS: {max_cosine_similarities[best_feature]}\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"max\")\n",
    "text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"uniform\")\n",
    "visualize_text(full_text, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-3a7d7a50-8ff3\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-3a7d7a50-8ff3\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"When\", \" firef\", \"ighter\", \" Randy\", \" Bog\", \"art\", \" per\", \"ishes\", \" in\", \" the\", \" ar\", \"son\", \" fire\", \" at\", \" an\", \" abandoned\", \" brewery\", \" known\", \" as\", \" the\", \" Brew\", \"master\", \"\\u2019\", \"\\n\", \"As\", \" Texas\", \" continues\", \" discour\", \"aging\", \" women\", \" from\", \" having\", \" abortions\", \" by\", \" making\", \" them\", \" jump\", \" through\", \" ho\", \"ops\", \",\", \" volunteer\", \"-\", \"led\", \" groups\", \" are\", \" spring\", \"ing\", \" into\", \" action\", \" to\", \" help\", \" women\", \" navigate\", \" the\", \" state\", \"\\u2019\", \"\\n\", \"M\", \"ario\", \" K\", \"art\", \" Arc\", \"ade\", \" GP\", \" DX\", \" USA\", \"/\", \"Europe\", \" Version\", \" Gets\", \" It\", \"\\u2019\", \"s\", \" First\", \" Major\", \" Update\", \" arc\", \"ade\", \"hero\", \"\\\\newline\", \"\\\\newline\", \"When\", \" it\", \" was\", \" first\", \" announced\", \",\", \" a\", \" unique\", \" aspect\", \" of\", \" Band\", \"ai\", \" Nam\", \"co\", \"\\u2019\", \"\\n\", \"E\", \"conomic\", \" and\", \" Environmental\", \" Role\", \" of\", \" Wet\", \"lands\", \"\\\\newline\", \"\\\\newline\", \"Inter\", \"view\", \" with\", \" Nick\", \" Davidson\", \",\", \" Rams\", \"ar\", \" Convention\", \"\\u2019\", \"\\n\", \"Lab\", \"our\", \"\\u2019\", \"\\n\", \"Take\", \" 8\", \"%\", \" of\", \" your\", \" gross\", \" income\", \",\", \" and\", \" that\", \"\\u2019\", \"\\n\", \"For\", \" those\", \" that\", \" missed\", \" it\", \" last\", \" night\", \",\", \" there\", \" was\", \" an\", \" incident\", \" during\", \" the\", \" game\", \" where\", \" J\", \"off\", \"rey\", \" L\", \"up\", \"ul\", \" tried\", \" to\", \" throw\", \" an\", \" elbow\", \" in\", \" Hen\", \"rik\", \" Sed\", \"in\", \"\\u2019\", \"\\n\", \"Month\", \"ly\", \" Archives\", \":\", \" March\", \" 2014\", \"\\\\newline\", \"\\\\newline\", \"Ph\", \"ar\", \"rell\", \" Williams\", \" will\", \" replace\", \" judge\", \" C\", \"ee\", \" Lo\", \" Green\", \" on\", \" The\", \" Voice\", \" for\", \" it\", \"\\u2019\", \"\\n\", \"If\", \" you\", \" are\", \" like\", \" the\", \" rest\", \" of\", \" our\", \" user\", \" community\", \",\", \" your\", \" IT\", \" team\", \" is\", \" busy\", \".\", \" With\", \" pressure\", \" to\", \" deliver\", \" on\", \"-\", \"time\", \" projects\", \",\", \" you\", \" don\", \"\\u2019\", \"\\n\", \" PM\", \"\\\\newline\", \"How\", \" many\", \" minutes\", \" are\", \" there\", \" between\", \" 9\", \":\", \"57\", \" PM\", \" and\", \" 8\", \":\", \"28\", \" AM\", \"?\", \"\\\\newline\", \"6\", \"31\", \"\\\\newline\", \"What\", \" is\", \" 6\", \"34\", \" minutes\", \" after\", \" 3\", \":\", \"55\", \" AM\", \"\\n\"], \"activations\": [[[-0.4147911071777344]], [[0.033303260803222656]], [[-0.03009033203125]], [[-0.011828422546386719]], [[0.0671834945678711]], [[-0.0190887451171875]], [[-0.13498306274414062]], [[-0.16710662841796875]], [[0.08392143249511719]], [[-0.050136566162109375]], [[0.03223896026611328]], [[0.016422271728515625]], [[0.052359580993652344]], [[-0.02048969268798828]], [[-0.13088417053222656]], [[0.028026580810546875]], [[0.02032470703125]], [[0.1023406982421875]], [[0.38961029052734375]], [[-0.3816032409667969]], [[-0.3953666687011719]], [[-0.2170562744140625]], [[-5.569450378417969]], [[0.0]], [[-0.26692867279052734]], [[0.06635856628417969]], [[-0.04314136505126953]], [[-0.1974506378173828]], [[-0.01953411102294922]], [[-0.01355743408203125]], [[0.1389780044555664]], [[0.07016468048095703]], [[0.028228759765625]], [[0.0878896713256836]], [[0.035973548889160156]], [[0.09770870208740234]], [[0.025358200073242188]], [[0.02124500274658203]], [[0.00045871734619140625]], [[0.03925895690917969]], [[0.10550308227539062]], [[0.032578468322753906]], [[-0.12959766387939453]], [[0.07614803314208984]], [[-0.00940704345703125]], [[0.011202812194824219]], [[0.011364936828613281]], [[0.033336639404296875]], [[0.00345611572265625]], [[0.03615379333496094]], [[-0.10370731353759766]], [[-0.033150672912597656]], [[-0.008514404296875]], [[-0.3086891174316406]], [[-0.32793235778808594]], [[-3.226051092147827]], [[-5.044563293457031]], [[0.0]], [[-0.02121734619140625]], [[0.011850357055664062]], [[-0.054119110107421875]], [[0.009981155395507812]], [[-0.025796890258789062]], [[0.002414703369140625]], [[0.0007467269897460938]], [[-0.038649559020996094]], [[0.014245986938476562]], [[-0.09551048278808594]], [[0.012648582458496094]], [[-0.05109596252441406]], [[0.10442733764648438]], [[-0.0720987319946289]], [[0.1835155487060547]], [[-0.4379293918609619]], [[-0.13193893432617188]], [[-0.0641937255859375]], [[-0.06918144226074219]], [[-0.0684518814086914]], [[-0.058055877685546875]], [[-0.15524578094482422]], [[-0.1083364486694336]], [[-0.1083364486694336]], [[-0.10956096649169922]], [[-0.08167552947998047]], [[0.03200054168701172]], [[-0.10749053955078125]], [[-0.08952617645263672]], [[0.003032684326171875]], [[-0.10417366027832031]], [[-0.10966873168945312]], [[-0.06806659698486328]], [[-0.11576175689697266]], [[-0.24140644073486328]], [[0.03530693054199219]], [[0.1737499237060547]], [[0.37047481536865234]], [[-4.415582656860352]], [[0.0]], [[-0.04204845428466797]], [[-0.07823848724365234]], [[0.06496620178222656]], [[0.01850414276123047]], [[-0.045975685119628906]], [[0.09149646759033203]], [[0.09209060668945312]], [[-0.025168418884277344]], [[-0.26282525062561035]], [[-0.26282525062561035]], [[-0.07957744598388672]], [[-0.03244495391845703]], [[-0.16113018989562988]], [[0.1622762680053711]], [[0.08076763153076172]], [[0.22397422790527344]], [[-0.2396237850189209]], [[-0.20929503440856934]], [[0.09166145324707031]], [[-4.141079902648926]], [[0.0]], [[-0.5295009613037109]], [[-0.0844569206237793]], [[-3.0928213596343994]], [[0.0]], [[-0.790308952331543]], [[-0.19108867645263672]], [[-0.17941808700561523]], [[-0.08229780197143555]], [[-0.15848112106323242]], [[0.11407947540283203]], [[-0.08924484252929688]], [[-0.04918622970581055]], [[-0.050531864166259766]], [[-1.846672773361206]], [[-3.001559019088745]], [[0.0]], [[0.10052919387817383]], [[0.04699563980102539]], [[0.008214950561523438]], [[-0.017881393432617188]], [[0.03352975845336914]], [[-0.005101203918457031]], [[0.006818294525146484]], [[-0.02324390411376953]], [[-0.02463674545288086]], [[0.060273170471191406]], [[0.0008296966552734375]], [[-0.02359294891357422]], [[0.07421207427978516]], [[0.03063678741455078]], [[0.11656856536865234]], [[0.06574630737304688]], [[-0.029229164123535156]], [[-0.02932119369506836]], [[-0.021455764770507812]], [[0.026271343231201172]], [[-0.01383209228515625]], [[0.004368782043457031]], [[0.14267921447753906]], [[0.04726362228393555]], [[0.09111976623535156]], [[0.02030801773071289]], [[0.05481433868408203]], [[0.05492544174194336]], [[-0.11885571479797363]], [[0.24988746643066406]], [[0.44416332244873047]], [[2.0567219257354736]], [[-2.1467015743255615]], [[0.0]], [[0.23827314376831055]], [[-0.03020334243774414]], [[0.021411418914794922]], [[0.04077291488647461]], [[-0.05419301986694336]], [[0.10062789916992188]], [[0.03411054611206055]], [[0.03411054611206055]], [[-0.25730443000793457]], [[-0.02462315559387207]], [[-0.01146697998046875]], [[-0.020006895065307617]], [[-0.16910696029663086]], [[0.06328964233398438]], [[-0.13455677032470703]], [[-0.0915679931640625]], [[-0.031255245208740234]], [[0.1313648223876953]], [[0.047510623931884766]], [[0.09459590911865234]], [[0.23122239112854004]], [[-0.08354902267456055]], [[-0.39063024520874023]], [[2.3122239112854004]], [[-1.2852718830108643]], [[0.0]], [[-0.09481573104858398]], [[0.08236074447631836]], [[-0.009677410125732422]], [[0.04484701156616211]], [[0.02223825454711914]], [[0.013573169708251953]], [[0.021271944046020508]], [[0.02715778350830078]], [[0.021785259246826172]], [[-0.023834705352783203]], [[0.07473134994506836]], [[0.005399465560913086]], [[0.06224560737609863]], [[0.002654552459716797]], [[-0.05496811866760254]], [[0.023654460906982422]], [[-0.04164600372314453]], [[0.13976025581359863]], [[-0.08873128890991211]], [[0.024742603302001953]], [[-0.009945869445800781]], [[0.0641481876373291]], [[-0.06184196472167969]], [[-0.03606104850769043]], [[0.22683405876159668]], [[-0.020221233367919922]], [[0.28421926498413086]], [[-0.6238234043121338]], [[-0.6238234043121338]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fdb804594b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_text(text_list, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-32b6472e-2690\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-32b6472e-2690\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\" firef\", \"ighter\", \" Randy\", \" Bog\", \"art\", \" per\", \"ishes\", \" in\", \" the\", \" ar\", \"son\", \" fire\", \" at\", \" an\", \" abandoned\", \" brewery\", \" known\", \" as\", \" the\", \" Brew\", \"master\", \"\\u2019\", \"s\", \" Wid\", \"ow\", \",\", \" Death\", \" Bog\", \"art\", \" and\", \" his\", \" girlfriend\", \" W\", \"ren\", \" Morgan\", \" travel\", \" to\", \" St\", \".\", \"\\n\", \" Texas\", \" continues\", \" discour\", \"aging\", \" women\", \" from\", \" having\", \" abortions\", \" by\", \" making\", \" them\", \" jump\", \" through\", \" ho\", \"ops\", \",\", \" volunteer\", \"-\", \"led\", \" groups\", \" are\", \" spring\", \"ing\", \" into\", \" action\", \" to\", \" help\", \" women\", \" navigate\", \" the\", \" state\", \"\\u2019\", \"s\", \" unjust\", \" system\", \".\", \" This\", \" summer\", \",\", \"\\n\", \"ario\", \" K\", \"art\", \" Arc\", \"ade\", \" GP\", \" DX\", \" USA\", \"/\", \"Europe\", \" Version\", \" Gets\", \" It\", \"\\u2019\", \"s\", \" First\", \" Major\", \" Update\", \" arc\", \"ade\", \"hero\", \"\\\\newline\", \"\\\\newline\", \"When\", \" it\", \" was\", \" first\", \" announced\", \",\", \" a\", \" unique\", \" aspect\", \" of\", \" Band\", \"ai\", \" Nam\", \"co\", \"\\u2019\", \"s\", \"\\n\", \"conomic\", \" and\", \" Environmental\", \" Role\", \" of\", \" Wet\", \"lands\", \"\\\\newline\", \"\\\\newline\", \"Inter\", \"view\", \" with\", \" Nick\", \" Davidson\", \",\", \" Rams\", \"ar\", \" Convention\", \"\\u2019\", \"s\", \" Deputy\", \" Secretary\", \" General\", \" at\", \" CBD\", \",\", \" COP\", \"11\", \".\", \" The\", \" key\", \" role\", \" that\", \" rapidly\", \" diminishing\", \" wet\", \"lands\", \" play\", \" in\", \"\\n\", \"our\", \"\\u2019\", \"s\", \" annual\", \" conference\", \" has\", \" voted\", \" to\", \" reject\", \" a\", \" bid\", \" to\", \" commit\", \" the\", \" party\", \" to\", \" campaigning\", \" for\", \" Rem\", \"ain\", \" in\", \" any\", \" future\", \" EU\", \" referendum\", \".\", \"\\\\newline\", \"\\\\newline\", \"The\", \" result\", \" bol\", \"sters\", \" the\", \" position\", \" of\", \" leader\", \" Jeremy\", \" Corbyn\", \",\", \"\\n\", \" 8\", \"%\", \" of\", \" your\", \" gross\", \" income\", \",\", \" and\", \" that\", \"\\u2019\", \"s\", \" your\", \" tax\", \" cut\", \".\", \" Take\", \" 1\", \".\", \"5\", \"%\", \" of\", \" the\", \" equity\", \" in\", \" your\", \" house\", \" and\", \" that\", \"\\u2019\", \"s\", \" the\", \" additional\", \" tax\", \" to\", \" pay\", \".\", \" This\", \" gives\", \" you\", \"\\n\", \" those\", \" that\", \" missed\", \" it\", \" last\", \" night\", \",\", \" there\", \" was\", \" an\", \" incident\", \" during\", \" the\", \" game\", \" where\", \" J\", \"off\", \"rey\", \" L\", \"up\", \"ul\", \" tried\", \" to\", \" throw\", \" an\", \" elbow\", \" in\", \" Hen\", \"rik\", \" Sed\", \"in\", \"\\u2019\", \"s\", \" face\", \",\", \" missed\", \",\", \" and\", \" ended\", \"\\n\", \"ly\", \" Archives\", \":\", \" March\", \" 2014\", \"\\\\newline\", \"\\\\newline\", \"Ph\", \"ar\", \"rell\", \" Williams\", \" will\", \" replace\", \" judge\", \" C\", \"ee\", \" Lo\", \" Green\", \" on\", \" The\", \" Voice\", \" for\", \" it\", \"\\u2019\", \"s\", \" upcoming\", \" seventh\", \" season\", \".\", \" Williams\", \" was\", \" behind\", \" the\", \" two\", \" tracks\", \" that\", \" were\", \" each\", \" dubbed\", \"\\n\", \" you\", \" are\", \" like\", \" the\", \" rest\", \" of\", \" our\", \" user\", \" community\", \",\", \" your\", \" IT\", \" team\", \" is\", \" busy\", \".\", \" With\", \" pressure\", \" to\", \" deliver\", \" on\", \"-\", \"time\", \" projects\", \",\", \" you\", \" don\", \"\\u2019\", \"t\", \" have\", \" a\", \" lot\", \" of\", \" time\", \" to\", \" spend\", \" making\", \" your\", \" management\", \"\\n\", \"pl\", \"en\", \"ote\", \"\\\\newline\", \"\\\\newline\", \"Back\", \" in\", \" November\", \",\", \" I\", \" held\", \" up\", \" Apple\", \"\\u2019\", \"s\", \" own\", \" Notes\", \" app\", \" as\", \" a\", \" great\", \" example\", \" of\", \" iPhone\", \" software\", \" design\", \".\", \" I\", \" wrote\", \":\", \"\\\\newline\", \"\\\\newline\", \"I\", \"\\u2019\", \"ve\", \" looked\", \" at\", \" several\", \" note\", \"\\n\", \"\\\\newline\", \"How\", \" many\", \" minutes\", \" are\", \" there\", \" between\", \" 9\", \":\", \"57\", \" PM\", \" and\", \" 8\", \":\", \"28\", \" AM\", \"?\", \"\\\\newline\", \"6\", \"31\", \"\\\\newline\", \"What\", \" is\", \" 6\", \"34\", \" minutes\", \" after\", \" 3\", \":\", \"55\", \" AM\", \"?\", \"\\\\newline\", \"2\", \":\", \"29\", \" PM\", \"\\\\newline\", \"How\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[-0.0015802383422851562]], [[-0.0005464553833007812]], [[0.0037093162536621094]], [[0.0]], [[-0.00011181831359863281]], [[-7.152557373046875e-07]], [[-0.0003561973571777344]], [[0.0004420280456542969]], [[0.0]], [[2.3365020751953125e-05]], [[-0.000152587890625]], [[-0.0007104873657226562]], [[-0.0010585784912109375]], [[0.00029277801513671875]], [[0.0009570121765136719]], [[0.0]], [[-0.0020700693130493164]], [[0.0009684562683105469]], [[0.0]], [[0.00013208389282226562]], [[-5.513659954071045]], [[4.76837158203125e-06]], [[2.2649765014648438e-06]], [[-9.417533874511719e-05]], [[7.62939453125e-06]], [[2.002716064453125e-05]], [[0.0]], [[9.5367431640625e-07]], [[-1.5139579772949219e-05]], [[-2.384185791015625e-06]], [[5.245208740234375e-06]], [[0.0]], [[3.814697265625e-06]], [[0.0]], [[-5.960464477539063e-08]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0009326934814453125]], [[-7.915496826171875e-05]], [[0.0]], [[0.0004329681396484375]], [[-0.00014215707778930664]], [[-0.00017404556274414062]], [[-0.0017828941345214844]], [[0.0]], [[0.0020923614501953125]], [[-7.426738739013672e-05]], [[0.0]], [[0.0]], [[0.0022211074829101562]], [[0.0]], [[1.4901161193847656e-05]], [[0.003620147705078125]], [[9.584426879882812e-05]], [[0.0]], [[0.0027990341186523438]], [[-0.00017786026000976562]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.001592874526977539]], [[-0.0002808570861816406]], [[0.0]], [[0.001261591911315918]], [[0.0]], [[-0.003571152687072754]], [[-4.159342288970947]], [[5.7220458984375e-06]], [[-1.9073486328125e-06]], [[-1.3709068298339844e-06]], [[-1.621246337890625e-05]], [[0.0]], [[1.1920928955078125e-07]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-6.580352783203125e-05]], [[0.00214385986328125]], [[-0.0005788803100585938]], [[0.00017833709716796875]], [[-0.0006351470947265625]], [[0.0021581649780273438]], [[0.0]], [[-0.000339508056640625]], [[9.5367431640625e-05]], [[-0.4680235981941223]], [[-0.0005793571472167969]], [[0.0024509429931640625]], [[0.0003209114074707031]], [[-0.0023365020751953125]], [[0.0]], [[5.7220458984375e-06]], [[8.392333984375e-05]], [[1.6987323760986328e-05]], [[-0.0006222724914550781]], [[-0.0005090236663818359]], [[0.0]], [[-0.0008456707000732422]], [[0.0]], [[0.0002053976058959961]], [[-0.0008816719055175781]], [[-0.003498077392578125]], [[-0.0027976036071777344]], [[0.00018320977687835693]], [[0.024003982543945312]], [[0.0]], [[-0.016431093215942383]], [[0.0]], [[0.005116701126098633]], [[-3.694756031036377]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.00014257431030273438]], [[-0.0005941390991210938]], [[0.0]], [[8.296966552734375e-05]], [[0.0]], [[-0.00028514862060546875]], [[0.0]], [[-3.7550926208496094e-05]], [[0.0013666152954101562]], [[-0.0029087066650390625]], [[0.0]], [[-0.0015974044799804688]], [[0.0]], [[-0.0006093978881835938]], [[-0.001323699951171875]], [[-3.2622807025909424]], [[-0.004496574401855469]], [[7.319450378417969e-05]], [[0.0019407272338867188]], [[0.013699054718017578]], [[4.76837158203125e-06]], [[-0.0001952648162841797]], [[1.9073486328125e-06]], [[9.5367431640625e-07]], [[-1.9073486328125e-06]], [[0.0]], [[1.9073486328125e-06]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-3.5762786865234375e-06]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-4.081376075744629]], [[0.0062961578369140625]], [[-3.1948089599609375e-05]], [[-0.04736042022705078]], [[0.0]], [[-0.0005828738212585449]], [[7.152557373046875e-06]], [[-0.00014090538024902344]], [[5.7220458984375e-06]], [[-6.079673767089844e-06]], [[-4.76837158203125e-06]], [[-4.76837158203125e-07]], [[-1.6689300537109375e-06]], [[-1.3113021850585938e-06]], [[0.0]], [[9.28640365600586e-05]], [[-3.814697265625e-06]], [[0.0]], [[-8.106231689453125e-06]], [[-4.76837158203125e-07]], [[-7.152557373046875e-06]], [[-0.0001811981201171875]], [[3.24249267578125e-05]], [[0.00010573863983154297]], [[-5.704164505004883e-05]], [[0.0]], [[0.0]], [[-2.86102294921875e-06]], [[0.0]], [[0.0]], [[0.0]], [[1.9073486328125e-06]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0006480216979980469]], [[-0.0003021359443664551]], [[0.0016045570373535156]], [[0.0]], [[0.0]], [[0.0]], [[-0.0005483627319335938]], [[-0.004096269607543945]], [[-0.0008654594421386719]], [[-0.26280900835990906]], [[-8.177757263183594e-05]], [[-0.0006310939788818359]], [[0.0]], [[3.933906555175781e-05]], [[-0.0018138885498046875]], [[0.00054931640625]], [[2.384185791015625e-07]], [[0.0]], [[0.0]], [[-0.00024828314781188965]], [[-0.00023984909057617188]], [[0.0]], [[-0.00010585784912109375]], [[0.0006651878356933594]], [[0.0]], [[-0.00025177001953125]], [[-0.005936622619628906]], [[0.0]], [[-0.11901907622814178]], [[2.384185791015625e-07]], [[2.384185791015625e-06]], [[4.76837158203125e-07]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.00011301040649414062]], [[-1.049041748046875e-05]], [[-0.00011396408081054688]], [[-0.00011396408081054688]], [[-0.00011038780212402344]], [[1.3768672943115234e-05]], [[0.0006206035614013672]], [[1.3589859008789062e-05]], [[-7.343292236328125e-05]], [[-0.0008907318115234375]], [[-8.249282836914062e-05]], [[-3.6954879760742188e-06]], [[0.0]], [[-7.271766662597656e-05]], [[0.0018315315246582031]], [[0.0]], [[0.0]], [[-0.0010790824890136719]], [[0.0]], [[0.0]], [[-0.0007205009460449219]], [[-3.039836883544922e-06]], [[-0.000759124755859375]], [[4.649162292480469e-05]], [[0.0]], [[0.00012445449829101562]], [[0.007232666015625]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-1.9421569108963013]], [[-5.7697296142578125e-05]], [[-0.00042176246643066406]], [[-1.9073486328125e-06]], [[0.0]], [[0.0004737973213195801]], [[-4.291534423828125e-06]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.00019550323486328125]], [[0.0]], [[0.0]], [[0.0]], [[0.0005464553833007812]], [[0.0]], [[0.0]], [[0.0]], [[1.1920928955078125e-05]], [[-0.0008149147033691406]], [[-0.00034809112548828125]], [[0.001968860626220703]], [[0.0]], [[0.0]], [[0.0]], [[0.0001838207244873047]], [[0.004108905792236328]], [[-0.0006475448608398438]], [[-0.00018906593322753906]], [[0.00046443939208984375]], [[1.1682510375976562e-05]], [[-0.16987740993499756]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.049041748046875e-05]], [[-2.3603439331054688e-05]], [[4.57763671875e-05]], [[2.868473529815674e-06]], [[-0.00010466575622558594]], [[0.0]], [[0.00011873245239257812]], [[0.0]], [[-8.58306884765625e-05]], [[-3.814697265625e-06]], [[-0.00019085407257080078]], [[3.457069396972656e-05]], [[0.000331878662109375]], [[5.0067901611328125e-06]], [[0.0]], [[-0.0009241104125976562]], [[-5.5789947509765625e-05]], [[-0.00013637542724609375]], [[-0.0018982887268066406]], [[9.393692016601562e-05]], [[0.0]], [[0.0]], [[3.0994415283203125e-06]], [[-0.00025403499603271484]], [[0.0]], [[0.0]], [[2.3481043172068894e-05]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.009154319763183594]], [[0.00016498565673828125]], [[-0.0010333061218261719]], [[5.435943603515625e-05]], [[2.7418136596679688e-06]], [[0.00032711029052734375]], [[-4.5299530029296875e-06]], [[0.00011157989501953125]], [[-9.119510650634766e-05]], [[-0.00020360946655273438]], [[6.67572021484375e-06]], [[0.0007326602935791016]], [[-0.0007114410400390625]], [[2.765655517578125e-05]], [[-2.706895112991333]], [[0.005921363830566406]], [[4.38690185546875e-05]], [[-7.62939453125e-06]], [[3.814697265625e-06]], [[-4.0531158447265625e-06]], [[-5.7220458984375e-06]], [[-9.5367431640625e-06]], [[-1.1622905731201172e-06]], [[-1.1444091796875e-05]], [[2.09808349609375e-05]], [[2.384185791015625e-06]], [[2.980232238769531e-07]], [[1.1563301086425781e-05]], [[-4.76837158203125e-07]], [[9.059906005859375e-06]], [[-2.980232238769531e-07]], [[-1.4975666999816895e-06]], [[9.5367431640625e-06]], [[6.198883056640625e-06]], [[0.001365959644317627]], [[2.86102294921875e-06]], [[-3.4570693969726562e-06]], [[1.9073486328125e-06]], [[4.76837158203125e-07]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fdb7d98ded0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_feature_direction_display(full_text, best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'teenth', 'sburg', '’.', '14514500', 'soever', 'ssä', 'sat', 'rsquo', '’,', 'ses', '.’', '’', 'sie', '�', 'll', '�', ' ”', \"'):\", 'inous']\n",
      "tensor([4.8057, 2.5318, 2.4812, 1.9314, 1.9189, 1.9169, 1.9084, 1.8943, 1.8872,\n",
      "        1.8796, 1.8780, 1.8302, 1.8170, 1.8099, 1.7680, 1.7668, 1.7366, 1.7358,\n",
      "        1.7151, 1.6975])\n"
     ]
    }
   ],
   "source": [
    "logit_lens(model,best_feature, smaller_dict, layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
