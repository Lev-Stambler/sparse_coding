{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchorse/miniconda3/envs/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "import pickle\n",
    "\n",
    "# Define the autoencoder so pickle knows how to serialize it. \n",
    "# Later, we should actually save as a state_dict instead of a dumb pickle\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components, t_type=torch.float32, l1_coef=0.0):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Only defining the decoder layer, encoder will share its weights\n",
    "        self.decoder = nn.Linear(n_dict_components, activation_size, bias=True)\n",
    "        \n",
    "        # Create a bias layer\n",
    "        self.encoder_bias= nn.Parameter(torch.zeros(n_dict_components))\n",
    "        \n",
    "        # Initialize the decoder weights orthogonally\n",
    "        nn.init.orthogonal_(self.decoder.weight)\n",
    "        self.decoder = self.decoder.to(t_type)\n",
    "\n",
    "        # Encoder is a Sequential with the ReLU activation\n",
    "        # No need to define a Linear layer for the encoder as its weights are tied with the decoder\n",
    "        self.encoder = nn.Sequential(nn.ReLU()).to(t_type)\n",
    "\n",
    "        self.l1_coef = l1_coef\n",
    "        self.activation_size = activation_size\n",
    "        self.n_dict_components = n_dict_components\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.encoder(x @ self.decoder.weight + self.encoder_bias)\n",
    "        # Apply unit norm constraint to the decoder weights\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "\n",
    "        # Decoding step as before\n",
    "        x_hat = self.decoder(c)\n",
    "        return x_hat, c\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l1_r8/_9/learned_dicts.pt\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l2_r4/_19/learned_dicts.pt\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m auto_num \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# Selects which specific autoencoder to use\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m all_autoencoders \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(autoencoder_filename)\n\u001b[1;32m     10\u001b[0m num_dictionaries \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(all_autoencoders)\n\u001b[1;32m     11\u001b[0m autoencoder, hyperparams \u001b[39m=\u001b[39m all_autoencoders[auto_num]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoder_filename' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "import pickle\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l1_r8/_9/learned_dicts.pt\"\n",
    "# autoencoder_filename = \"/home/mchorse/logan/sparse_coding/output_sweep_tied_mlpout_l2_r4/_19/learned_dicts.pt\"\n",
    "auto_num = 0 # Selects which specific autoencoder to use\n",
    "all_autoencoders = torch.load(autoencoder_filename)\n",
    "num_dictionaries = len(all_autoencoders)\n",
    "autoencoder, hyperparams = all_autoencoders[auto_num]\n",
    "l1_alpha = hyperparams['l1_alpha']\n",
    "autoencoder2, hyperparams2 = all_autoencoders[auto_num+1]\n",
    "smaller_dict = autoencoder.get_learned_dict()\n",
    "larger_dict = autoencoder2.get_learned_dict()\n",
    "\n",
    "#Change these settings to load the correct autoencoder\n",
    "layer = 2\n",
    "setting = \"residual\"\n",
    "# setting = \"attention\"\n",
    "# setting = \"mlp\"\n",
    "# setting = \"mlp_out\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# model_name = \"EleutherAI/pythia-160m\"\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "elif setting == \"attention\":\n",
    "    cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp_out\":\n",
    "    cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "    neurons = model.cfg.d_model\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_pca = \"/mnt/ssd-cluster/baselines/l4_residual/pca_topk.pt\"\n",
    "pca_topk = torch.load(filename_pca)\n",
    "filename_ica = \"/mnt/ssd-cluster/baselines/l4_residual/ica_topk.pt\"\n",
    "ica_topk = torch.load(filename_ica)\n",
    "pca_dict = pca_topk.get_learned_dict()\n",
    "ica_dict = ica_topk.get_learned_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n",
      "torch.Size([512, 512])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([2048, 512])\n",
      "torch.Size([4096, 512])\n",
      "torch.Size([8192, 512])\n",
      "len of autoencoders:  1\n",
      "smaller_dict.shape:  torch.Size([1024, 512])\n",
      "larger_dict.shape:  torch.Size([2048, 512])\n"
     ]
    }
   ],
   "source": [
    "filename = \"ae4.pkl\"\n",
    "layer = 4\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# Load the pickle file\n",
    "with open(filename, 'rb') as file:\n",
    "    autoencoders = pickle.load(file)\n",
    "\n",
    "# Index for l1 value, usually only 1 value is available\n",
    "l1_index = 0\n",
    "dictionaries = [autoencoder.decoder.weight.data.T for autoencoder in autoencoders[l1_index]]\n",
    "for d in dictionaries:\n",
    "    print(d.shape)\n",
    "print(\"len of autoencoders: \", len(autoencoders))\n",
    "dict_index = 1\n",
    "smaller_dict, larger_dict = dictionaries[dict_index], dictionaries[dict_index+1]\n",
    "smaller_auto_encoder, larger_auto_encoder = autoencoders[l1_index][dict_index], autoencoders[l1_index][dict_index+1]\n",
    "print(\"smaller_dict.shape: \", smaller_dict.shape)\n",
    "print(\"larger_dict.shape: \", larger_dict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# of features above 0.9:', 680)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "#Dictionary Comparison\n",
    "smaller_dict_features, _ = smaller_dict.shape\n",
    "larger_dict_features, _ = larger_dict.shape\n",
    "larger_dict = larger_dict.to(device)\n",
    "# Hungary algorithm\n",
    "# Calculate all cosine similarities and store in a 2D array\n",
    "cos_sims = np.zeros((smaller_dict_features, larger_dict_features))\n",
    "for idx, vector in enumerate(smaller_dict):\n",
    "    cos_sims[idx] = torch.nn.functional.cosine_similarity(vector.to(device), larger_dict, dim=1).cpu().numpy()\n",
    "# Convert to a minimization problem\n",
    "cos_sims = 1 - cos_sims\n",
    "# Use the Hungarian algorithm to solve the assignment problem\n",
    "row_ind, col_ind = linear_sum_assignment(cos_sims)\n",
    "# Retrieve the max cosine similarities and corresponding indices\n",
    "max_cosine_similarities = 1 - cos_sims[row_ind, col_ind]\n",
    "\n",
    "# Get the indices of the max cosine similarities in descending order\n",
    "max_indices = np.argsort(max_cosine_similarities)[::-1].copy()\n",
    "print((\"# of features above 0.9:\", (max_cosine_similarities > .9).sum()))\n",
    "# Plot histogram of max_cosine_similarities\n",
    "plt.hist(max_cosine_similarities, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3d918a3b8bb67278.arrow\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-936b79d074ce1a9e.arrow\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-5899fa893faa3f67.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount= 40\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [00:07<00:00, 22.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "# neurons = model.W_in.shape[-1]\n",
    "neurons = model.cfg.d_model\n",
    "datapoints = dataset.num_rows\n",
    "batch_size = 64\n",
    "neuron_activations = torch.zeros((datapoints*token_amount, neurons))\n",
    "dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features))\n",
    "pca_dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features))\n",
    "ica_dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features//2))\n",
    "smaller_auto_encoder = smaller_auto_encoder.to(device)\n",
    "pca_topk.to_device(device)\n",
    "ica_topk.to_device(device)\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        _, cache = model.run_with_cache(batch.to(device))\n",
    "        batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "        neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "        # normal dictionary\n",
    "        reconstruction, batched_dictionary_activations = smaller_auto_encoder(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu()\n",
    "        # pca dictionary\n",
    "        batched_pca_dictionary_activations = pca_topk.encode(batched_neuron_activations)\n",
    "        pca_dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_pca_dictionary_activations.cpu()\n",
    "        # ica dictionary\n",
    "        batched_ica_dictionary_activations = ica_topk.encode(batched_neuron_activations)\n",
    "        ica_dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_ica_dictionary_activations.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints(feature_index, dictionary_activations, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    datapoint_indices =[np.unravel_index(i, (datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(model.tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = model.tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list\n",
    "\n",
    "def get_neuron_activation(token, feature, model, setting=\"dictionary_basis\"):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "        neuron_act_batch = cache[cache_name]\n",
    "        if setting==\"dictionary_basis\":\n",
    "            _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "            return act[0, :, feature].tolist()\n",
    "        else: # neuron/residual basis\n",
    "            return neuron_act_batch[0, :, feature].tolist()\n",
    "\n",
    "def ablate_text(text, feature, model, setting=\"plot\"):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    display_text_list = []\n",
    "    activation_list = []\n",
    "    for t in text:\n",
    "        # Convert text into tokens\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t equals tokens\n",
    "            tokens = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        seq_size = tokens.shape[1]\n",
    "        if(seq_size == 1): # If the text is a single token, we can't ablate it\n",
    "            continue\n",
    "        original = get_neuron_activation(tokens, feature, model)[-1]\n",
    "        changed_activations = torch.zeros(seq_size, device=device).cpu()\n",
    "        for i in range(seq_size):\n",
    "            # Remove the i'th token from the input\n",
    "            ablated_tokens = torch.cat((tokens[:,:i], tokens[:,i+1:]), dim=1)\n",
    "            changed_activations[i] += get_neuron_activation(ablated_tokens, feature, model)[-1]\n",
    "        changed_activations -= original\n",
    "        display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        activation_list += changed_activations.tolist() + [0.0]\n",
    "    activation_list = torch.tensor(activation_list).reshape(-1,1,1)\n",
    "    if setting == \"plot\":\n",
    "        return text_neuron_activations(tokens=display_text_list, activations=activation_list)\n",
    "    else:\n",
    "        return display_text_list, activation_list\n",
    "def visualize_text(text, feature, model, setting=\"dictionary_basis\", max_activation = None):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    if isinstance(feature, int):\n",
    "        feature = [feature]\n",
    "    display_text_list = []\n",
    "    act_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            token = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t are tokens\n",
    "            token = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        for f in feature:\n",
    "            display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            act_list += get_neuron_activation(token, f, model, setting) + [0.0]\n",
    "    act_list = torch.tensor(act_list).reshape(-1,1,1)\n",
    "    if(max_activation is not None):\n",
    "        act_list = torch.clamp(act_list, max=max_activation)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=act_list)\n",
    "# Ablate the feature direction of the tokens\n",
    "# token_list is a list of tokens, convert to tensor of shape (batch_size, seq_len)\n",
    "from einops import rearrange\n",
    "def ablate_feature_direction(tokens, feature, model, autoencoder, entire_feature_direction=False):\n",
    "    def mlp_ablation_hook(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "\n",
    "        # Run through the autoencoder\n",
    "        _, act = autoencoder(int_val)\n",
    "        feature_to_ablate = feature # TODO: bring this out of the function\n",
    "\n",
    "        # Subtract value with feature direction*act_of_feature\n",
    "        feature_direction = torch.outer(act[:, feature_to_ablate].squeeze(), autoencoder.decoder.weight[:, feature_to_ablate].squeeze())\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value -= feature_direction\n",
    "        return value\n",
    "    \n",
    "    def ablated_this_feature_dir(value, hook):\n",
    "        value -= feature\n",
    "        return value\n",
    "    if(entire_feature_direction):\n",
    "        return model.run_with_hooks(tokens, \n",
    "            fwd_hooks=[(\n",
    "                cache_name, \n",
    "                ablated_this_feature_dir\n",
    "                )]\n",
    "            )\n",
    "    else:\n",
    "        return model.run_with_hooks(tokens, \n",
    "            fwd_hooks=[(\n",
    "                cache_name, \n",
    "                mlp_ablation_hook\n",
    "                )]\n",
    "            )\n",
    "\n",
    "def add_feature_direction(tokens, feature, model, autoencoder, scalar=1.0):\n",
    "    def residual_add_hook(value, hook):\n",
    "        feature_direction = autoencoder.decoder.weight[:, feature].squeeze()\n",
    "        value += scalar*feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name,\n",
    "            residual_add_hook\n",
    "            )]\n",
    "        )\n",
    "def ablate_feature_direction_display(text, features=None, setting=\"true_tokens\", verbose=False, entire_feature_direction=False):\n",
    "\n",
    "    if features==None:\n",
    "        features = torch.tensor([best_feature])\n",
    "    if isinstance(features, int):\n",
    "        features = torch.tensor([features])\n",
    "    if isinstance(features, list):\n",
    "        features = torch.tensor(features)\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    text_list = []\n",
    "    logit_list = []\n",
    "    for t in text:\n",
    "        tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        with torch.no_grad():\n",
    "            original_logits = model(tokens).log_softmax(-1).cpu()\n",
    "            ablated_logits = ablate_feature_direction(tokens, features, model, smaller_auto_encoder, entire_feature_direction).log_softmax(-1).cpu()\n",
    "        diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "        tokens = tokens.cpu()\n",
    "        if setting == \"true_tokens\":\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            gather_tokens = rearrange(tokens[:,1:], \"b s -> b s 1\") # TODO: verify this is correct\n",
    "            # Gather the logits for the true tokens\n",
    "            diff = rearrange(diff_logits[:, :-1].gather(-1,gather_tokens), \"b s n -> (b s n)\")\n",
    "        elif setting == \"max\":\n",
    "            # Negate the diff_logits to see which tokens have the largest effect on the neuron\n",
    "            val, ind = (-1*diff_logits).max(-1)\n",
    "            diff = rearrange(val[:, :-1], \"b s -> (b s)\")\n",
    "            diff*= -1 # Negate the values gathered\n",
    "            split_text = model.to_str_tokens(ind, prepend_bos=False)\n",
    "            gather_tokens = rearrange(ind[:,1:], \"1 s -> 1 s 1\")\n",
    "        split_text = split_text[1:] # Remove the first token since we're not predicting it\n",
    "        if(verbose):\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            orig = rearrange(original_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            ablated = rearrange(ablated_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            logit_list += orig.tolist() + [0.0]\n",
    "            logit_list += ablated.tolist() + [0.0]\n",
    "        text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        logit_list += diff.tolist() + [0.0]\n",
    "    logit_list = torch.tensor(logit_list).reshape(-1,1,1)\n",
    "    if verbose:\n",
    "        print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
    "    return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
    "def generate_text(input_text, num_tokens, model, autoencoder, feature, temperature=0.7, setting=\"add\", scalar=1.0):\n",
    "    # Convert input text to tokens\n",
    "    input_ids = model.tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        # Generate logits\n",
    "        with torch.no_grad():\n",
    "            if(setting==\"add\"):\n",
    "                logits = add_feature_direction(input_ids, feature, model, autoencoder, scalar=scalar)\n",
    "            else:\n",
    "                logits = model(input_ids)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        predicted_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append predicted token to input_ids\n",
    "        input_ids = torch.cat((input_ids, predicted_token), dim=-1)\n",
    "\n",
    "    # Decode the tokens to text\n",
    "    output_text = model.tokenizer.decode(input_ids[0])\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Logit Lens\n",
    "def logit_lens(model, best_feature, smaller_dict, layer):\n",
    "    with torch.no_grad():\n",
    "        # There are never-used tokens, which have high norm. We want to ignore these.\n",
    "        bad_ind = (model.W_U.norm(dim=0) > 20)\n",
    "        feature_direction = smaller_dict[best_feature].to(device)\n",
    "        # feature_direction = torch.matmul(feature_direction, model.W_out[layer]) # if MLP\n",
    "        logits = torch.matmul(feature_direction, model.W_U).cpu()\n",
    "    # Don't include bad indices\n",
    "    logits[bad_ind] = -1000\n",
    "    topk_values, topk_indices = torch.topk(logits, 20)\n",
    "    top_text = model.to_str_tokens(topk_indices)\n",
    "    print(f\"{top_text}\")\n",
    "    print(topk_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a tokenizer like this (for example, from HuggingFace's tokenizers)\n",
    "  # Replace with your tokenizer\n",
    "\n",
    "def char_to_token_position(tokenized_output, char_idx):\n",
    "    \"\"\"\n",
    "    Given a char-level index in the original string, find the corresponding token-level index.\n",
    "    \"\"\"\n",
    "    token_positions = tokenized_output.char_to_token(char_idx)\n",
    "    return token_positions\n",
    "\n",
    "def find_text_in_regex(tokens, regex_pattern):\n",
    "    \"\"\"\n",
    "    Find tokens inside parentheses and return token-level start and end indices.\n",
    "    \"\"\"\n",
    "    detokenized_text = model.tokenizer.decode(tokens.input_ids)\n",
    "    \n",
    "    # Find matches in the detokenized text\n",
    "    pattern = re.compile(regex_pattern)\n",
    "    matches = pattern.finditer(detokenized_text)\n",
    "    \n",
    "    token_positions = []\n",
    "    for match in matches:\n",
    "        start_char, end_char = match.span()\n",
    "        start_token = char_to_token_position(tokens, start_char)\n",
    "        end_token = char_to_token_position(tokens, end_char)\n",
    "        token_positions.append((start_token, end_token))\n",
    "    return token_positions\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def prepare_data(dictionary_activations, feature_index, num_bins=10):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    min_value = torch.min(best_feature_activations)\n",
    "    max_value = torch.max(best_feature_activations)\n",
    "    bin_boundaries = torch.linspace(min_value, max_value, num_bins+1)\n",
    "    bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "    return best_feature_activations, bin_boundaries, bins\n",
    "\n",
    "def compute_bar_positions(bin_boundaries):\n",
    "    barWidth = (bin_boundaries[1] - bin_boundaries[0]) * 0.8\n",
    "    r = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n",
    "    return r, barWidth\n",
    "\n",
    "def process_data_type1(bins, dataset, num_bins, num_unique_tokens, model):\n",
    "    nonzero = bins.nonzero()[:,0]\n",
    "    datapoint_indices = [np.unravel_index(i, (datapoints, token_amount)) for i in nonzero]\n",
    "    max_token = [dataset[int(md)][\"input_ids\"][int(s_ind)] for md, s_ind in datapoint_indices]\n",
    "    \n",
    "    ac = Counter(max_token)\n",
    "    max_tokens = [token_ind for token_ind, count in ac.most_common(num_unique_tokens)]\n",
    "    token_dict = {token: np.zeros(num_bins) for token in max_tokens}\n",
    "    misc_count = np.zeros(num_bins)\n",
    "\n",
    "    for i in range(1, num_bins+1):\n",
    "        bin_tokens_ind = (bins == i).nonzero()[:,0]\n",
    "        datapoint_indices = [np.unravel_index(ind, (datapoints, token_amount)) for ind in bin_tokens_ind]\n",
    "        max_token = [dataset[int(md)][\"input_ids\"][int(s_ind)] for md, s_ind in datapoint_indices]\n",
    "        ac = Counter(max_token)\n",
    "        for token_ind, count in ac.most_common(10):\n",
    "            if token_ind in max_tokens:\n",
    "                token_dict[token_ind][i-1] = count\n",
    "            else:\n",
    "                misc_count[i-1] += count\n",
    "\n",
    "    return token_dict, misc_count, max_tokens\n",
    "\n",
    "def process_data_type2(bins, dataset, num_bins, masks):\n",
    "    regex_matches_vs_nonmatches = np.zeros((num_bins, 2))\n",
    "    all_non_matched_tokens = []\n",
    "    non_matched_all_tokens = []\n",
    "    for i in range(1,num_bins+1):\n",
    "        bin_tokens_ind = (bins == i).nonzero()[:,0]\n",
    "        datapoint_indices = [np.unravel_index(i, (datapoints, token_amount)) for i in bin_tokens_ind]\n",
    "        for md, s_ind in datapoint_indices:\n",
    "            md = int(md)\n",
    "            s_ind = int(s_ind)\n",
    "            regex_matches_vs_nonmatches[i-1, masks[md][s_ind]] += 1\n",
    "            if not masks[md][s_ind]:\n",
    "                all_non_matched_tokens.append(dataset[md][\"input_ids\"][s_ind])\n",
    "                non_matched_all_tokens.append(dataset[md][\"input_ids\"][:s_ind+1])\n",
    "    return regex_matches_vs_nonmatches, all_non_matched_tokens\n",
    "\n",
    "def plot_data_type1(r, barWidth, bin_boundaries, data, model):\n",
    "    token_dict, misc_count, max_tokens = data\n",
    "    colors = sns.color_palette('colorblind', len(max_tokens) + 1)\n",
    "    running_sum = np.zeros(len(bin_boundaries) - 1)\n",
    "    for i in range(len(max_tokens)):\n",
    "        token_id_name = max_tokens[i]\n",
    "        token_count_array = token_dict[token_id_name]\n",
    "        text_label = model.tokenizer.decode(token_id_name)\n",
    "        plt.bar(r, token_count_array, bottom=running_sum, label=text_label, color=colors[i], width=barWidth)\n",
    "        running_sum += token_count_array\n",
    "    plt.bar(r, misc_count, bottom=running_sum, label='[All Other Tokens]', color=colors[-1], width=barWidth)\n",
    "    plt.title(f'Token Count for Feature {feature_index} Across Activation Ranges')\n",
    "    plt.xlabel('Activation')\n",
    "    plt.xticks(bin_boundaries, [f\"{val:.2f}\" for val in bin_boundaries])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_type2(r, barWidth, bin_boundaries, data, regex_pattern):\n",
    "    regex_matches_vs_nonmatches, all_non_matched_tokens = data\n",
    "    colors = sns.color_palette('deep', regex_matches_vs_nonmatches.shape[1] + 1)\n",
    "    running_sum = np.zeros(len(bin_boundaries) - 1)\n",
    "    for i in range(regex_matches_vs_nonmatches.shape[1]):\n",
    "        text_label = regex_pattern if i == 0 else \"non-matches\"\n",
    "        token_count_array = regex_matches_vs_nonmatches[:, (i+1)%2]\n",
    "        plt.bar(r, token_count_array, bottom=running_sum, label=text_label, color=colors[i], width=barWidth)\n",
    "        running_sum += token_count_array\n",
    "    plt.title(f'Token Count for Feature {feature_index} Across Activation Ranges')\n",
    "    plt.xlabel('Activation')\n",
    "    plt.xticks(bin_boundaries, [f\"{val:.2f}\" for val in bin_boundaries])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_masks(dataset, model, feature_index, token_amount, regex_pattern, extra_token_amount=2):\n",
    "    \"\"\"\n",
    "    Generates masks based on a regex pattern.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: dataset containing the texts to be searched\n",
    "    - model: the model containing the tokenizer\n",
    "    - feature_index: index of the feature (not used in the function based on given code, but kept for consistency)\n",
    "    - token_amount: limit to number of tokens to be considered from the tokenized text\n",
    "    - regex_pattern: the regular expression pattern to be searched for\n",
    "    - extra_token_amount: additional tokens to consider\n",
    "\n",
    "    Returns:\n",
    "    - masks: a list of masks generated based on the regex_pattern\n",
    "    \"\"\"\n",
    "    texts = dataset[\"text\"]\n",
    "    masks = []\n",
    "\n",
    "    for text_ind, text in enumerate(texts):\n",
    "        tokens = model.tokenizer(text)\n",
    "        tokens['input_ids'] = tokens['input_ids'][:token_amount + extra_token_amount]\n",
    "        token_positions = find_text_in_regex(tokens, regex_pattern=regex_pattern)\n",
    "        mask = np.zeros(token_amount, dtype=int)  # Initial mask\n",
    "        for start, end in token_positions:\n",
    "            if end is None:\n",
    "                print(\"No match found for text: \", model.tokenizer.decode(tokens.input_ids))\n",
    "                print(start, end)\n",
    "                continue\n",
    "            if end > token_amount:\n",
    "                end = token_amount\n",
    "            mask[start:end] = 1\n",
    "        masks.append(mask)\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: [3.25, 2.13, 1.63, 1.32, 1.01, 0.96, 0.4, 0.31, 0.3, 0.25]\n",
      "Feature_ids [966, 928, 290, 832, 346, 994, 655, 199, 756, 385]\n",
      "PCA: [6.43, 6.32, 3.74, 3.05, 2.78, 2.67, 2.39, 1.92, 1.9, 1.6]\n",
      "PCA_ids [513, 0, 1022, 514, 3, 4, 521, 53, 519, 5]\n",
      "ICA: [4.55, 0.09, 0.06, 0.06, 0.06, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "ICA_ids [339, 421, 216, 234, 456, 311, 232, 319, 55, 53]\n"
     ]
    }
   ],
   "source": [
    "# Feature Search\n",
    "# t = \" Bratwurst, Sauerkraut, und Bier\"\n",
    "# t = \"我们一起去玩吧\"\n",
    "# t = \"for i in range\"\n",
    "t = \" I don't know about Dave'\"\n",
    "# t = \" I like to eat them (apples and oranges\"\n",
    "# t = \" I like to eat them at National Institute of Justice (NIJ\"\n",
    "# t = \" I like to eat them at National Institute of Justice (\"\n",
    "# t = \" I like to eat them but (NIJ\"\n",
    "# t = \"いさんさん��に\"\n",
    "split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "token = model.to_tokens(t, prepend_bos=False)\n",
    "_, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "neuron_act_batch = cache[cache_name]\n",
    "_, act = smaller_auto_encoder(neuron_act_batch)\n",
    "pca_act = pca_topk.encode(neuron_act_batch.squeeze())\n",
    "ica_act = ica_topk.encode(neuron_act_batch.squeeze())\n",
    "v, i = act[0, -1, :].topk(10)\n",
    "pv, pi = pca_act[-1, :].topk(10)\n",
    "iv, ii = ica_act[-1, :].topk(10)\n",
    "\n",
    "print(\"Activations:\",[round(val,2) for val in v.tolist()])\n",
    "print(\"Feature_ids\", i.tolist())\n",
    "print(\"PCA:\",[round(val,2) for val in pv.tolist()])\n",
    "print(\"PCA_ids\", pi.tolist())\n",
    "print(\"ICA:\",[round(val,2) for val in iv.tolist()])\n",
    "print(\"ICA_ids\", ii.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_act_batch.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(308060)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m activations, bin_boundaries, bins \u001b[39m=\u001b[39m prepare_data(pca_dictionary_activations, feature_index)\n\u001b[1;32m      8\u001b[0m r, barWidth \u001b[39m=\u001b[39m compute_bar_positions(bin_boundaries)\n\u001b[0;32m----> 9\u001b[0m data1 \u001b[39m=\u001b[39m process_data_type1(bins, dataset, num_bins, num_unique_tokens\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m     10\u001b[0m plot_data_type1(r, barWidth, bin_boundaries, data1, model)\n",
      "Cell \u001b[0;32mIn[19], line 54\u001b[0m, in \u001b[0;36mprocess_data_type1\u001b[0;34m(bins, dataset, num_bins, num_unique_tokens, model)\u001b[0m\n\u001b[1;32m     52\u001b[0m nonzero \u001b[39m=\u001b[39m bins\u001b[39m.\u001b[39mnonzero()[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m datapoint_indices \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39munravel_index(i, (datapoints, token_amount)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nonzero]\n\u001b[0;32m---> 54\u001b[0m max_token \u001b[39m=\u001b[39m [dataset[\u001b[39mint\u001b[39m(md)][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mint\u001b[39m(s_ind)] \u001b[39mfor\u001b[39;00m md, s_ind \u001b[39min\u001b[39;00m datapoint_indices]\n\u001b[1;32m     56\u001b[0m ac \u001b[39m=\u001b[39m Counter(max_token)\n\u001b[1;32m     57\u001b[0m max_tokens \u001b[39m=\u001b[39m [token_ind \u001b[39mfor\u001b[39;00m token_ind, count \u001b[39min\u001b[39;00m ac\u001b[39m.\u001b[39mmost_common(num_unique_tokens)]\n",
      "Cell \u001b[0;32mIn[19], line 54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m nonzero \u001b[39m=\u001b[39m bins\u001b[39m.\u001b[39mnonzero()[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m datapoint_indices \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39munravel_index(i, (datapoints, token_amount)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nonzero]\n\u001b[0;32m---> 54\u001b[0m max_token \u001b[39m=\u001b[39m [dataset[\u001b[39mint\u001b[39;49m(md)][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mint\u001b[39m(s_ind)] \u001b[39mfor\u001b[39;00m md, s_ind \u001b[39min\u001b[39;00m datapoint_indices]\n\u001b[1;32m     56\u001b[0m ac \u001b[39m=\u001b[39m Counter(max_token)\n\u001b[1;32m     57\u001b[0m max_tokens \u001b[39m=\u001b[39m [token_ind \u001b[39mfor\u001b[39;00m token_ind, count \u001b[39min\u001b[39;00m ac\u001b[39m.\u001b[39mmost_common(num_unique_tokens)]\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/datasets/arrow_dataset.py:2778\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2778\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/datasets/arrow_dataset.py:2763\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2762\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2763\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2764\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   2765\u001b[0m )\n\u001b[1;32m   2766\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/datasets/formatting/formatting.py:624\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    622\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    625\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    626\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/datasets/formatting/formatting.py:396\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable, query_type: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_row(pa_table)\n\u001b[1;32m    397\u001b[0m     \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    398\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/datasets/formatting/formatting.py:431\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlazy:\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m LazyRow(pa_table, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 431\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpython_arrow_extractor()\u001b[39m.\u001b[39;49mextract_row(pa_table)\n\u001b[1;32m    432\u001b[0m row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_row(row)\n\u001b[1;32m    433\u001b[0m \u001b[39mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/datasets/formatting/formatting.py:144\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_row\u001b[39m(\u001b[39mself\u001b[39m, pa_table: pa\u001b[39m.\u001b[39mTable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mreturn\u001b[39;00m _unnest(pa_table\u001b[39m.\u001b[39;49mto_pydict())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Token level\n",
    "# feature_index = 966 # dictionary\n",
    "feature_index = 513 # PCA\n",
    "# feature_index = 966 # ICA\n",
    "num_bins=10\n",
    "# activations, bin_boundaries, bins = prepare_data(dictionary_activations, feature_index)\n",
    "activations, bin_boundaries, bins = prepare_data(pca_dictionary_activations, feature_index)\n",
    "r, barWidth = compute_bar_positions(bin_boundaries)\n",
    "data1 = process_data_type1(bins, dataset, num_bins, num_unique_tokens=5, model=model)\n",
    "plot_data_type1(r, barWidth, bin_boundaries, data1, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_index = 2808\n",
    "# regex_pattern =  r'(?<=\\().*?(?=\\))'\n",
    "regex_pattern = r'\\((?=[A-Z]{1,})'\n",
    "masks = generate_masks(dataset, model, feature_index, token_amount, regex_pattern)\n",
    "\n",
    "activations, bin_boundaries, bins = prepare_data(dictionary_activations, feature_index)\n",
    "r, barWidth = compute_bar_positions(bin_boundaries)\n",
    "data2 = process_data_type2(bins, dataset, num_bins, masks)\n",
    "plot_data_type2(r, barWidth, bin_boundaries, data2, regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992,) (292,) (1705,)\n",
      "Matches: 992, False Positives: 292, False Negatives: 1705\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-16b2bebe-a15a\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-16b2bebe-a15a\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"But\", \" something\", \" is\", \" happening\", \" here\", \" and\", \" you\", \" don\", \"\\u2019\", \"t\", \" know\", \" what\", \" it\", \" is\", \"\\\\newline\", \"\\\\newline\", \"Do\", \" you\", \",\", \" Mr\", \",\", \" Jones\", \"?\", \" (\", \"\\n\", \"Story\", \" highlights\", \" To\", \"ys\", \" \\\"\", \"R\", \"\\\"\", \" Us\", \" Canada\", \" is\", \" searching\", \" for\", \" a\", \" new\", \" chief\", \" play\", \" officer\", \"\\\\newline\", \"\\\\newline\", \"Alex\", \" Th\", \"orne\", \",\", \" 13\", \",\", \" is\", \" stepping\", \" down\", \"\\\\newline\", \"\\\\newline\", \"(\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"The\", \" purpose\", \" of\", \" this\", \" article\", \" is\", \" to\", \" study\", \" the\", \" problem\", \" of\", \" finding\", \" sharp\", \" lower\", \" bounds\", \" for\", \" the\", \" norm\", \" of\", \" the\", \" product\", \" of\", \" polynomials\", \" in\", \" the\", \" ultra\", \"products\", \" of\", \" Banach\", \" spaces\", \" $(\", \"\\n\", \"J\", \"ake\", \" Jones\", \"\\\\newline\", \"\\\\newline\", \"James\", \" Mur\", \"rell\", \" \\\"\", \"J\", \"ake\", \"\\\"\", \" Jones\", \" (\", \"\\n\", \"T\", \"ar\", \"ab\", \"ou\", \"ra\", \"\\\\newline\", \"\\\\newline\", \"T\", \"ar\", \"ab\", \"ou\", \"ra\", \" (\", \"\\n\", \"In\", \" the\", \" Community\", \"\\\\newline\", \"\\\\newline\", \"Near\", \"by\", \" Schools\", \"\\\\newline\", \"\\\\newline\", \"3\", \"208\", \" Per\", \"dot\", \" Avenue\", \",\", \" Ros\", \"amond\", \",\", \" CA\", \" 9\", \"35\", \"60\", \" (\", \"\\n\", \"E\", \"och\", \"ry\", \"sis\", \",\", \" a\", \" new\", \" replacement\", \" name\", \" for\", \" the\", \" fossil\", \" Pro\", \"to\", \"ch\", \"ry\", \"sis\", \" B\", \"isch\", \"off\", \",\", \" 1916\", \" (\", \"\\n\", \"A\", \"ra\", \"uc\", \"aria\", \" clonal\", \" forest\", \"ry\", \":\", \" types\", \" of\", \" cut\", \"t\", \"ings\", \" and\", \" mother\", \" tree\", \" sex\", \" in\", \" field\", \" survival\", \" and\", \" growth\", \"\\\\newline\", \"\\\\newline\", \"Res\", \"um\", \"o\", \":\", \"\\\\newline\", \"\\\\newline\", \"A\", \"ra\", \"uc\", \"aria\", \" ang\", \"ust\", \"if\", \"olia\", \" (\", \"\\n\", \"\\\\newline\", \"#\", \"if\", \"!\", \"defined\", \"\\n\", \"Div\", \"esting\", \" Of\", \" K\", \"ru\", \"ger\", \"\\u2019\", \"s\", \" Cash\", \" (\", \"\\n\", \"Rob\", \"ins\", \"ons\", \" ready\", \" to\", \" roll\", \"\\\\newline\", \"\\\\newline\", \"Tw\", \"ins\", \" Ty\", \"rell\", \" and\", \" Ty\", \"ree\", \" Robinson\", \" making\", \" their\", \" marks\", \" in\", \" football\", \",\", \" basketball\", \"\\\\newline\", \"\\\\newline\", \"A\", \" quick\", \" look\", \" at\", \" twin\", \" brothers\", \" Ty\", \"ree\", \" and\", \" Ty\", \"rell\", \" Robinson\", \" (\", \"\\n\", \"M\", \"oss\", \" (\", \"\\n\", \"Poly\", \"(\", \"\\n\", \"We\", \" now\", \" require\", \" registration\", \" to\", \" download\", \" high\", \" resolution\", \" fan\", \" art\", \".\", \" Please\", \" take\", \" a\", \" few\", \" seconds\", \" to\", \" register\", \" absolutely\", \" free\", \"!\", \" Click\", \" here\", \" now\", \".\", \" (\", \"\\n\", \"PE\", \"OR\", \"IA\", \",\", \" Ill\", \".\", \" (\", \"\\n\", \"A\", \"val\", \"anch\", \"a\", \" de\", \" \\u00c9\", \"xit\", \"os\", \"\\\\newline\", \"\\\\newline\", \"A\", \"val\", \"anch\", \"a\", \" de\", \" \\u00c9\", \"xit\", \"os\", \" (\", \"\\n\", \"Anthony\", \" I\", \"lu\", \"obe\", \"\\\\newline\", \"\\\\newline\", \"Chief\", \" Anthony\", \" I\", \"lu\", \"obe\", \" (\", \"\\n\", \"Anthony\", \" I\", \"lu\", \"obe\", \"\\\\newline\", \"\\\\newline\", \"Chief\", \" Anthony\", \" I\", \"lu\", \"obe\", \" (\", \"JP\", \")\", \" was\", \" born\", \" in\", \" 1945\", \" to\", \" the\", \" family\", \" of\", \" Chief\", \" and\", \" Mrs\", \" Joseph\", \" Ag\", \"im\", \"hel\", \"en\", \" I\", \"lu\", \"obe\", \" (\", \"\\n\", \"(\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"union\", \" all\", \" in\", \" SQL\", \" (\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.01338648796081543]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7ff81414b610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_nonzero, feature_nonzero = np.array(masks).flatten().nonzero()[0], dictionary_activations[:, feature_index].numpy().nonzero()[0]\n",
    "index_matches = np.intersect1d(regex_nonzero, feature_nonzero)\n",
    "false_positives = np.setdiff1d(regex_nonzero, index_matches)\n",
    "false_negatives = np.setdiff1d(feature_nonzero, index_matches)\n",
    "print(index_matches.shape, false_positives.shape, false_negatives.shape)\n",
    "print(f\"Matches: {index_matches.shape[0]}, False Positives: {false_positives.shape[0]}, False Negatives: {false_negatives.shape[0]}\")\n",
    "found_indices = false_positives[:20]\n",
    "\n",
    "datapoint_indices =[np.unravel_index(i, (datapoints, token_amount)) for i in found_indices]\n",
    "text_list = []\n",
    "full_text = []\n",
    "token_list = []\n",
    "full_token_list = []\n",
    "for md, s_ind in datapoint_indices:\n",
    "    md = int(md)\n",
    "    s_ind = int(s_ind)\n",
    "    full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "    full_text.append(model.tokenizer.decode(full_tok))\n",
    "    tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "    text = model.tokenizer.decode(tok)\n",
    "    text_list.append(text)\n",
    "    token_list.append(tok)\n",
    "    full_token_list.append(full_tok)\n",
    "# text_list, full_text, token_list, full_token_list\n",
    "visualize_text(text_list, feature_index, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias: -1.4355048\n",
      "Feature index: 2\n",
      "MCS: 0.904906690120697\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-2fb762e2-a6f3\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-2fb762e2-a6f3\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"Education\", \" Week\", \" reporter\", \" Ben\", \" Her\", \"old\", \" explores\", \" how\", \" technology\", \" is\", \" shaping\", \" teaching\", \" and\", \" learning\", \" and\", \" the\", \" management\", \" of\", \" schools\", \".\", \" Join\", \" the\", \" discussion\", \" as\", \" he\", \" analy\", \"zes\", \" the\", \" latest\", \" developments\", \".\", \"\\\\newline\", \"\\n\", \"1\", \" Family\", \" Practice\", \" (\", \"House\", \").\", \"\\\\newline\", \"\\\\newline\", \"Figure\", \" Metal\", \" hip\", \" prosthesis\", \" causing\", \" cobalt\", \" intoxication\", \" Show\", \" full\", \" caption\", \" Rem\", \"oved\", \" metal\", \" head\", \" with\", \" hole\", \" due\", \" to\", \" severe\", \" metal\", \" loss\", \".\", \"\\\\newline\", \"\\n\", \"B\", \"illion\", \"aire\", \" returns\", \" top\", \" Australian\", \" award\", \"\\\\newline\", \"\\\\newline\", \"February\", \" 25\", \",\", \" 2008\", \" 10\", \":\", \"00\", \"am\", \"\\\\newline\", \"\\\\newline\", \"An\", \" Australian\", \" billionaire\", \" philanth\", \"rop\", \"ist\", \" returned\", \" the\", \" highest\", \" civic\", \" honor\", \" awarded\", \" by\", \" his\", \" country\", \".\", \"\\\\newline\", \"\\n\", \"Federal\", \" prosecutors\", \" are\", \" investigating\", \" interactions\", \" between\", \" vendors\", \" and\", \" officials\", \" at\", \" President\", \" Trump\", \"\\u2019\", \"s\", \" inaugural\", \" committee\", \",\", \" the\", \" Wall\", \" Street\", \" Journal\", \" reports\", \".\", \"\\\\newline\", \"\\n\", \"Black\", \" Beauty\", \" has\", \" found\", \" her\", \" forever\", \" home\", \".\", \" Cong\", \"ratulations\", \" Beauty\", \"!\", \"\\\\newline\", \"\\\\newline\", \"Hi\", \",\", \" My\", \" name\", \" is\", \" Black\", \" Beauty\", \"!\", \"\\\\newline\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Personal\", \" Val\", \"gr\", \"ind\", \" An\", \"omaly\", \"\\\\newline\", \"\\\\newline\", \"First\", \" I\", \" would\", \" like\", \" to\", \" thank\", \" in\", \" you\", \" in\", \" advance\", \" for\", \" any\", \" help\", \" in\", \" this\", \" matter\", \".\", \"\\\\newline\", \"The\", \" Val\", \"\\n\", \"Tr\", \"um\", \"ai\", \" language\", \"\\\\newline\", \"\\\\newline\", \"Tr\", \"um\", \"ai\", \" is\", \" an\", \" endangered\", \" language\", \" isolate\", \" of\", \" Brazil\", \".\", \" Most\", \" Tr\", \"\\n\", \"R\", \"oles\", \" of\", \" Ca\", \"2\", \"+\", \" and\", \" protein\", \" kinase\", \" C\", \" in\", \" regulation\", \" of\", \" prostagland\", \"in\", \" E\", \"2\", \" release\", \" by\", \" cultured\", \" rabbit\", \" gastric\", \" epithelial\", \" cells\", \".\", \"\\\\newline\", \"\\n\", \"M\", \"ening\", \"i\", \"oma\", \":\", \" an\", \" update\", \".\", \"\\\\newline\", \"Recent\", \" clinical\", \" and\", \" molecular\", \" research\", \" has\", \" shed\", \" new\", \" light\", \" on\", \"\\n\", \"Header\", \"$\", \"type\", \"=\", \"menu\", \"\\\\newline\", \"\\\\newline\", \"World\", \" Con\", \"quer\", \"or\", \" 3\", \" Ap\", \"k\", \" Android\", \" Game\", \"\\\\newline\\\\newline\", \"\\n\", \"Ex\", \"posure\", \" to\", \" toxic\", \" metals\", \" and\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1366182565689087]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23655521869659424]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2959631681442261]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.012767195701599121]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[7.37905502319336e-05]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f44180dffd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 101\n",
    "best_feature = int(max_indices[N])\n",
    "# best_feature = 2122 #Mandarin\n",
    "# best_feature = 2118 # 's\n",
    "best_feature = 2079 # (within parentheses)\n",
    "# 2808, 3434, 83\n",
    "# best_feature = 2917 # open paran before acronym\n",
    "# best_feature = 2808 # open paran before acronym\n",
    "best_feature = 2808 # (within parentheses) 966, 928, 290\n",
    "best_feature = 2 # (within parentheses)\n",
    "# best_feature = 3482 # 3482, 4072, 3920, 310\n",
    "print(\"bias:\", smaller_auto_encoder.encoder_bias.detach().cpu().numpy()[best_feature])\n",
    "print(f\"Feature index: {best_feature}\")\n",
    "print(f\"MCS: {max_cosine_similarities[best_feature]}\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"max\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"uniform\")\n",
    "text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, pca_dictionary_activations, dataset, setting=\"uniform\", pca=True)\n",
    "visualize_text(text_list, best_feature, model)\n",
    "# visualize_text(full_text, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b875b9b6-44ed\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b875b9b6-44ed\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"P\", \"EN\", \"R\", \"ITH\", \" young\", \" gun\", \" Nathan\", \" Cle\", \"ary\", \" has\", \" claimed\", \" an\", \" impressive\", \" record\", \" in\", \" his\", \" side\", \"\\u2019\", \"s\", \" thrilling\", \" 26\", \"-\", \"22\", \" win\", \" on\", \" Sunday\", \" which\", \" ended\", \" Can\", \"ber\", \"ra\", \"\\u2019\", \"\\n\", \"I\", \" once\", \" thought\", \" the\", \" hardest\", \" thing\", \" about\", \" adulthood\", \" was\", \" putting\", \" a\", \" Weber\", \" grill\", \" cover\", \" back\", \" in\", \" place\", \".\", \" Now\", \"?\", \" It\", \"\\u2019\", \"s\", \" believing\", \" anything\", \" that\", \" comes\", \" out\", \" of\", \" Roger\", \" Good\", \"ell\", \"\\u2019\", \"\\n\", \"United\", \" Kingdom\", \"\\\\newline\", \"\\\\newline\", \"The\", \" look\", \" on\", \" M\", \"im\", \"\\u2019\", \"\\n\", \"Post\", \" navigation\", \"\\\\newline\", \"\\\\newline\", \"We\", \" have\", \" our\", \" new\", \" work\", \" for\", \" October\", \"\\u2019\", \"\\n\", \"\\u2018\", \"Rock\", \"\\u2019\", \"\\n\", \"As\", \" E\", \"rick\", \" noted\", \" earlier\", \" today\", \",\", \" my\", \" fellow\", \" Texans\", \" have\", \" a\", \" clear\", \",\", \" distinct\", \" choice\", \" as\", \" to\", \" who\", \" will\", \" be\", \" the\", \" Speaker\", \" of\", \" the\", \" Texas\", \" House\", \" coming\", \" up\", \" this\", \" Tuesday\", \",\", \" January\", \" 11\", \"th\", \".\", \" Let\", \"\\u2019\", \"\\n\", \"J\", \"org\", \"inho\", \" \\u2018\", \"amaz\", \"ed\", \"\\u2019\", \"\\n\", \"Comparison\", \" of\", \" patient\", \" satisfaction\", \" with\", \" acrylic\", \" and\", \" flexible\", \" partial\", \" dent\", \"ures\", \".\", \"\\\\newline\", \"Rest\", \"oration\", \" of\", \" partial\", \" ed\", \"ent\", \"ulous\", \" mouth\", \" may\", \" be\", \" done\", \" using\", \" a\", \" variety\", \"\\n\", \"For\", \" years\", \",\", \" the\", \" appendix\", \" got\", \" no\", \" respect\", \".\", \" Doctors\", \" regarded\", \" it\", \" as\", \" nothing\", \" but\", \" a\", \" source\", \" of\", \" trouble\", \":\", \" It\", \"\\n\"], \"activations\": [[[0.013630390167236328]], [[-0.020949840545654297]], [[-0.008414745330810547]], [[0.019329071044921875]], [[-0.12304973602294922]], [[-0.07601165771484375]], [[0.011755943298339844]], [[0.004045009613037109]], [[0.030683040618896484]], [[-0.02677774429321289]], [[-0.1383533477783203]], [[-0.23031949996948242]], [[0.024302959442138672]], [[0.16982507705688477]], [[0.031982421875]], [[-0.0011982917785644531]], [[-0.06906557083129883]], [[0.03179168701171875]], [[-0.5699844360351562]], [[0.08679342269897461]], [[0.22259855270385742]], [[0.06470251083374023]], [[0.1816387176513672]], [[0.1887807846069336]], [[0.06788969039916992]], [[0.1321558952331543]], [[-0.2748398780822754]], [[-0.10024023056030273]], [[-0.2833828926086426]], [[-0.651710033416748]], [[-1.1175479888916016]], [[-5.841888427734375]], [[0.0]], [[-0.06110191345214844]], [[-0.029175281524658203]], [[0.028553485870361328]], [[0.01333761215209961]], [[-0.0083770751953125]], [[0.02585744857788086]], [[0.0015816688537597656]], [[-0.04128837585449219]], [[0.07381963729858398]], [[-0.028555870056152344]], [[-0.025011062622070312]], [[0.024437427520751953]], [[-0.013721942901611328]], [[0.010381698608398438]], [[0.03963899612426758]], [[-0.007973194122314453]], [[0.015723705291748047]], [[-0.11654233932495117]], [[0.08490180969238281]], [[-0.025972843170166016]], [[-0.13962602615356445]], [[-0.06849431991577148]], [[-0.4109635353088379]], [[-0.03978681564331055]], [[-0.042446136474609375]], [[-0.01532745361328125]], [[-0.05047464370727539]], [[0.017693519592285156]], [[-0.017291545867919922]], [[-0.23410558700561523]], [[-0.2392873764038086]], [[0.26953125]], [[-4.914370059967041]], [[0.0]], [[-0.02739858627319336]], [[-0.14252138137817383]], [[-0.11377477645874023]], [[-0.11377477645874023]], [[-0.14767932891845703]], [[-0.042829036712646484]], [[0.059001922607421875]], [[-0.817385196685791]], [[-2.8131847381591797]], [[-4.688683986663818]], [[0.0]], [[-0.2829122543334961]], [[0.042052268981933594]], [[-0.28778982162475586]], [[-0.28778982162475586]], [[0.0766153335571289]], [[-0.1266012191772461]], [[-0.08404254913330078]], [[-0.005287647247314453]], [[0.09879493713378906]], [[-0.02500152587890625]], [[-1.2192792892456055]], [[-3.9529242515563965]], [[0.0]], [[-0.1777973175048828]], [[0.3180851936340332]], [[-3.2034168243408203]], [[0.0]], [[-0.04575824737548828]], [[-0.008415699005126953]], [[-0.04010820388793945]], [[-0.0607447624206543]], [[-0.046245574951171875]], [[0.012456893920898438]], [[-0.0076885223388671875]], [[0.02489757537841797]], [[0.055715084075927734]], [[0.0062885284423828125]], [[0.022917747497558594]], [[-0.012547016143798828]], [[0.003154277801513672]], [[0.0013041496276855469]], [[-0.03794097900390625]], [[0.027355670928955078]], [[-0.01719522476196289]], [[-0.01129293441772461]], [[-0.10218572616577148]], [[-0.05058431625366211]], [[-0.03253650665283203]], [[-0.04012346267700195]], [[0.060460567474365234]], [[-0.03604269027709961]], [[-0.01840686798095703]], [[-0.02260446548461914]], [[-0.05076932907104492]], [[0.07884740829467773]], [[0.041855812072753906]], [[-0.027930259704589844]], [[-0.09996795654296875]], [[-0.024780750274658203]], [[-0.17166996002197266]], [[-0.016521930694580078]], [[-0.0695953369140625]], [[0.46756410598754883]], [[-0.2980513572692871]], [[-2.967416763305664]], [[0.0]], [[0.0026497840881347656]], [[-0.08621692657470703]], [[0.17962265014648438]], [[1.6632682085037231]], [[-0.0011556148529052734]], [[0.10985565185546875]], [[-1.2330125570297241]], [[0.0]], [[-0.06120133399963379]], [[-0.0538175106048584]], [[0.003624558448791504]], [[-0.027390360832214355]], [[-0.01834547519683838]], [[0.005610942840576172]], [[-0.0183335542678833]], [[-0.020594358444213867]], [[-0.05122017860412598]], [[-0.05740153789520264]], [[-0.040261268615722656]], [[-0.06000328063964844]], [[-0.06472396850585938]], [[-0.037137746810913086]], [[-0.05402028560638428]], [[0.010470747947692871]], [[0.018662333488464355]], [[-0.023459672927856445]], [[-0.010068655014038086]], [[-0.012566328048706055]], [[-0.09527313709259033]], [[-0.05391573905944824]], [[-0.09527313709259033]], [[0.07200336456298828]], [[-0.09527313709259033]], [[-0.09527313709259033]], [[-0.09527313709259033]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f49548b3730>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_text(text_list, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-7a7764f0-a4c5\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-7a7764f0-a4c5\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"EN\", \"R\", \"ITH\", \" young\", \" gun\", \" Nathan\", \" Cle\", \"ary\", \" has\", \" claimed\", \" an\", \" impressive\", \" record\", \" in\", \" his\", \" side\", \"\\u2019\", \"s\", \" thrilling\", \" 26\", \"-\", \"22\", \" win\", \" on\", \" Sunday\", \" which\", \" ended\", \" Can\", \"ber\", \"ra\", \"\\u2019\", \"s\", \" 2017\", \" finals\", \" hopes\", \".\", \"\\\\newline\", \"\\\\newline\", \"He\", \"\\n\", \" once\", \" thought\", \" the\", \" hardest\", \" thing\", \" about\", \" adulthood\", \" was\", \" putting\", \" a\", \" Weber\", \" grill\", \" cover\", \" back\", \" in\", \" place\", \".\", \" Now\", \"?\", \" It\", \"\\u2019\", \"s\", \" believing\", \" anything\", \" that\", \" comes\", \" out\", \" of\", \" Roger\", \" Good\", \"ell\", \"\\u2019\", \"s\", \" offices\", \".\", \"\\\\newline\", \"\\\\newline\", \"Last\", \" week\", \"\\n\", \" Kingdom\", \"\\\\newline\", \"\\\\newline\", \"The\", \" look\", \" on\", \" M\", \"im\", \"\\u2019\", \"s\", \" face\", \" said\", \" it\", \" all\", \";\", \" \\u201c\", \"That\", \" is\", \" one\", \" of\", \" the\", \" best\", \" things\", \" I\", \"\\u2019\", \"ve\", \" ever\", \" tasted\", \"\\u201d,\", \" she\", \" said\", \" gle\", \"eful\", \"ly\", \",\", \" pushing\", \" the\", \" glor\", \"iously\", \"\\n\", \" navigation\", \"\\\\newline\", \"\\\\newline\", \"We\", \" have\", \" our\", \" new\", \" work\", \" for\", \" October\", \"\\u2019\", \"s\", \" workshop\", \"!\", \" David\", \" G\", \"au\", \"k\", \"ro\", \"ger\", \" is\", \" busy\", \" producing\", \" the\", \" music\", \" for\", \" his\", \" new\", \" children\", \"\\u2019\", \"s\", \" opera\", \" which\", \" we\", \" expect\", \" to\", \" become\", \" as\", \" popular\", \"\\n\", \"Rock\", \"\\u2019\", \"n\", \"\\u2019\", \"Roll\", \" Bangkok\", \"\\u2019\", \" to\", \" be\", \" witnessed\", \" at\", \" The\", \" Over\", \"stay\", \" in\", \" Pink\", \"la\", \"o\", \".\", \" Fe\", \"aturing\", \" five\", \" bands\", \" of\", \" original\", \" and\", \" authentic\", \" R\", \"\\u2019\", \"n\", \"\\u2019\", \"R\", \" music\", \" in\", \" a\", \" most\", \" extravag\", \"ant\", \" and\", \"\\n\", \" E\", \"rick\", \" noted\", \" earlier\", \" today\", \",\", \" my\", \" fellow\", \" Texans\", \" have\", \" a\", \" clear\", \",\", \" distinct\", \" choice\", \" as\", \" to\", \" who\", \" will\", \" be\", \" the\", \" Speaker\", \" of\", \" the\", \" Texas\", \" House\", \" coming\", \" up\", \" this\", \" Tuesday\", \",\", \" January\", \" 11\", \"th\", \".\", \" Let\", \"\\u2019\", \"s\", \" be\", \"\\n\", \"Av\", \"atar\", \"\\u2019\", \" The\", \"ft\", \" Plaintiff\", \" L\", \"oses\", \" B\", \"id\", \" To\", \" Dis\", \"qual\", \"ify\", \" Judge\", \"\\\\newline\", \"\\\\newline\", \"That\", \"\\u2019\", \"s\", \" the\", \" second\", \" Av\", \"atar\", \"legal\", \" loss\", \" in\", \" a\", \" row\", \" for\", \" Eric\", \" Ry\", \"der\", \" and\", \" another\", \" win\", \" for\", \" James\", \" Cameron\", \"\\n\", \"org\", \"inho\", \" \\u2018\", \"amaz\", \"ed\", \"\\u2019\", \" by\", \" Chelsea\", \" change\", \"\\\\newline\", \"\\\\newline\", \"By\", \" Football\", \" It\", \"alia\", \" staff\", \"\\\\newline\", \"\\\\newline\", \"J\", \"org\", \"inho\", \" says\", \" he\", \" is\", \" \\u2018\", \"very\", \" happy\", \"\\u2019\", \" and\", \" \\u2018\", \"amaz\", \"ed\", \"\\u2019\", \" by\", \" how\", \" he\", \" has\", \" won\", \" the\", \"\\n\", \"I\", \"\\ufffd\", \"v\", \"\\ufffd\", \"V\", \"\\ufffd\\ufffd\", \"\\ufffd\\ufffd\", \"\\ufffd\", \"\\ufffd\", \"\\ufffd\", \"v\", \"\\ufffd\\ufffd\", \"\\ufffd\", \"[\", \"\\ufffd\\ufffd\", \"\\ufffd\", \"\\u0442\", \"\\ufffd\", \"\\ufffd\", \"\\u0082\", \"\\ufffd\\ufffd\", \"\\ufffd\", \"\\\\newline\", \"\\\\newline\", \"H\", \"-\", \"R\", \"AND\", \"OM\", \" \\ufffd\", \"\\ufffd\\ufffd\", \"\\ufffd\", \" S\", \"-\", \"R\", \"AND\", \"OM\", \" \\ufffd\", \"\\ufffd\\ufffd\", \"\\n\", \" of\", \" patient\", \" satisfaction\", \" with\", \" acrylic\", \" and\", \" flexible\", \" partial\", \" dent\", \"ures\", \".\", \"\\\\newline\", \"Rest\", \"oration\", \" of\", \" partial\", \" ed\", \"ent\", \"ulous\", \" mouth\", \" may\", \" be\", \" done\", \" using\", \" a\", \" variety\", \" of\", \" treatment\", \" options\", \".\", \" Rem\", \"ov\", \"able\", \" partial\", \" dent\", \"ure\", \" (\", \"R\", \"PD\", \"\\n\", \" years\", \",\", \" the\", \" appendix\", \" got\", \" no\", \" respect\", \".\", \" Doctors\", \" regarded\", \" it\", \" as\", \" nothing\", \" but\", \" a\", \" source\", \" of\", \" trouble\", \":\", \" It\", \" didn\", \"\\u2019\", \"t\", \" seem\", \" to\", \" do\", \" anything\", \",\", \" and\", \" it\", \" sometimes\", \" got\", \" infected\", \" and\", \" required\", \" an\", \" emergency\", \" removal\", \".\", \"\\n\"], \"activations\": [[[0.0]], [[0.0007915496826171875]], [[0.0013670921325683594]], [[0.0037078857421875]], [[-0.0011472702026367188]], [[-0.0036115646362304688]], [[-0.005890846252441406]], [[0.0]], [[0.0]], [[0.0]], [[-0.00035381317138671875]], [[0.0]], [[0.0012965202331542969]], [[-0.0001380443572998047]], [[-0.003721475601196289]], [[-0.004614830017089844]], [[-0.004778861999511719]], [[-7.568543910980225]], [[6.29425048828125e-05]], [[0.0020265579223632812]], [[-7.450580596923828e-07]], [[4.76837158203125e-07]], [[1.430511474609375e-06]], [[-0.012242317199707031]], [[-1.239776611328125e-05]], [[-0.006550312042236328]], [[-0.0008625984191894531]], [[9.5367431640625e-07]], [[0.0]], [[0.0]], [[0.00017781555652618408]], [[-6.378512382507324]], [[0.07674551010131836]], [[0.001617431640625]], [[-0.003345012664794922]], [[-7.712841033935547e-05]], [[-0.0019584298133850098]], [[-8.259806782007217e-05]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[9.059906005859375e-06]], [[0.00098419189453125]], [[0.0004150867462158203]], [[8.130073547363281e-05]], [[0.0033206939697265625]], [[0.0]], [[-0.0003857612609863281]], [[-0.00014328956604003906]], [[-0.0008707046508789062]], [[-0.0024137496948242188]], [[0.0010213851928710938]], [[-2.3126602172851562e-05]], [[0.0]], [[0.00667572021484375]], [[1.8596649169921875e-05]], [[0.0]], [[0.0008840560913085938]], [[-0.0007467269897460938]], [[0.00023126602172851562]], [[-0.0528276264667511]], [[0.0025005340576171875]], [[0.004040241241455078]], [[-0.0004665851593017578]], [[-0.0066258907318115234]], [[0.0]], [[0.0]], [[0.0029621124267578125]], [[0.0036268234252929688]], [[0.0]], [[8.475780487060547e-05]], [[-4.744143962860107]], [[0.0]], [[0.0004974603652954102]], [[-3.2186508178710938e-06]], [[-0.00010045990347862244]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0002276897430419922]], [[0.0034275054931640625]], [[0.0]], [[0.0]], [[-5.199733734130859]], [[0.0002837181091308594]], [[0.0002722740173339844]], [[-2.5272369384765625e-05]], [[-2.574920654296875e-05]], [[1.430511474609375e-06]], [[-3.4332275390625e-05]], [[2.09808349609375e-05]], [[0.048296451568603516]], [[0.0003209114074707031]], [[1.3113021850585938e-05]], [[3.230571746826172e-05]], [[-0.001008749008178711]], [[-0.001886129379272461]], [[0.00029158592224121094]], [[-8.344650268554688e-07]], [[0.0030503049492836]], [[-5.960464477539062e-07]], [[-3.814697265625e-06]], [[5.7220458984375e-06]], [[1.430511474609375e-06]], [[-5.960464477539063e-08]], [[-1.1444091796875e-05]], [[0.0]], [[0.0]], [[1.1920928955078125e-07]], [[9.5367431640625e-07]], [[-0.002355217933654785]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.0012164115905761719]], [[0.0021800994873046875]], [[0.0016527175903320312]], [[-0.0011801719665527344]], [[0.0029931068420410156]], [[-0.0005292892456054688]], [[-4.156707286834717]], [[-0.009465217590332031]], [[0.00022268295288085938]], [[0.0026216506958007812]], [[0.0010814666748046875]], [[0.0]], [[-3.337860107421875e-06]], [[-0.0009350776672363281]], [[-1.9073486328125e-06]], [[0.0001938343048095703]], [[0.0030345916748046875]], [[0.0008087158203125]], [[-0.00039768218994140625]], [[0.00018739700317382812]], [[-7.724761962890625e-05]], [[-0.004129886627197266]], [[0.002284526824951172]], [[0.005485057830810547]], [[0.00023937225341796875]], [[-6.059873580932617]], [[9.5367431640625e-06]], [[-4.100799560546875e-05]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-1.9073486328125e-06]], [[0.0]], [[0.0]], [[0.19571304321289062]], [[-0.22302961349487305]], [[-2.2959747314453125]], [[-0.016608238220214844]], [[-0.11861753463745117]], [[0.05010795593261719]], [[0.013041466474533081]], [[-0.3311147689819336]], [[-0.035329341888427734]], [[0.006707191467285156]], [[0.004484415054321289]], [[-0.0006747245788574219]], [[-1.049041748046875e-05]], [[0.023056983947753906]], [[0.003940582275390625]], [[0.000885009765625]], [[0.008037567138671875]], [[-0.019887924194335938]], [[0.009279251098632812]], [[-0.0006284713745117188]], [[5.066394805908203e-07]], [[-0.0010619163513183594]], [[0.0029349327087402344]], [[0.0005209445953369141]], [[-0.0019712448120117188]], [[-0.0015420913696289062]], [[-0.0031800270080566406]], [[0.0]], [[0.0]], [[-0.5450115203857422]], [[-0.15315711498260498]], [[0.04567432403564453]], [[0.008726119995117188]], [[-4.76837158203125e-06]], [[-4.76837158203125e-07]], [[-9.5367431640625e-07]], [[-9.5367431640625e-06]], [[-4.3213367462158203e-07]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0011744499206542969]], [[0.000530242919921875]], [[0.0006914138793945312]], [[0.0]], [[-0.000652313232421875]], [[-2.574920654296875e-05]], [[-0.0021610260009765625]], [[0.0003123283386230469]], [[-4.506111145019531e-05]], [[0.00024175643920898438]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.004899024963378906]], [[2.9802322387695312e-05]], [[0.00019598007202148438]], [[0.0]], [[0.0003718137741088867]], [[-0.0041828155517578125]], [[0.00023061037063598633]], [[-0.00034737586975097656]], [[0.0]], [[0.0]], [[-0.0009517669677734375]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0017347335815429688]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[6.890296936035156e-05]], [[-0.45073702931404114]], [[0.0006670951843261719]], [[0.0]], [[-0.5665435791015625]], [[0.045482635498046875]], [[0.043022871017456055]], [[1.0400314331054688]], [[0.02173614501953125]], [[0.013875007629394531]], [[0.025609970092773438]], [[-2.86102294921875e-06]], [[0.014953136444091797]], [[-0.00041961669921875]], [[0.0018086433410644531]], [[-3.528594970703125e-05]], [[1.9073486328125e-06]], [[0.00016939640045166016]], [[0.0005369186401367188]], [[-0.03860902786254883]], [[-0.009377151727676392]], [[-0.00019741058349609375]], [[0.0004241466522216797]], [[-1.4636512994766235]], [[-1.430511474609375e-06]], [[0.0]], [[0.0]], [[0.0]], [[1.621246337890625e-05]], [[9.5367431640625e-07]], [[-3.814697265625e-06]], [[0.0]], [[0.0]], [[-0.0005598068237304688]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.00021982192993164062]], [[0.0]], [[-0.024465560913085938]], [[0.003936290740966797]], [[-0.0014168024063110352]], [[0.255112886428833]], [[-0.01134490966796875]], [[0.054123878479003906]], [[0.037337541580200195]], [[-0.0011915862560272217]], [[0.0003833770751953125]], [[-0.00015592575073242188]], [[4.291534423828125e-06]], [[5.364418029785156e-07]], [[6.616115570068359e-06]], [[1.138448715209961e-05]], [[1.043081283569336e-07]], [[-5.4836273193359375e-05]], [[0.0]], [[0.0]], [[0.0]], [[3.337860107421875e-05]], [[3.0517578125e-05]], [[3.62396240234375e-05]], [[0.0]], [[0.0]], [[0.0]], [[-0.07443928718566895]], [[0.014748334884643555]], [[-0.00014209747314453125]], [[0.0]], [[-6.631016731262207e-07]], [[0.0009202957153320312]], [[-0.006733417510986328]], [[-7.933378219604492e-05]], [[3.1948089599609375e-05]], [[4.6253204345703125e-05]], [[0.0006917715072631836]], [[0.0]], [[-0.007230281829833984]], [[-0.005584716796875]], [[-0.17176294326782227]], [[-0.0045906007289886475]], [[0.05693769454956055]], [[0.0026521682739257812]], [[0.12249255180358887]], [[-0.01476830244064331]], [[0.03513336181640625]], [[0.011409759521484375]], [[0.03716421127319336]], [[-0.0013225078582763672]], [[-0.0020550787448883057]], [[-0.10121774673461914]], [[0.002416849136352539]], [[-0.004391223192214966]], [[0.014251708984375]], [[0.012911796569824219]], [[-0.06439447402954102]], [[-0.0003833770751953125]], [[-0.007957935333251953]], [[-0.003998428583145142]], [[-0.002185344696044922]], [[0.016894102096557617]], [[-0.007880210876464844]], [[0.00027632713317871094]], [[0.003719329833984375]], [[0.0003337860107421875]], [[0.0002760887145996094]], [[0.0007791519165039062]], [[4.6938657760620117e-07]], [[-0.00011932849884033203]], [[9.250640869140625e-05]], [[1.52587890625e-05]], [[-4.76837158203125e-07]], [[1.4722347259521484e-05]], [[2.3562461137771606e-07]], [[-2.0563602447509766e-06]], [[0.0]], [[0.0]], [[0.0]], [[-7.152557373046875e-06]], [[-8.106231689453125e-06]], [[4.76837158203125e-06]], [[5.7220458984375e-06]], [[-1.9073486328125e-06]], [[5.7220458984375e-06]], [[0.0]], [[0.0]], [[0.0]], [[-7.152557373046875e-07]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-2.09808349609375e-05]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.2172927856445312e-05]], [[-1.4781951904296875e-05]], [[-2.1457672119140625e-05]], [[3.1741801649332047e-05]], [[-4.76837158203125e-06]], [[-0.003854990005493164]], [[0.00013524293899536133]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.00016260147094726562]], [[-0.0001952648162841797]], [[0.0]], [[-0.0004892349243164062]], [[0.0002288818359375]], [[0.0]], [[0.0]], [[0.0]], [[-0.0004062652587890625]], [[8.511543273925781e-05]], [[0.0]], [[0.0012412071228027344]], [[0.0001266002655029297]], [[-0.0002033710479736328]], [[0.0008349418640136719]], [[-0.00016177445650100708]], [[-0.00022125244140625]], [[-5.340576171875e-05]], [[-0.0013532638549804688]], [[0.0001964569091796875]], [[0.0]], [[-1.0251709682052024e-05]], [[2.384185791015625e-06]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f4715a07940>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_feature_direction_display(full_text, best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14514500', '�', '�', 'disambiguation', '?)', '.),', 'formerly', \")',\", ' {¶', '?).', 'founded', 'himself', 'NFTA', '\"),', \"'),\", 'aka', 'marined', ',)', ')\",', ')_']\n",
      "tensor([2.9057, 2.8045, 2.6344, 2.4723, 2.2787, 2.2187, 2.0828, 1.9988, 1.9918,\n",
      "        1.9741, 1.9419, 1.9317, 1.9227, 1.8658, 1.8624, 1.8610, 1.8581, 1.8155,\n",
      "        1.7683, 1.7504])\n"
     ]
    }
   ],
   "source": [
    "logit_lens(model,best_feature, smaller_dict, layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
