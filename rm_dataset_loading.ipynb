{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5661cbf7f21f4154963bdc28c21985fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b513c8de4340c1a613dd97e99b2626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3e96211b71499999c67d6f85cf53a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9baa9b08784b76bdeb930e23c19c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import argparse\n",
    "from utils import dotdict\n",
    "from activation_dataset import setup_token_data\n",
    "import wandb\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from typing import Any, Dict, List, Optional, Tuple, TypeVar, Union\n",
    "import multiprocessing as mp\n",
    "from transformers import GPT2Tokenizer, PreTrainedTokenizerBase, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "cfg = dotdict()\n",
    "# models: \"EleutherAI/pythia-70m-deduped\", \"usvsnsp/pythia-6.9b-ppo\", \"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "# cfg.model_name=\"lomahony/eleuther-pythia6.9b-hh-sft\"\n",
    "cfg.model_name=\"EleutherAI/pythia-6.9b-deduped\"\n",
    "cfg.layers=[10]\n",
    "cfg.setting=\"residual\"\n",
    "cfg.tensor_name=\"gpt_neox.layers.{layer}\"\n",
    "original_l1_alpha = 8e-4\n",
    "cfg.l1_alpha=original_l1_alpha\n",
    "cfg.sparsity=None\n",
    "cfg.num_epochs=10\n",
    "cfg.model_batch_size=8\n",
    "cfg.max_length = 256\n",
    "cfg.lr=1e-3\n",
    "cfg.kl=False\n",
    "cfg.reconstruction=False\n",
    "# cfg.dataset_name=\"NeelNanda/pile-10k\"\n",
    "cfg.dataset_name=\"Dahoas/rm-static\"\n",
    "cfg.device=\"cuda:0\"\n",
    "cfg.ratio = 4\n",
    "cfg.seed = 0\n",
    "# cfg.device=\"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa69cde3217414897db4bc11a67ef83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/926 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d289b691fc49aa9ddbcc2f14b4e0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/530 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08398e690e1f4cb4bf7280dcad0ba78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc451490df62454aa0e6b34ae536d878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/68.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd4dc39afec481090fca2a1287e1f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b6b8f7b7dc41ca8a62816d849628ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850e147e7b554465aee4123e12bccbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/76256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00e45e4dfc545f9bacd6144d11e2d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255cca95845b483398b71369039c3fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ea384c3bc945deb3662477e0e38c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ea3ebfdf7a45ca88263edc39a7357b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/303M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730c5d39a4f34d9d99cc388312d2dfee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd33f6c21e8c4a1a95eb8c67c207bdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/Elriggs___parquet/Elriggs--openwebtext-100k-79076ecafee8a6d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset(cfg.dataset_name, split=\"train\")\n",
    "alt_dset = load_dataset(\"Elriggs/openwebtext-100k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prompt', 'response', 'chosen', 'rejected']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = dset['chosen'] + dset['rejected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_dataset(texts, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Convert a list of strings into a tokenized datasets.Dataset.\n",
    "\n",
    "    Args:\n",
    "    - texts (list of str): The list of strings to be tokenized.\n",
    "    - tokenizer: A tokenizer from the HuggingFace Transformers library.\n",
    "    - max_length (int): Maximum length to which the tokenized strings should be truncated/padded.\n",
    "\n",
    "    Returns:\n",
    "    - A datasets.Dataset with tokenized inputs and attention masks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a datasets.Dataset from the list of texts\n",
    "    dset = Dataset.from_dict({'text': texts})\n",
    "\n",
    "    # Define a function to tokenize the texts\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_length)\n",
    "\n",
    "    # Tokenize the texts\n",
    "    tokenized_dset = dset.map(tokenize_function, batched=True)\n",
    "\n",
    "    return tokenized_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06e7582d6c344e2a2f4d7500b2dd36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_tokenized_dataset(string_list, tokenizer, \u001b[39m256\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mcreate_tokenized_dataset\u001b[0;34m(texts, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m], truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m, max_length\u001b[39m=\u001b[39mmax_length)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Tokenize the texts\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m tokenized_dset \u001b[39m=\u001b[39m dset\u001b[39m.\u001b[39;49mmap(tokenize_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_dset\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    541\u001b[0m }\n\u001b[1;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3065\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3066\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3067\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3068\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3071\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3074\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3075\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3449\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3445\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3446\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3447\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3448\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3449\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3450\u001b[0m         batch,\n\u001b[1;32m   3451\u001b[0m         indices,\n\u001b[1;32m   3452\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3453\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3454\u001b[0m     )\n\u001b[1;32m   3455\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3456\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3458\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3330\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3328\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3329\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3330\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3332\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3333\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3334\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mcreate_tokenized_dataset.<locals>.tokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(examples):\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m], truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, max_length\u001b[39m=\u001b[39;49mmax_length)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2561\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2559\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2560\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2561\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2562\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2563\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2647\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2643\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2644\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2645\u001b[0m         )\n\u001b[1;32m   2646\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2648\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2649\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2650\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2651\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2652\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2653\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2654\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2655\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2656\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2657\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2658\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2659\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2660\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2661\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2662\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2663\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2664\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2665\u001b[0m     )\n\u001b[1;32m   2666\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2667\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2668\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2669\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2685\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2686\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2829\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2825\u001b[0m \u001b[39m        details in `encode_plus`).\u001b[39;00m\n\u001b[1;32m   2826\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m-> 2829\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[1;32m   2830\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2831\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2832\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2833\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2834\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2835\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2836\u001b[0m )\n\u001b[1;32m   2838\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[1;32m   2839\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2840\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2855\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2856\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2466\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2464\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[1;32m   2465\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m-> 2466\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2467\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2468\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2469\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2470\u001b[0m     )\n\u001b[1;32m   2472\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[1;32m   2473\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2474\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[1;32m   2475\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m   2479\u001b[0m ):\n",
      "\u001b[0;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "create_tokenized_dataset(string_list, tokenizer, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = alt_dset\n",
    "text_key = \"text\"\n",
    "max_length = cfg.max_length\n",
    "\n",
    "\n",
    "chunk_size = min(tokenizer.model_max_length, max_length)  # tokenizer max length is 1024 for gpt2\n",
    "sep = tokenizer.eos_token or \"<|endoftext|>\"\n",
    "joined_text = sep.join([\"\"] + x[text_key])\n",
    "output = tokenizer(\n",
    "    # Concatenate all the samples together, separated by the EOS token.\n",
    "    joined_text,  # start with an eos token\n",
    "    max_length=chunk_size,\n",
    "    return_attention_mask=False,\n",
    "    return_overflowing_tokens=True,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "if overflow := output.pop(\"overflowing_tokens\", None):\n",
    "    # Slow Tokenizers return unnested lists of ints\n",
    "    assert isinstance(output[\"input_ids\"][0], int)\n",
    "\n",
    "    # Chunk the overflow into batches of size `chunk_size`\n",
    "    chunks = [output[\"input_ids\"]] + [\n",
    "        overflow[i * chunk_size : (i + 1) * chunk_size] for i in range(math.ceil(len(overflow) / chunk_size))\n",
    "    ]\n",
    "    output = {\"input_ids\": chunks}\n",
    "\n",
    "total_tokens = sum(len(ids) for ids in output[\"input_ids\"])\n",
    "total_bytes = len(joined_text.encode(\"utf-8\"))\n",
    "\n",
    "if not return_final_batch:\n",
    "    # We know that the last sample will almost always be less than the max\n",
    "    # number of tokens, and we don't want to pad, so we just drop it.\n",
    "    output = {k: v[:-1] for k, v in output.items()}\n",
    "\n",
    "output_batch_size = len(output[\"input_ids\"])\n",
    "\n",
    "if output_batch_size == 0:\n",
    "    raise ValueError(\n",
    "        \"Not enough data to create a single batch complete batch.\"\n",
    "        \" Either allow the final batch to be returned,\"\n",
    "        \" or supply more data.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = tokenizer.eos_token or \"<|endoftext|>\"\n",
    "def concat_prompt_response(example):\n",
    "    example['text_chosen'] = example['prompt'] + example['chosen']\n",
    "    example['text_rejected'] = example['prompt'] + example['rejected']\n",
    "    return example\n",
    "\n",
    "dset = dset.map(concat_prompt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = min(tokenizer.model_max_length, cfg.max_length)\n",
    "output = tokenizer(\n",
    "            dset[\"text_chosen\"],  # start with an eos token\n",
    "            max_length=chunk_size,\n",
    "            return_attention_mask=False,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del alt_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_and_tokenize_rm(\n",
    "    data: Dataset,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    *,\n",
    "    format: str = \"torch\",\n",
    "    num_proc: int = min(mp.cpu_count() // 2, 8),\n",
    "    text_key: str = \"text_chosen\",\n",
    "    max_length: int = 2048,\n",
    "    return_final_batch: bool = False,\n",
    "    load_from_cache_file: bool = True,\n",
    "):\n",
    "    \"\"\"Perform GPT-style chunking and tokenization on a dataset.\n",
    "\n",
    "    The resulting dataset will consist entirely of chunks exactly `max_length` tokens\n",
    "    long. Long sequences will be split into multiple chunks, and short sequences will\n",
    "    be merged with their neighbors, using `eos_token` as a separator. The fist token\n",
    "    will also always be an `eos_token`.\n",
    "\n",
    "    Args:\n",
    "        data: The dataset to chunk and tokenize.\n",
    "        tokenizer: The tokenizer to use.\n",
    "        format: The format to return the dataset in, passed to `Dataset.with_format`.\n",
    "        num_proc: The number of processes to use for tokenization.\n",
    "        text_key: The key in the dataset to use as the text to tokenize.\n",
    "        max_length: The maximum length of a batch of input ids.\n",
    "        return_final_batch: Whether to return the final batch, which may be smaller\n",
    "            than the others.\n",
    "        load_from_cache_file: Whether to load from the cache file.\n",
    "\n",
    "    Returns:\n",
    "        * The chunked and tokenized dataset.\n",
    "        * The ratio of nats to bits per byte see https://arxiv.org/pdf/2101.00027.pdf,\n",
    "            section 3.1.\n",
    "    \"\"\"\n",
    "\n",
    "    def _tokenize_fn(x: Dict[str, list]):\n",
    "        chunk_size = min(tokenizer.model_max_length, max_length)  # tokenizer max length is 1024 for gpt2\n",
    "        print\n",
    "        sep = tokenizer.eos_token or \"<|endoftext|>\"\n",
    "        joined_text = sep.join([\"\"] + x[text_key])\n",
    "        output = tokenizer(\n",
    "            # Concatenate all the samples together, separated by the EOS token.\n",
    "            joined_text,  # start with an eos token\n",
    "            max_length=chunk_size,\n",
    "            return_attention_mask=False,\n",
    "            return_overflowing_tokens=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        if overflow := output.pop(\"overflowing_tokens\", None):\n",
    "            # Slow Tokenizers return unnested lists of ints\n",
    "            assert isinstance(output[\"input_ids\"][0], int)\n",
    "\n",
    "            # Chunk the overflow into batches of size `chunk_size`\n",
    "            chunks = [output[\"input_ids\"]] + [\n",
    "                overflow[i * chunk_size : (i + 1) * chunk_size] for i in range(math.ceil(len(overflow) / chunk_size))\n",
    "            ]\n",
    "            output = {\"input_ids\": chunks}\n",
    "\n",
    "\n",
    "        if not return_final_batch:\n",
    "            # We know that the last sample will almost always be less than the max\n",
    "            # number of tokens, and we don't want to pad, so we just drop it.\n",
    "            output = {k: v[:-1] for k, v in output.items()}\n",
    "\n",
    "        output_batch_size = len(output[\"input_ids\"])\n",
    "\n",
    "        if output_batch_size == 0:\n",
    "            raise ValueError(\n",
    "                \"Not enough data to create a single batch complete batch.\"\n",
    "                \" Either allow the final batch to be returned,\"\n",
    "                \" or supply more data.\"\n",
    "            )\n",
    "\n",
    "        return output\n",
    "\n",
    "    data = data.map(\n",
    "        _tokenize_fn,\n",
    "        # Batching is important for ensuring that we don't waste tokens\n",
    "        # since we always throw away the last element of the batch we\n",
    "        # want to keep the batch size as large as possible\n",
    "        batched=True,\n",
    "        batch_size=2048,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=data.column_names,\n",
    "        load_from_cache_file=load_from_cache_file,\n",
    "    )\n",
    "    return data.with_format(format, columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked = chunk_and_tokenize_rm(dset, tokenizer, max_length=cfg.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# TODO iteratively grab dataset?\n",
    "cfg.max_length = 256\n",
    "token_loader = setup_token_data(cfg, tokenizer, model, seed=cfg.seed)\n",
    "num_tokens = cfg.max_length*cfg.model_batch_size*len(token_loader)\n",
    "print(f\"Number of tokens: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1 datapoint on model to get the activation size\n",
    "from baukit import Trace\n",
    "\n",
    "text = \"1\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# Your activation name will be different. In the next cells, we will show you how to find it.\n",
    "with torch.no_grad():\n",
    "    with Trace(model, tensor_names[0]) as ret:\n",
    "        _ = model(tokens)\n",
    "        representation = ret.output\n",
    "        # check if instance tuple\n",
    "        if(isinstance(representation, tuple)):\n",
    "            representation = representation[0]\n",
    "        activation_size = representation.shape[-1]\n",
    "print(f\"Activation size: {activation_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize New autoencoder\n",
    "from autoencoders.learned_dict import TiedSAE, UntiedSAE, AnthropicSAE\n",
    "from torch import nn\n",
    "params = dict()\n",
    "n_dict_components = activation_size*cfg.ratio\n",
    "params[\"encoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"encoder\"])\n",
    "\n",
    "params[\"decoder\"] = torch.empty((n_dict_components, activation_size), device=cfg.device)\n",
    "nn.init.xavier_uniform_(params[\"decoder\"])\n",
    "\n",
    "params[\"encoder_bias\"] = torch.empty((n_dict_components,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"encoder_bias\"])\n",
    "\n",
    "params[\"shift_bias\"] = torch.empty((activation_size,), device=cfg.device)\n",
    "nn.init.zeros_(params[\"shift_bias\"])\n",
    "\n",
    "autoencoder = AnthropicSAE(  # TiedSAE, UntiedSAE, AnthropicSAE\n",
    "    # n_feats = n_dict_components, \n",
    "    # activation_size=activation_size,\n",
    "    encoder=params[\"encoder\"],\n",
    "    encoder_bias=params[\"encoder_bias\"],\n",
    "    decoder=params[\"decoder\"],\n",
    "    shift_bias=params[\"shift_bias\"],\n",
    ")\n",
    "autoencoder.to_device(cfg.device)\n",
    "autoencoder.set_grad()\n",
    "# autoencoder.encoder.requires_grad = True\n",
    "# autoencoder.encoder_bias.requires_grad = True\n",
    "# autoencoder.decoder.requires_grad = True\n",
    "# autoencoder.shift_bias.requires_grad = True\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        autoencoder.encoder, \n",
    "        autoencoder.encoder_bias,\n",
    "        autoencoder.decoder,\n",
    "        autoencoder.shift_bias,\n",
    "    ], lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target sparsity to 10% of activation_size if not set\n",
    "if cfg.sparsity is None:\n",
    "    cfg.sparsity = int(activation_size*0.05)\n",
    "    print(f\"Target sparsity: {cfg.sparsity}\")\n",
    "\n",
    "target_lower_sparsity = cfg.sparsity * 0.9\n",
    "target_upper_sparsity = cfg.sparsity * 1.1\n",
    "adjustment_factor = 0.1  # You can set this to whatever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_bias = autoencoder.encoder_bias.clone().detach()\n",
    "# Wandb setup\n",
    "secrets = json.load(open(\"secrets.json\"))\n",
    "wandb.login(key=secrets[\"wandb_key\"])\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "wandb_run_name = f\"{cfg.model_name}_{start_time[4:]}_{cfg.sparsity}\"  # trim year\n",
    "print(f\"wandb_run_name: {wandb_run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"sparse coding\", config=dict(cfg), name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_activation = torch.zeros(autoencoder.encoder.shape[0])\n",
    "total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "max_num_tokens = 30_000_000\n",
    "save_every = 5_000_000\n",
    "num_saved_so_far = 0\n",
    "# Freeze model parameters \n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "model.to(cfg.device)\n",
    "last_encoder = autoencoder.encoder.clone().detach()\n",
    "for i, batch in enumerate(tqdm(token_loader)):\n",
    "    tokens = batch[\"input_ids\"].to(cfg.device)\n",
    "    with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "        with Trace(model, tensor_names[0]) as ret:\n",
    "            _ = model(tokens)\n",
    "            representation = ret.output\n",
    "            if(isinstance(representation, tuple)):\n",
    "                representation = representation[0]\n",
    "    layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    # activation_saver.save_batch(layer_activations.clone().cpu().detach())\n",
    "\n",
    "    c = autoencoder.encode(layer_activations)\n",
    "    x_hat = autoencoder.decode(c)\n",
    "    \n",
    "    reconstruction_loss = (x_hat - layer_activations).pow(2).mean()\n",
    "    l1_loss = torch.norm(c, 1, dim=-1).mean()\n",
    "    total_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "\n",
    "    time_since_activation += 1\n",
    "    time_since_activation = time_since_activation * (c.sum(dim=0).cpu()==0)\n",
    "    total_activations += c.sum(dim=0).cpu()\n",
    "    if ((i+1) % 10 == 0): # Check here so first check is model w/o change\n",
    "        # self_similarity = torch.cosine_similarity(c, last_encoder, dim=-1).mean().cpu().item()\n",
    "        # Above is wrong, should be similarity between encoder and last encoder\n",
    "        self_similarity = torch.cosine_similarity(autoencoder.encoder, last_encoder, dim=-1).mean().cpu().item()\n",
    "        last_encoder = autoencoder.encoder.clone().detach()\n",
    "        num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "        with torch.no_grad():\n",
    "            sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            # Count number of dead_features are zero\n",
    "            num_dead_features = (time_since_activation >= min(i, 200)).sum().item()\n",
    "        print(f\"Sparsity: {sparsity:.1f} | Dead Features: {num_dead_features} | Total Loss: {total_loss:.2f} | Reconstruction Loss: {reconstruction_loss:.2f} | L1 Loss: {cfg.l1_alpha*l1_loss:.2f} | l1_alpha: {cfg.l1_alpha:.2e} | Tokens: {num_tokens_so_far} | Self Similarity: {self_similarity:.2f}\")\n",
    "        wandb.log({\n",
    "            'Sparsity': sparsity,\n",
    "            'Dead Features': num_dead_features,\n",
    "            'Total Loss': total_loss.item(),\n",
    "            'Reconstruction Loss': reconstruction_loss.item(),\n",
    "            'L1 Loss': (cfg.l1_alpha*l1_loss).item(),\n",
    "            'l1_alpha': cfg.l1_alpha,\n",
    "            'Tokens': num_tokens_so_far,\n",
    "            'Self Similarity': self_similarity\n",
    "        })\n",
    "        \n",
    "        dead_features = torch.zeros(autoencoder.encoder.shape[0])\n",
    "        \n",
    "        if(num_tokens_so_far > max_num_tokens):\n",
    "            print(f\"Reached max number of tokens: {max_num_tokens}\")\n",
    "            break\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # resample_period = 10000\n",
    "    # if (i % resample_period == 0):\n",
    "    #     # RESAMPLING\n",
    "    #     with torch.no_grad():\n",
    "    #         # Count number of dead_features are zero\n",
    "    #         num_dead_features = (total_activations == 0).sum().item()\n",
    "    #         print(f\"Dead Features: {num_dead_features}\")\n",
    "            \n",
    "    #     if num_dead_features > 0:\n",
    "    #         print(\"Resampling!\")\n",
    "    #         # hyperparams:\n",
    "    #         max_resample_tokens = 1000 # the number of token activations that we consider for inserting into the dictionary\n",
    "    #         # compute loss of model on random subset of inputs\n",
    "    #         resample_loader = setup_token_data(cfg, tokenizer, model, seed=i)\n",
    "    #         num_resample_data = 0\n",
    "\n",
    "    #         resample_activations = torch.empty(0, activation_size)\n",
    "    #         resample_losses = torch.empty(0)\n",
    "\n",
    "    #         for resample_batch in resample_loader:\n",
    "    #             resample_tokens = resample_batch[\"input_ids\"].to(cfg.device)\n",
    "    #             with torch.no_grad(): # As long as not doing KL divergence, don't need gradients for model\n",
    "    #                 with Trace(model, tensor_names[0]) as ret:\n",
    "    #                     _ = model(resample_tokens)\n",
    "    #                     representation = ret.output\n",
    "    #                     if(isinstance(representation, tuple)):\n",
    "    #                         representation = representation[0]\n",
    "    #             layer_activations = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "    #             resample_activations = torch.cat((resample_activations, layer_activations.detach().cpu()), dim=0)\n",
    "\n",
    "    #             c = autoencoder.encode(layer_activations)\n",
    "    #             x_hat = autoencoder.decode(c)\n",
    "                \n",
    "    #             reconstruction_loss = (x_hat - layer_activations).pow(2).mean(dim=-1)\n",
    "    #             l1_loss = torch.norm(c, 1, dim=-1)\n",
    "    #             temp_loss = reconstruction_loss + cfg.l1_alpha*l1_loss\n",
    "                \n",
    "    #             resample_losses = torch.cat((resample_losses, temp_loss.detach().cpu()), dim=0)\n",
    "                \n",
    "    #             num_resample_data +=layer_activations.shape[0]\n",
    "    #             if num_resample_data > max_resample_tokens:\n",
    "    #                 break\n",
    "\n",
    "                \n",
    "    #         # sample num_dead_features vectors of input activations\n",
    "    #         probabilities = resample_losses**2\n",
    "    #         probabilities /= probabilities.sum()\n",
    "    #         sampled_indices = torch.multinomial(probabilities, num_dead_features, replacement=True)\n",
    "    #         new_vectors = resample_activations[sampled_indices]\n",
    "\n",
    "    #         # calculate average encoder norm of alive neurons\n",
    "    #         alive_neurons = list((total_activations!=0))\n",
    "    #         modified_columns = total_activations==0\n",
    "    #         avg_norm = autoencoder.encoder.data[alive_neurons].norm(dim=-1).mean()\n",
    "\n",
    "    #         # replace dictionary and encoder weights with vectors\n",
    "    #         new_vectors = new_vectors / new_vectors.norm(dim=1, keepdim=True)\n",
    "            \n",
    "    #         params_to_modify = [autoencoder.encoder, autoencoder.encoder_bias]\n",
    "\n",
    "    #         current_weights = autoencoder.encoder.data\n",
    "    #         current_weights[modified_columns] = (new_vectors.to(cfg.device) * avg_norm * 0.02)\n",
    "    #         autoencoder.encoder.data = current_weights\n",
    "\n",
    "    #         current_weights = autoencoder.encoder_bias.data\n",
    "    #         current_weights[modified_columns] = 0\n",
    "    #         autoencoder.encoder_bias.data = current_weights\n",
    "            \n",
    "    #         if hasattr(autoencoder, 'decoder'):\n",
    "    #             current_weights = autoencoder.decoder.data\n",
    "    #             current_weights[modified_columns] = new_vectors.to(cfg.device)\n",
    "    #             autoencoder.decoder.data = current_weights\n",
    "    #             params_to_modify += [autoencoder.decoder]\n",
    "\n",
    "    #         for param_group in optimizer.param_groups:\n",
    "    #             for param in param_group['params']:\n",
    "    #                 if any(param is d_ for d_ in params_to_modify):\n",
    "    #                     # Extract the corresponding rows from m and v\n",
    "    #                     m = optimizer.state[param]['exp_avg']\n",
    "    #                     v = optimizer.state[param]['exp_avg_sq']\n",
    "                        \n",
    "    #                     # Update the m and v values for the modified columns\n",
    "    #                     m[modified_columns] = 0  # Reset moving average for modified columns\n",
    "    #                     v[modified_columns] = 0  # Reset squared moving average for modified columns\n",
    "        \n",
    "    #     total_activations = torch.zeros(autoencoder.encoder.shape[0])\n",
    "\n",
    "    if ((i+2) % 10_000==0): # save periodically but before big changes\n",
    "        model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "        save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}_ckpt{num_saved_so_far}\"  # trim year\n",
    "\n",
    "        # Make directory traiend_models if it doesn't exist\n",
    "        import os\n",
    "        if not os.path.exists(\"trained_models\"):\n",
    "            os.makedirs(\"trained_models\")\n",
    "        # Save model\n",
    "        torch.save(autoencoder, f\"trained_models/{save_name}.pt\")\n",
    "        \n",
    "        num_saved_so_far += 1\n",
    "\n",
    "    # Running sparsity check\n",
    "    num_tokens_so_far = i*cfg.max_length*cfg.model_batch_size\n",
    "    if(num_tokens_so_far > 200000):\n",
    "        if(i % 100 == 0):\n",
    "            with torch.no_grad():\n",
    "                sparsity = (c != 0).float().mean(dim=0).sum().cpu().item()\n",
    "            if sparsity > target_upper_sparsity:\n",
    "                cfg.l1_alpha *= (1 + adjustment_factor)\n",
    "            elif sparsity < target_lower_sparsity:\n",
    "                cfg.l1_alpha *= (1 - adjustment_factor)\n",
    "            # print(f\"Sparsity: {sparsity:.1f} | l1_alpha: {cfg.l1_alpha:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = cfg.model_name.split(\"/\")[-1]\n",
    "save_name = f\"{model_save_name}_sp{cfg.sparsity}_r{cfg.ratio}_{tensor_names[0]}\"  # trim year\n",
    "\n",
    "# Make directory traiend_models if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"trained_models\"):\n",
    "    os.makedirs(\"trained_models\")\n",
    "# Save model\n",
    "torch.save(autoencoder, f\"trained_models/{save_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
