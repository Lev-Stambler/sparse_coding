{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mchorse/miniconda3/envs/logan/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import numpy as np \n",
    "from torch import nn\n",
    "import pickle\n",
    "\n",
    "# # Define the autoencoder so pickle knows how to serialize it. \n",
    "# # Later, we should actually save as a state_dict instead of a dumb pickle\n",
    "# class AutoEncoder(nn.Module):\n",
    "#     def __init__(self, activation_size, n_dict_components, t_type=torch.float32, l1_coef=0.0):\n",
    "#         super(AutoEncoder, self).__init__()\n",
    "        \n",
    "#         # Only defining the decoder layer, encoder will share its weights\n",
    "#         self.decoder = nn.Linear(n_dict_components, activation_size, bias=True)\n",
    "#         # Create a bias layer\n",
    "#         self.encoder_bias= nn.Parameter(torch.zeros(n_dict_components))\n",
    "\n",
    "        \n",
    "#         # Initialize the decoder weights orthogonally\n",
    "#         nn.init.orthogonal_(self.decoder.weight)\n",
    "#         self.decoder = self.decoder.to(t_type)\n",
    "\n",
    "#         # Encoder is a Sequential with the ReLU activation\n",
    "#         # No need to define a Linear layer for the encoder as its weights are tied with the decoder\n",
    "#         self.encoder = nn.Sequential(nn.ReLU()).to(t_type)\n",
    "\n",
    "#         self.l1_coef = l1_coef\n",
    "#         self.activation_size = activation_size\n",
    "#         self.n_dict_components = n_dict_components\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         c = self.encoder(x @ self.decoder.weight + self.encoder_bias)\n",
    "#         # Apply unit norm constraint to the decoder weights\n",
    "#         self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "\n",
    "#         # Decoding step as before\n",
    "#         x_hat = self.decoder(c)\n",
    "#         return x_hat, c\n",
    "\n",
    "\n",
    "#     @property\n",
    "#     def device(self):\n",
    "#         return next(self.parameters()).device\n",
    "all_autoencoders = torch.load(\"/home/mchorse/sparse_coding_aidan_new/output_4_rd_deep/_7/learned_dicts.pt\")\n",
    "num_dictionaries = len(all_autoencoders)\n",
    "autoencoder, hyperparams = all_autoencoders[7]\n",
    "l1_alpha = hyperparams['l1_alpha']\n",
    "\n",
    "autoencoder_larger, hyperparams_larger = all_autoencoders[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Change these settings to load the correct autoencoder\n",
    "layer = 2\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "# model_name = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "# filename = \"1bautoencoder.pkl\"\n",
    "# filename = \"pythia70m_layer2_residual_tied_ (1).pkl\"\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "if setting == \"residual\":\n",
    "    cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    neurons = model.cfg.d_model\n",
    "elif setting == \"mlp\":\n",
    "    cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "    neurons = model.cfg.d_mlp\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# # Load the pickle file\n",
    "# with open(filename, 'rb') as file:\n",
    "#     autoencoders = pickle.load(file)\n",
    "\n",
    "# Index for l1 value, usually only 1 value is available\n",
    "# l1_index = 0\n",
    "# dictionaries = [autoencoder.decoder.weight.data.T for autoencoder in autoencoders[l1_index]]\n",
    "# for d in dictionaries:\n",
    "#     print(d.shape)\n",
    "# print(\"len of autoencoders: \", len(autoencoders))\n",
    "# dict_index = 2\n",
    "# smaller_dict, larger_dict = dictionaries[dict_index], dictionaries[dict_index+1]\n",
    "smaller_dict = autoencoder.get_learned_dict()\n",
    "larger_dict = autoencoder_larger.get_learned_dict()\n",
    "# smaller_auto_encoder, larger_auto_encoder = autoencoders[l1_index][dict_index], autoencoders[l1_index][dict_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_dict.shape, larger_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_dict.shape, l1_alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCS\n",
    "Max cosine similarity between one dictionary & another one. If they learned the same feature, then they'll have high cosine similarity. \n",
    "\n",
    "If two dictionaries learned it, it's probably a real feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# of features above 0.9:', 0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "#Dictionary Comparison\n",
    "smaller_dict_features, _ = smaller_dict.shape\n",
    "larger_dict_features, _ = larger_dict.shape\n",
    "larger_dict = larger_dict.to(device)\n",
    "# Hungary algorithm\n",
    "# Calculate all cosine similarities and store in a 2D array\n",
    "cos_sims = np.zeros((smaller_dict_features, larger_dict_features))\n",
    "for idx, vector in enumerate(smaller_dict):\n",
    "    cos_sims[idx] = torch.nn.functional.cosine_similarity(vector.to(device), larger_dict, dim=1).cpu().numpy()\n",
    "# Convert to a minimization problem\n",
    "cos_sims = 1 - cos_sims\n",
    "# Use the Hungarian algorithm to solve the assignment problem\n",
    "row_ind, col_ind = linear_sum_assignment(cos_sims)\n",
    "# Retrieve the max cosine similarities and corresponding indices\n",
    "max_cosine_similarities = 1 - cos_sims[row_ind, col_ind]\n",
    "\n",
    "# Get the indices of the max cosine similarities in descending order\n",
    "max_indices = np.argsort(max_cosine_similarities)[::-1]\n",
    "max_cosine_similarities[max_indices][:20]\n",
    "print((\"# of features above 0.9:\", (max_cosine_similarities > .9).sum()))\n",
    "# Plot histogram of max_cosine_similarities\n",
    "plt.hist(max_cosine_similarities, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model activations & Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ed7f2edc0483960e.arrow\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-965cfb5b1daceda6.arrow\n",
      "Loading cached processed dataset at /home/mchorse/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-b01c7d09751dca4c.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount= 60\n",
    "dataset = load_dataset(dataset_name, split=\"train\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 605/605 [00:13<00:00, 45.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "# neurons = model.W_in.shape[-1]\n",
    "neurons = model.cfg.d_model\n",
    "datapoints = dataset.num_rows\n",
    "batch_size = 16\n",
    "neuron_activations = torch.zeros((datapoints*token_amount, neurons))\n",
    "dictionary_activations = torch.zeros((datapoints*token_amount, smaller_dict_features))\n",
    "smaller_auto_encoder = autoencoder\n",
    "smaller_auto_encoder.to_device(device)\n",
    "\n",
    "with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "    dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "    for i, batch in enumerate(tqdm(dl)):\n",
    "        _, cache = model.run_with_cache(batch.to(device))\n",
    "        batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "        neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "        batched_dictionary_activations = smaller_auto_encoder.encode(batched_neuron_activations)\n",
    "        dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = neuron_activations.max(dim=0).values\n",
    "print(v.topk(10, largest=True))\n",
    "print(\"Median: \", v.median().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = neuron_activations.min(dim=0).values\n",
    "print(v.topk(10, largest=False))\n",
    "print(\"Median: \", v.median().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_activations.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of non-zero activations in dictionary_activations\n",
    "(dictionary_activations[:100000] > 0).sum(dim=0).topk(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dictionary_activations[:100000] > 0.2).sum(dim=0).topk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dictionary_activations > 0.0).sum(dim=0).topk(10).values / dictionary_activations.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_activations.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Activation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints(feature_index, dictionary_activations, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    datapoint_indices =[np.unravel_index(i, (datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(model.tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = model.tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list\n",
    "\n",
    "def get_neuron_activation(token, feature, model, setting=\"dictionary_basis\"):\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "        neuron_act_batch = cache[cache_name]\n",
    "        if setting==\"dictionary_basis\":\n",
    "            neuron_act_batch = rearrange(neuron_act_batch, \"b s n -> (b s) n\" )\n",
    "            act = smaller_auto_encoder.encode(neuron_act_batch)\n",
    "            return act[:, feature].tolist()\n",
    "        else: # neuron/residual basis\n",
    "            return neuron_act_batch[0, :, feature].tolist()\n",
    "\n",
    "def ablate_text(text, feature, model, setting=\"plot\"):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    display_text_list = []\n",
    "    activation_list = []\n",
    "    for t in text:\n",
    "        # Convert text into tokens\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t equals tokens\n",
    "            tokens = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        seq_size = tokens.shape[1]\n",
    "        if(seq_size == 1): # If the text is a single token, we can't ablate it\n",
    "            continue\n",
    "        original = get_neuron_activation(tokens, feature, model)[-1]\n",
    "        changed_activations = torch.zeros(seq_size, device=device).cpu()\n",
    "        for i in range(seq_size):\n",
    "            # Remove the i'th token from the input\n",
    "            ablated_tokens = torch.cat((tokens[:,:i], tokens[:,i+1:]), dim=1)\n",
    "            changed_activations[i] += get_neuron_activation(ablated_tokens, feature, model)[-1]\n",
    "        changed_activations -= original\n",
    "        display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        activation_list += changed_activations.tolist() + [0.0]\n",
    "    activation_list = torch.tensor(activation_list).reshape(-1,1,1)\n",
    "    if setting == \"plot\":\n",
    "        return text_neuron_activations(tokens=display_text_list, activations=activation_list)\n",
    "    else:\n",
    "        return display_text_list, activation_list\n",
    "def visualize_text(text, feature, model, setting=\"dictionary_basis\", max_activation = None):\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    if isinstance(feature, int):\n",
    "        feature = [feature]\n",
    "    display_text_list = []\n",
    "    act_list = []\n",
    "    for t in text:\n",
    "        if isinstance(t, str): # If the text is a list of tokens\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            token = model.to_tokens(t, prepend_bos=False)\n",
    "        else: # t are tokens\n",
    "            token = t\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "        for f in feature:\n",
    "            display_text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            act_list += get_neuron_activation(token, f, model, setting) + [0.0]\n",
    "    act_list = torch.tensor(act_list).reshape(-1,1,1)\n",
    "    if(max_activation is not None):\n",
    "        act_list = torch.clamp(act_list, max=max_activation)\n",
    "    return text_neuron_activations(tokens=display_text_list, activations=act_list)\n",
    "# Ablate the feature direction of the tokens\n",
    "# token_list is a list of tokens, convert to tensor of shape (batch_size, seq_len)\n",
    "from einops import rearrange\n",
    "def ablate_feature_direction(tokens, feature, model, autoencoder):\n",
    "    def mlp_ablation_hook(value, hook):\n",
    "        # Rearrange to fit autoencoder\n",
    "        int_val = rearrange(value, 'b s h -> (b s) h')\n",
    "\n",
    "        # Run through the autoencoder\n",
    "        _, act = autoencoder(int_val)\n",
    "        feature_to_ablate = feature # TODO: bring this out of the function\n",
    "\n",
    "        # Subtract value with feature direction*act_of_feature\n",
    "        feature_direction = torch.outer(act[:, feature_to_ablate].squeeze(), autoencoder.decoder.weight[:, feature_to_ablate].squeeze())\n",
    "        batch, seq_len, hidden_size = value.shape\n",
    "        feature_direction = rearrange(feature_direction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "        value -= feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name, \n",
    "            mlp_ablation_hook\n",
    "            )]\n",
    "        )\n",
    "def add_feature_direction(tokens, feature, model, autoencoder, scalar=1.0):\n",
    "    def residual_add_hook(value, hook):\n",
    "        feature_direction = autoencoder.decoder.weight[:, feature].squeeze()\n",
    "        value += scalar*feature_direction\n",
    "        return value\n",
    "\n",
    "    return model.run_with_hooks(tokens, \n",
    "        fwd_hooks=[(\n",
    "            cache_name,\n",
    "            residual_add_hook\n",
    "            )]\n",
    "        )\n",
    "def ablate_feature_direction_display(text, features=None, setting=\"true_tokens\", verbose=False):\n",
    "\n",
    "    if features==None:\n",
    "        features = torch.tensor([best_feature])\n",
    "    if isinstance(features, int):\n",
    "        features = torch.tensor([features])\n",
    "    if isinstance(features, list):\n",
    "        features = torch.tensor(features)\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    text_list = []\n",
    "    logit_list = []\n",
    "    for t in text:\n",
    "        tokens = model.to_tokens(t, prepend_bos=False)\n",
    "        with torch.no_grad():\n",
    "            original_logits = model(tokens).log_softmax(-1).cpu()\n",
    "            ablated_logits = ablate_feature_direction(tokens, features, model, smaller_auto_encoder).log_softmax(-1).cpu()\n",
    "        diff_logits = ablated_logits  - original_logits# ablated > original -> negative diff\n",
    "        tokens = tokens.cpu()\n",
    "        if setting == \"true_tokens\":\n",
    "            split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "            gather_tokens = rearrange(tokens[:,1:], \"b s -> b s 1\") # TODO: verify this is correct\n",
    "            # Gather the logits for the true tokens\n",
    "            diff = rearrange(diff_logits[:, :-1].gather(-1,gather_tokens), \"b s n -> (b s n)\")\n",
    "        elif setting == \"max\":\n",
    "            # Negate the diff_logits to see which tokens have the largest effect on the neuron\n",
    "            val, ind = (-1*diff_logits).max(-1)\n",
    "            diff = rearrange(val[:, :-1], \"b s -> (b s)\")\n",
    "            diff*= -1 # Negate the values gathered\n",
    "            split_text = model.to_str_tokens(ind, prepend_bos=False)\n",
    "            gather_tokens = rearrange(ind[:,1:], \"1 s -> 1 s 1\")\n",
    "        split_text = split_text[1:] # Remove the first token since we're not predicting it\n",
    "        if(verbose):\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "            orig = rearrange(original_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            ablated = rearrange(ablated_logits[:, :-1].gather(-1, gather_tokens), \"b s n -> (b s n)\")\n",
    "            logit_list += orig.tolist() + [0.0]\n",
    "            logit_list += ablated.tolist() + [0.0]\n",
    "        text_list += [x.replace('\\n', '\\\\newline') for x in split_text] + [\"\\n\"]\n",
    "        logit_list += diff.tolist() + [0.0]\n",
    "    logit_list = torch.tensor(logit_list).reshape(-1,1,1)\n",
    "    if verbose:\n",
    "        print(f\"Max & Min logit-diff: {logit_list.max().item():.2f} & {logit_list.min().item():.2f}\")\n",
    "    return text_neuron_activations(tokens=text_list, activations=logit_list)\n",
    "def generate_text(input_text, num_tokens, model, autoencoder, feature, temperature=0.7, setting=\"add\", scalar=1.0):\n",
    "    # Convert input text to tokens\n",
    "    input_ids = model.tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    for _ in range(num_tokens):\n",
    "        # Generate logits\n",
    "        with torch.no_grad():\n",
    "            if(setting==\"add\"):\n",
    "                logits = add_feature_direction(input_ids, feature, model, autoencoder, scalar=scalar)\n",
    "            else:\n",
    "                logits = model(input_ids)\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        probs = torch.nn.functional.softmax(logits[:, -1, :], dim=-1)\n",
    "        predicted_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append predicted token to input_ids\n",
    "        input_ids = torch.cat((input_ids, predicted_token), dim=-1)\n",
    "\n",
    "    # Decode the tokens to text\n",
    "    output_text = model.tokenizer.decode(input_ids[0])\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Logit Lens\n",
    "def logit_lens(model, best_feature, smaller_dict, layer):\n",
    "    with torch.no_grad():\n",
    "        # There are never-used tokens, which have high norm. We want to ignore these.\n",
    "        bad_ind = (model.W_U.norm(dim=0) > 20)\n",
    "        feature_direction = smaller_dict[best_feature].to(device)\n",
    "        # feature_direction = torch.matmul(feature_direction, model.W_out[layer]) # if MLP\n",
    "        logits = torch.matmul(feature_direction, model.W_U).cpu()\n",
    "    # Don't include bad indices\n",
    "    logits[bad_ind] = -1000\n",
    "    topk_values, topk_indices = torch.topk(logits, 20)\n",
    "    top_text = model.to_str_tokens(topk_indices)\n",
    "    print(f\"{top_text}\")\n",
    "    print(topk_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "You can use the functions below to find interesting features to then add here to \"feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \" for\"\n",
    "temp = 0.7\n",
    "tokens_to_generate = 20\n",
    "feature = 10 \n",
    "scalar = 100.0\n",
    "# Using the function:\n",
    "print(\"Normal:\\n\" + generate_text(sentence, tokens_to_generate, model, smaller_auto_encoder, feature=feature, temperature=temp, scalar=scalar, setting=\"normal\"))\n",
    "print(\"Add:\\n\" + generate_text(sentence, tokens_to_generate, model, smaller_auto_encoder, feature=feature, temperature=temp, scalar=scalar, setting=\"add\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Search\n",
    "Type in a sentence & see which features activate \n",
    "Note: Some features may be outliers, which will typically show up as high activations for the first word & first period or \\n (or high positive bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \" I do like a\"\n",
    "split_text = model.to_str_tokens(t, prepend_bos=False)\n",
    "token = model.to_tokens(t, prepend_bos=False)\n",
    "_, cache = model.run_with_cache(token.to(model.cfg.device))\n",
    "neuron_act_batch = cache[cache_name]\n",
    "_, act = smaller_auto_encoder(neuron_act_batch)\n",
    "v, i = act[0, -1, :].topk(10)\n",
    "\n",
    "print(\"Activations:\",[round(val,2) for val in v.tolist()])\n",
    "print(\"Feature_ids\", i.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz_act = dictionary_activations.cpu().count_nonzero(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.3660)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_activations.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-12.6967)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_activations.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 8],\n",
       "        [10],\n",
       "        [12],\n",
       "        [14],\n",
       "        [16]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz_act.nonzero()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot as hist\n",
    "plt.hist(nz_act.numpy(), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAHHCAYAAACV96NPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8xElEQVR4nO3deXhN597/8U8SkUQmY6YiaM3UWCR6Gq0c0apTpw7lRClKTxtDTC391Vw1lRpqqD6KtlqdB5RSYxGkVB81JKiStpLUlBQVkdy/PzzZxxYq0ZBb+n5d17pY97rXWt+1srP3J2vaLsYYIwAAAIu4FnYBAAAAVyKgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAARUSlSpX0xBNPFHYZN9X69evl4uKi9evXF3YpAG4yAgpguUOHDumpp55SlSpV5OnpKT8/PzVv3lzTp0/X77//Xtjl3db27t2rUaNG6ccff8xT/2PHjmno0KG6//775evre92wtGXLFt17770qUaKEgoKC1K9fP505cyZXv4ULF8rFxSXPdQB/BcUKuwAA17Z8+XJ16NBBHh4e6tq1q+rUqaMLFy5o06ZNGjJkiPbs2aN58+YVdpm3rb1792r06NFq0aKFKlWqdN3+CQkJmjhxoqpWraq6desqLi7umn137dqlli1bqmbNmpo6dap++uknvfzyyzpw4IBWrFhRgFsBFE0EFMBShw8fVqdOnRQaGqq1a9cqODjYMS0mJkYHDx7U8uXLC7HCv55GjRrpxIkTKl26tD788EN16NDhmn2ff/55lSpVSuvXr5efn5+kS6fhevXqpVWrVqlVq1a3qmzgtsQpHsBSkyZN0pkzZzR//nyncJLjrrvuUv/+/a85/8mTJzV48GDVrVtXPj4+8vPz04MPPqjvvvvOqV/OdR3vv/++Ro8erTvuuEO+vr7617/+pbS0NGVkZCg2NlYBAQHy8fFR9+7dlZGR4bQMFxcX9enTR4sXL1b16tXl6empRo0aaePGjbnq+vnnn9WjRw8FBgbKw8NDtWvX1htvvJGr308//aR27drJ29tbAQEBGjBgQK71XsuRI0f0zDPPqHr16vLy8lKZMmXUoUMHp1MoCxcudASM+++/Xy4uLtc9ZePr66vSpUtfd/3p6elavXq1unTp4ggnktS1a1f5+Pjo/fffz9N2AH9lHEEBLLV06VJVqVJF4eHhNzT/Dz/8oE8//VQdOnRQ5cqVlZKSotdee00RERHau3evQkJCnPqPHz9eXl5eGjp0qA4ePKiZM2fK3d1drq6uOnXqlEaNGqWtW7dq4cKFqly5skaMGOE0/4YNG/Tee++pX79+8vDw0OzZs9W6dWtt375dderUkSSlpKSoWbNmjkBTrlw5rVixQj179lR6erpiY2MlSb///rtatmypo0ePql+/fgoJCdFbb72ltWvX5mnb4+PjtWXLFnXq1Enly5fXjz/+qDlz5qhFixbau3evSpQoofvuu0/9+vXTjBkz9Pzzz6tmzZqS5Pj3z9i9e7cuXryoxo0bO7UXL15c9evX17fffvun1wEUeQaAddLS0owk88gjj+R5ntDQUNOtWzfH+Pnz501WVpZTn8OHDxsPDw8zZswYR9u6deuMJFOnTh1z4cIFR3vnzp2Ni4uLefDBB52WERYWZkJDQ53aJBlJ5ptvvnG0HTlyxHh6epp//vOfjraePXua4OBgc/z4caf5O3XqZPz9/c25c+eMMcZMmzbNSDLvv/++o8/Zs2fNXXfdZSSZdevW/eG+yFnO5eLi4owk8+abbzraPvjggzwt72r+aN6caRs3bsw1rUOHDiYoKMipbcGCBUaSOXz4cL7rAIoqTvEAFkpPT5d06ZTCjfLw8JCr66Vf8aysLJ04cUI+Pj6qXr26du7cmat/165d5e7u7hhv2rSpjDHq0aOHU7+mTZsqKSlJFy9edGoPCwtTo0aNHOMVK1bUI488oi+//FJZWVkyxuijjz5S27ZtZYzR8ePHHUNUVJTS0tIcdX3xxRcKDg7Wv/71L8fySpQood69e+dp2728vBz/z8zM1IkTJ3TXXXepZMmSV932gpZzd5WHh0euaZ6entx9BeQBp3gAC+Vct/Dbb7/d8DKys7M1ffp0zZ49W4cPH1ZWVpZjWpkyZXL1r1ixotO4v7+/JKlChQq52rOzs5WWlua0nKpVq+ZaZrVq1XTu3Dn9+uuvcnV11enTpzVv3rxr3nmUmpoq6dI1JHfddZdcXFycplevXv2PNtnh999/1/jx47VgwQL9/PPPMsY4pqWlpeVpGX9GTkC62jUz58+fdwpQAK6OgAJYyM/PTyEhIfr+++9veBkvvfSShg8frh49emjs2LEqXbq0XF1dFRsbq+zs7Fz93dzcrrqca7Vf/qGfFznr7NKli7p163bVPnfffXe+lnktffv21YIFCxQbG6uwsDD5+/vLxcVFnTp1uuq2F7Sci5qPHTuWa9qxY8dyXf8DIDcCCmCphx9+WPPmzVNcXJzCwsLyPf+HH36o+++/X/Pnz3dqP336tMqWLVtQZTocOHAgV1tiYqJKlCihcuXKSbp0yiorK0uRkZF/uKzQ0FB9//33MsY4HUVJSEjIUy0ffvihunXrpilTpjjazp8/r9OnTzv1u/IITUGpU6eOihUrpm+++UYdO3Z0tF+4cEG7du1yapOkJ554osg/BRjIL65BASz17LPPytvbW08++aRSUlJyTT906JCmT59+zfnd3NxyHeX44IMP9PPPPxd4rZIUFxfndH1HUlKSPvvsM7Vq1Upubm5yc3NT+/bt9dFHH131yNCvv/7q+P9DDz2kX375RR9++KGj7dy5c3l+KN3Vtn3mzJlOp7kkydvbW5JyBZc/y9/fX5GRkXr77bedTtO99dZbOnPmTK7npxw/flz79+9XZmZmgdYB3M44ggJY6s4779Q777yjxx57TDVr1nR6kuyWLVv0wQcf/OFf3Q8//LDGjBmj7t27Kzw8XLt379bixYtVpUqVm1JvnTp1FBUV5XSbsSSNHj3a0WfChAlat26dmjZtql69eqlWrVo6efKkdu7cqa+++konT56UJPXq1Uuvvvqqunbtqh07dig4OFhvvfWWSpQokadaHn74Yb311lvy9/dXrVq1FBcXp6+++irXtTf169eXm5ubJk6cqLS0NHl4eOiBBx5QQEDANZf94osvSpL27Nkj6VLo2LRpkyTphRdecPQbN26cwsPDFRERod69e+unn37SlClT1KpVK7Vu3dppma+++qpGjx6tw4cP5+mJtsBfQuHdQAQgLxITE02vXr1MpUqVTPHixY2vr69p3ry5mTlzpjl//ryj39VuMx40aJAJDg42Xl5epnnz5iYuLs5ERESYiIgIR7+c24w/+OADp/Xm3PoaHx/v1D5y5Egjyfz666+ONkkmJibGvP3226Zq1arGw8PDNGjQ4Kq34KakpJiYmBhToUIF4+7uboKCgkzLli3NvHnznPodOXLE/OMf/zAlSpQwZcuWNf379zcrV67M023Bp06dMt27dzdly5Y1Pj4+Jioqyuzfvz/XPjLGmNdff91UqVLFuLm55WnZ+r9bqq82XOnrr7824eHhxtPT05QrV87ExMSY9PT0XP1y9im3GQP/5WJMPq90A4AruLi4KCYmRq+++mphlwKgiOAaFAAAYB0CCgAAsA4BBQAAWIe7eAD8aVzKBqCgcQQFAABYh4ACAACsc1ue4snOztYvv/wiX1/fm/aoagAAULCMMfrtt98UEhLi+Lb1a7ktA8ovv/yS6xtWAQDA7SEpKUnly5f/wz63ZUDx9fWVdGkDc76WHgAA2C09PV0VKlRwfI7/kdsyoOSc1vHz8yOgAABwm8nL5RlcJAsAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwTrHCLsBGlYYuz3PfHye0uYmVAADw18QRFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOvgJKVlaWhg8frsqVK8vLy0t33nmnxo4dK2OMo48xRiNGjFBwcLC8vLwUGRmpAwcOOC3n5MmTio6Olp+fn0qWLKmePXvqzJkzBbNFAADgtpevgDJx4kTNmTNHr776qvbt26eJEydq0qRJmjlzpqPPpEmTNGPGDM2dO1fbtm2Tt7e3oqKidP78eUef6Oho7dmzR6tXr9ayZcu0ceNG9e7du+C2CgAA3NZczOWHP67j4YcfVmBgoObPn+9oa9++vby8vPT222/LGKOQkBANGjRIgwcPliSlpaUpMDBQCxcuVKdOnbRv3z7VqlVL8fHxaty4sSRp5cqVeuihh/TTTz8pJCTkunWkp6fL399faWlp8vPzy+82X1elocvz3PfHCW0KfP0AABRF+fn8ztcRlPDwcK1Zs0aJiYmSpO+++06bNm3Sgw8+KEk6fPiwkpOTFRkZ6ZjH399fTZs2VVxcnCQpLi5OJUuWdIQTSYqMjJSrq6u2bdt21fVmZGQoPT3daQAAAEVXsfx0Hjp0qNLT01WjRg25ubkpKytL48aNU3R0tCQpOTlZkhQYGOg0X2BgoGNacnKyAgICnIsoVkylS5d29LnS+PHjNXr06PyUCgAAbmP5OoLy/vvva/HixXrnnXe0c+dOLVq0SC+//LIWLVp0s+qTJA0bNkxpaWmOISkp6aauDwAAFK58HUEZMmSIhg4dqk6dOkmS6tatqyNHjmj8+PHq1q2bgoKCJEkpKSkKDg52zJeSkqL69etLkoKCgpSamuq03IsXL+rkyZOO+a/k4eEhDw+P/JQKAABuY/k6gnLu3Dm5ujrP4ubmpuzsbElS5cqVFRQUpDVr1jimp6ena9u2bQoLC5MkhYWF6fTp09qxY4ejz9q1a5Wdna2mTZve8IYAAICiI19HUNq2batx48apYsWKql27tr799ltNnTpVPXr0kCS5uLgoNjZWL774oqpWrarKlStr+PDhCgkJUbt27SRJNWvWVOvWrdWrVy/NnTtXmZmZ6tOnjzp16pSnO3gAAEDRl6+AMnPmTA0fPlzPPPOMUlNTFRISoqeeekojRoxw9Hn22Wd19uxZ9e7dW6dPn9a9996rlStXytPT09Fn8eLF6tOnj1q2bClXV1e1b99eM2bMKLitAgAAt7V8PQfFFjwHBQCA289New4KAADArUBAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKyT74Dy888/q0uXLipTpoy8vLxUt25dffPNN47pxhiNGDFCwcHB8vLyUmRkpA4cOOC0jJMnTyo6Olp+fn4qWbKkevbsqTNnzvz5rQEAAEVCvgLKqVOn1Lx5c7m7u2vFihXau3evpkyZolKlSjn6TJo0STNmzNDcuXO1bds2eXt7KyoqSufPn3f0iY6O1p49e7R69WotW7ZMGzduVO/evQtuqwAAwG3NxRhj8tp56NCh2rx5s77++uurTjfGKCQkRIMGDdLgwYMlSWlpaQoMDNTChQvVqVMn7du3T7Vq1VJ8fLwaN24sSVq5cqUeeugh/fTTTwoJCbluHenp6fL391daWpr8/PzyWn6eVRq6PM99f5zQpsDXDwBAUZSfz+98HUH5/PPP1bhxY3Xo0EEBAQFq0KCBXn/9dcf0w4cPKzk5WZGRkY42f39/NW3aVHFxcZKkuLg4lSxZ0hFOJCkyMlKurq7atm3bVdebkZGh9PR0pwEAABRd+QooP/zwg+bMmaOqVavqyy+/1NNPP61+/fpp0aJFkqTk5GRJUmBgoNN8gYGBjmnJyckKCAhwml6sWDGVLl3a0edK48ePl7+/v2OoUKFCfsoGAAC3mXwFlOzsbDVs2FAvvfSSGjRooN69e6tXr16aO3fuzapPkjRs2DClpaU5hqSkpJu6PgAAULjyFVCCg4NVq1Ytp7aaNWvq6NGjkqSgoCBJUkpKilOflJQUx7SgoCClpqY6Tb948aJOnjzp6HMlDw8P+fn5OQ0AAKDoyldAad68uRISEpzaEhMTFRoaKkmqXLmygoKCtGbNGsf09PR0bdu2TWFhYZKksLAwnT59Wjt27HD0Wbt2rbKzs9W0adMb3hAAAFB0FMtP5wEDBig8PFwvvfSSOnbsqO3bt2vevHmaN2+eJMnFxUWxsbF68cUXVbVqVVWuXFnDhw9XSEiI2rVrJ+nSEZfWrVs7Tg1lZmaqT58+6tSpU57u4AEAAEVfvgLKPffco08++UTDhg3TmDFjVLlyZU2bNk3R0dGOPs8++6zOnj2r3r176/Tp07r33nu1cuVKeXp6OvosXrxYffr0UcuWLeXq6qr27dtrxowZBbdVAADgtpav56DYguegAABw+7lpz0EBAAC4FQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOscIuAAAA3HyVhi7PV/8fJ7S5SZXkDUdQAACAdQgoAADAOgQUAABgHQIKAACwzp8KKBMmTJCLi4tiY2MdbefPn1dMTIzKlCkjHx8ftW/fXikpKU7zHT16VG3atFGJEiUUEBCgIUOG6OLFi3+mFAAAUITccECJj4/Xa6+9prvvvtupfcCAAVq6dKk++OADbdiwQb/88oseffRRx/SsrCy1adNGFy5c0JYtW7Ro0SItXLhQI0aMuPGtAAAARcoNBZQzZ84oOjpar7/+ukqVKuVoT0tL0/z58zV16lQ98MADatSokRYsWKAtW7Zo69atkqRVq1Zp7969evvtt1W/fn09+OCDGjt2rGbNmqULFy4UzFYBAIDb2g0FlJiYGLVp00aRkZFO7Tt27FBmZqZTe40aNVSxYkXFxcVJkuLi4lS3bl0FBgY6+kRFRSk9PV179uy5kXIAAEARk+8HtS1ZskQ7d+5UfHx8rmnJyckqXry4SpYs6dQeGBio5ORkR5/Lw0nO9JxpV5ORkaGMjAzHeHp6en7LBgAAt5F8HUFJSkpS//79tXjxYnl6et6smnIZP368/P39HUOFChVu2boBAMCtl6+AsmPHDqWmpqphw4YqVqyYihUrpg0bNmjGjBkqVqyYAgMDdeHCBZ0+fdppvpSUFAUFBUmSgoKCct3VkzOe0+dKw4YNU1pammNISkrKT9kAAOA2k6+A0rJlS+3evVu7du1yDI0bN1Z0dLTj/+7u7lqzZo1jnoSEBB09elRhYWGSpLCwMO3evVupqamOPqtXr5afn59q1ap11fV6eHjIz8/PaQAAAEVXvq5B8fX1VZ06dZzavL29VaZMGUd7z549NXDgQJUuXVp+fn7q27evwsLC1KxZM0lSq1atVKtWLT3++OOaNGmSkpOT9cILLygmJkYeHh4FtFkAAOB2VuDfZvzKK6/I1dVV7du3V0ZGhqKiojR79mzHdDc3Ny1btkxPP/20wsLC5O3trW7dumnMmDEFXQoAALhN/emAsn79eqdxT09PzZo1S7NmzbrmPKGhofriiy/+7KoBAEARxXfxAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOscIuAAWj0tDlee7744Q2N7ESAAD+PI6gAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOvkKKOPHj9c999wjX19fBQQEqF27dkpISHDqc/78ecXExKhMmTLy8fFR+/btlZKS4tTn6NGjatOmjUqUKKGAgAANGTJEFy9e/PNbAwAAioR8BZQNGzYoJiZGW7du1erVq5WZmalWrVrp7Nmzjj4DBgzQ0qVL9cEHH2jDhg365Zdf9OijjzqmZ2VlqU2bNrpw4YK2bNmiRYsWaeHChRoxYkTBbRUAALitFctP55UrVzqNL1y4UAEBAdqxY4fuu+8+paWlaf78+XrnnXf0wAMPSJIWLFigmjVrauvWrWrWrJlWrVqlvXv36quvvlJgYKDq16+vsWPH6rnnntOoUaNUvHjxgts6AABwW/pT16CkpaVJkkqXLi1J2rFjhzIzMxUZGenoU6NGDVWsWFFxcXGSpLi4ONWtW1eBgYGOPlFRUUpPT9eePXv+TDkAAKCIyNcRlMtlZ2crNjZWzZs3V506dSRJycnJKl68uEqWLOnUNzAwUMnJyY4+l4eTnOk5064mIyNDGRkZjvH09PQbLRsAANwGbvgISkxMjL7//nstWbKkIOu5qvHjx8vf398xVKhQ4aavEwAAFJ4bCih9+vTRsmXLtG7dOpUvX97RHhQUpAsXLuj06dNO/VNSUhQUFOToc+VdPTnjOX2uNGzYMKWlpTmGpKSkGykbAADcJvIVUIwx6tOnjz755BOtXbtWlStXdpreqFEjubu7a82aNY62hIQEHT16VGFhYZKksLAw7d69W6mpqY4+q1evlp+fn2rVqnXV9Xp4eMjPz89pAAAARVe+rkGJiYnRO++8o88++0y+vr6Oa0b8/f3l5eUlf39/9ezZUwMHDlTp0qXl5+envn37KiwsTM2aNZMktWrVSrVq1dLjjz+uSZMmKTk5WS+88IJiYmLk4eFR8FsIAABuO/kKKHPmzJEktWjRwql9wYIFeuKJJyRJr7zyilxdXdW+fXtlZGQoKipKs2fPdvR1c3PTsmXL9PTTTyssLEze3t7q1q2bxowZ8+e2BAAAFBn5CijGmOv28fT01KxZszRr1qxr9gkNDdUXX3yRn1UDAIC/EL6LBwAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsk69H3QMFqdLQ5Xnu++OENjexEgCAbTiCAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsE6xwi4AuJ1UGro8z31/nNDmJlYCAEUbR1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xQr7AIA3FyVhi7Pc98fJ7S5iZUAQN5xBAUAAFiHIygArMNRHwAcQQEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArMOD2gCgAPBwOaBgEVAA4C8qP6FKIljh1iKgAABuG4Sqv45CDSizZs3S5MmTlZycrHr16mnmzJlq0qRJYZYEAECBIlTdmEK7SPa9997TwIEDNXLkSO3cuVP16tVTVFSUUlNTC6skAABgiUILKFOnTlWvXr3UvXt31apVS3PnzlWJEiX0xhtvFFZJAADAEoUSUC5cuKAdO3YoMjLyv4W4uioyMlJxcXGFURIAALBIoVyDcvz4cWVlZSkwMNCpPTAwUPv378/VPyMjQxkZGY7xtLQ0SVJ6evpNqS8741ye+96sGvKLmm8Nar41qPnWyE/Nkh11U/PtXUfOMo0x1+9sCsHPP/9sJJktW7Y4tQ8ZMsQ0adIkV/+RI0caSQwMDAwMDAxFYEhKSrpuViiUIyhly5aVm5ubUlJSnNpTUlIUFBSUq/+wYcM0cOBAx3h2drZOnjypMmXKyMXFpUBrS09PV4UKFZSUlCQ/P78CXTb+i/18a7Cfbw32863Bfr51bta+Nsbot99+U0hIyHX7FkpAKV68uBo1aqQ1a9aoXbt2ki6FjjVr1qhPnz65+nt4eMjDw8OprWTJkje1Rj8/P34BbgH2863Bfr412M+3Bvv51rkZ+9rf3z9P/QrtOSgDBw5Ut27d1LhxYzVp0kTTpk3T2bNn1b1798IqCQAAWKLQAspjjz2mX3/9VSNGjFBycrLq16+vlStX5rpwFgAA/PUU6pNk+/Tpc9VTOoXJw8NDI0eOzHVKCQWL/XxrsJ9vDfbzrcF+vnVs2NcuxuTlXh8AAIBbp9CeJAsAAHAtBBQAAGAdAgoAALAOAQUAAFiHgHKZWbNmqVKlSvL09FTTpk21ffv2wi6pSBk/frzuuece+fr6KiAgQO3atVNCQkJhl1XkTZgwQS4uLoqNjS3sUoqkn3/+WV26dFGZMmXk5eWlunXr6ptvvinssoqUrKwsDR8+XJUrV5aXl5fuvPNOjR07Nm/f54Jr2rhxo9q2bauQkBC5uLjo008/dZpujNGIESMUHBwsLy8vRUZG6sCBA7esPgLK/3nvvfc0cOBAjRw5Ujt37lS9evUUFRWl1NTUwi6tyNiwYYNiYmK0detWrV69WpmZmWrVqpXOnj1b2KUVWfHx8Xrttdd09913F3YpRdKpU6fUvHlzubu7a8WKFdq7d6+mTJmiUqVKFXZpRcrEiRM1Z84cvfrqq9q3b58mTpyoSZMmaebMmYVd2m3t7NmzqlevnmbNmnXV6ZMmTdKMGTM0d+5cbdu2Td7e3oqKitL58+dvTYEF8eV/RUGTJk1MTEyMYzwrK8uEhISY8ePHF2JVRVtqaqqRZDZs2FDYpRRJv/32m6latapZvXq1iYiIMP379y/skoqc5557ztx7772FXUaR16ZNG9OjRw+ntkcffdRER0cXUkVFjyTzySefOMazs7NNUFCQmTx5sqPt9OnTxsPDw7z77ru3pCaOoEi6cOGCduzYocjISEebq6urIiMjFRcXV4iVFW1paWmSpNKlSxdyJUVTTEyM2rRp4/S6RsH6/PPP1bhxY3Xo0EEBAQFq0KCBXn/99cIuq8gJDw/XmjVrlJiYKEn67rvvtGnTJj344IOFXFnRdfjwYSUnJzu9f/j7+6tp06a37HOxUJ8ka4vjx48rKysr12P2AwMDtX///kKqqmjLzs5WbGysmjdvrjp16hR2OUXOkiVLtHPnTsXHxxd2KUXaDz/8oDlz5mjgwIF6/vnnFR8fr379+ql48eLq1q1bYZdXZAwdOlTp6emqUaOG3NzclJWVpXHjxik6OrqwSyuykpOTJemqn4s50242AgoKRUxMjL7//ntt2rSpsEspcpKSktS/f3+tXr1anp6ehV1OkZadna3GjRvrpZdekiQ1aNBA33//vebOnUtAKUDvv/++Fi9erHfeeUe1a9fWrl27FBsbq5CQEPZzEcYpHklly5aVm5ubUlJSnNpTUlIUFBRUSFUVXX369NGyZcu0bt06lS9fvrDLKXJ27Nih1NRUNWzYUMWKFVOxYsW0YcMGzZgxQ8WKFVNWVlZhl1hkBAcHq1atWk5tNWvW1NGjRwupoqJpyJAhGjp0qDp16qS6devq8ccf14ABAzR+/PjCLq3IyvnsK8zPRQKKpOLFi6tRo0Zas2aNoy07O1tr1qxRWFhYIVZWtBhj1KdPH33yySdau3atKleuXNglFUktW7bU7t27tWvXLsfQuHFjRUdHa9euXXJzcyvsEouM5s2b57pVPjExUaGhoYVUUdF07tw5ubo6f1y5ubkpOzu7kCoq+ipXrqygoCCnz8X09HRt27btln0ucorn/wwcOFDdunVT48aN1aRJE02bNk1nz55V9+7dC7u0IiMmJkbvvPOOPvvsM/n6+jrOY/r7+8vLy6uQqys6fH19c13X4+3trTJlynC9TwEbMGCAwsPD9dJLL6ljx47avn275s2bp3nz5hV2aUVK27ZtNW7cOFWsWFG1a9fWt99+q6lTp6pHjx6FXdpt7cyZMzp48KBj/PDhw9q1a5dKly6tihUrKjY2Vi+++KKqVq2qypUra/jw4QoJCVG7du1uTYG35F6h28TMmTNNxYoVTfHixU2TJk3M1q1bC7ukIkXSVYcFCxYUdmlFHrcZ3zxLly41derUMR4eHqZGjRpm3rx5hV1SkZOenm769+9vKlasaDw9PU2VKlXM//t//89kZGQUdmm3tXXr1l31Pblbt27GmEu3Gg8fPtwEBgYaDw8P07JlS5OQkHDL6nMxhkfxAQAAu3ANCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQU3BQ//vijXFxctGvXrsIuxWH//v1q1qyZPD09Vb9+/cIu5y+nUqVKmjZt2k1fzxNPPHHrnnR5DcnJyfr73/8ub29vlSxZslBryatRo0ZZ8XthjFHv3r1VunRp695DcGsRUIqoJ554Qi4uLpowYYJT+6effioXF5dCqqpwjRw5Ut7e3kpISHD6fgkUrIULF171Qzk+Pl69e/cusPVcKwRPnz5dCxcuLLD13IhXXnlFx44d065du5SYmFioteTV4MGDrfi9WLlypRYuXKhly5bp2LFjBfb1DDYEV+QPAaUI8/T01MSJE3Xq1KnCLqXAXLhw4YbnPXTokO69916FhoaqTJkyBViVnTIzMwu7BCflypVTiRIlbvp6/P39C/2oxaFDh9SoUSNVrVpVAQEBN7SMP/NavxE+Pj5W/F4cOnRIwcHBCg8PV1BQkIoVs+sr47KysviSwlvllj1UH7dUt27dzMMPP2xq1KhhhgwZ4mj/5JNPzOU/9pEjR5p69eo5zfvKK6+Y0NBQp2U98sgjZty4cSYgIMD4+/ub0aNHm8zMTDN48GBTqlQpc8cdd5g33njDMc/hw4eNJPPuu++asLAw4+HhYWrXrm3Wr1/vtK7du3eb1q1bG29vbxMQEGC6dOlifv31V8f0iIgIExMTY/r372/KlCljWrRocdXtzcrKMqNHjzZ33HGHKV68uKlXr55ZsWKFY7qu+K6JkSNHXnU5ERERpm/fvmbIkCGmVKlSJjAwMFffI0eOmH/84x/G29vb+Pr6mg4dOpjk5ORc+/TNN980oaGhxs/Pzzz22GMmPT39qus05vrfiWGMMZ9++qlp0KCB8fDwMJUrVzajRo0ymZmZTts4e/Zs07ZtW1OiRAlH3bNnzzZVqlQx7u7uplq1aubNN9+8Zh3GGLN9+3YTGRlpypQpY/z8/Mx9991nduzY4dTn1KlTpnfv3iYgIMDxs126dOlVtyOnjtDQUPPKK68YY4zp3Lmz6dixo9MyL1y4YMqUKWMWLVpkjDFmxYoVpnnz5sbf39+ULl3atGnTxhw8eNBpey8fIiIijDH/fb3mOH/+vOnbt68pV66c8fDwMM2bNzfbt2/Pte+/+uor06hRI+Pl5WXCwsLM/v37HX127dplWrRoYXx8fIyvr69p2LChiY+Pv+r+Cw0NverPMK+vm9dff91UqlTJuLi4XHX5x48fN506dTIhISHGy8vL1KlTx7zzzjtX7Xu5efPmmfLlyxsvLy/Trl07M2XKFOPv759r/cYY8+WXXxoPDw9z6tQpp2X069fP3H///Y7xr7/+2tx7773G09PTlC9f3vTt29ecOXPGaV+MGzfOdO/e3fj4+JgKFSqY11577Zo1duvWzWnf5bwPZWVlmZdeeslUqlTJeHp6mrvvvtt88MEHjvkuXrxoevTo4ZherVo1M23aNKdtu/L1sm7dOsfP/vLt/Pbbb40kc/jwYWOMMQsWLDD+/v7ms88+MzVr1jRubm7m8OHD5vz582bQoEEmJCTElChRwjRp0sSsW7fuuj8H5B0BpYjKeZP++OOPjaenp0lKSjLG3HhA8fX1NTExMWb//v1m/vz5RpKJiooy48aNM4mJiWbs2LHG3d3dsZ6cgFK+fHnz4Ycfmr1795onn3zS+Pr6muPHjxtjLn3IlStXzgwbNszs27fP7Ny50/z97393egOMiIgwPj4+ZsiQIWb//v1OHxqXmzp1qvHz8zPvvvuu2b9/v3n22WeNu7u7SUxMNMYYc+zYMVO7dm0zaNAgc+zYMfPbb79ddTkRERHGz8/PjBo1yiQmJppFixYZFxcXs2rVKmPMpTfK+vXrm3vvvdd88803ZuvWraZRo0aOD8ecferj42MeffRRs3v3brNx40YTFBRknn/++Wv+vDIyMsyxY8ccw9q1a42np6eZP3++McaYjRs3Gj8/P7Nw4UJz6NAhs2rVKlOpUiUzatQoxzIkmYCAAPPGG2+YQ4cOmSNHjpiPP/7YuLu7m1mzZpmEhAQzZcoU4+bmZtauXXvNWtasWWPeeusts2/fPrN3717Ts2dPExgY6AhYWVlZplmzZqZ27dpm1apV5tChQ2bp0qXmiy++MBkZGWbatGnGz8/PsS05+/rygLJs2TLj5eXl9HNYunSp8fLycqznww8/NB999JE5cOCA+fbbb03btm1N3bp1TVZWljHmUpDKCRbHjh0zJ06cMMbkDij9+vUzISEh5osvvjB79uwx3bp1M6VKlXL0z/mQatq0qVm/fr3Zs2eP+dvf/mbCw8Mdy6hdu7bp0qWL2bdvn0lMTDTvv/++2bVr11X3X2pqqmndurXp2LGjOXbsmDl9+nSeXzfe3t6mdevWZufOnea777676vJ/+uknM3nyZPPtt9+aQ4cOmRkzZhg3Nzezbdu2a/5MN23aZFxdXc3kyZNNQkKCmTVrlilduvQ1A8rFixdNYGCg+Z//+R/H9CvbDh48aLy9vc0rr7xiEhMTzebNm02DBg3ME0884ZgnNDTUlC5d2syaNcscOHDAjB8/3ri6ul7z9/j06dNmzJgxpnz58ubYsWMmNTXVGGPMiy++aGrUqGFWrlxpDh06ZBYsWGA8PDwcf/BcuHDBjBgxwsTHx5sffvjBvP3226ZEiRLmvffeM8YY89tvv5mOHTua1q1bO16XGRkZeQ4o7u7uJjw83GzevNns37/fnD171jz55JMmPDzcbNy40Rw8eNBMnjzZeHh4ON5z8OcRUIqoy9+kmzVrZnr06GGMufGAEhoa6vhgMMaY6tWrm7/97W+O8YsXLxpvb2/z7rvvGmP+G1AmTJjg6JOZmWnKly9vJk6caIwxZuzYsaZVq1ZO605KSjKSHN+YGRERYRo0aHDd7Q0JCTHjxo1zarvnnnvMM8884xivV6/eNY+c5IiIiDD33ntvruU899xzxhhjVq1aZdzc3MzRo0cd0/fs2WMkOf4qHzlypClRooTTEZMhQ4aYpk2bXnc7jLn0F3KVKlWcam/ZsqV56aWXnPq99dZbJjg42DEuycTGxjr1CQ8PN7169XJq69Chg3nooYfyVIsxlwKJr6+vWbp0qTHm0l/Xrq6u1/xW05y/OK90eUDJzMw0ZcuWdTqa07lzZ/PYY49ds45ff/3VSDK7d+82xvz3Nfbtt9869bv8tX/mzBnj7u5uFi9e7Jh+4cIFExISYiZNmmSMcT6CkmP58uVGkvn999+NMcb4+vqahQsXXrO2Kz3yyCNOR7/y+rpxd3d3fCjnR5s2bcygQYOuOf2xxx4zbdq0cWqLjo6+ZkAxxpj+/fubBx54wDF+5VGVnj17mt69ezst8+uvvzaurq6O/RYaGmq6dOnimJ6dnW0CAgLMnDlzrlnrle8/58+fNyVKlDBbtmxx6tezZ0/TuXPnay4nJibGtG/f3jF+ZXA1xuQ5oEhyCqRHjhwxbm5u5ueff3ZaXsuWLc2wYcOuWRPyh2tQ/gImTpyoRYsWad++fTe8jNq1a8vV9b8vl8DAQNWtW9cx7ubmpjJlyig1NdVpvrCwMMf/ixUrpsaNGzvq+O6777Ru3Tr5+Pg4hho1aki6dB46R6NGjf6wtvT0dP3yyy9q3ry5U3vz5s1vaJvvvvtup/Hg4GDHdu3bt08VKlRQhQoVHNNr1aqlkiVLOq2rUqVK8vX1veoyFi9e7LTNX3/9taNfZmam2rdvr9DQUE2fPt3R/t1332nMmDFO8/Xq1UvHjh3TuXPnHP0aN27sVPu+ffvyvV9SUlLUq1cvVa1aVf7+/vLz89OZM2d09OhRSdKuXbtUvnx5VatW7ZrLuJ5ixYqpY8eOWrx4sSTp7Nmz+uyzzxQdHe3oc+DAAXXu3FlVqlSRn5+fKlWqJEmOOvLi0KFDyszMdNoH7u7uatKkSa59cPnPPTg4WJIcP7OBAwfqySefVGRkpCZMmOD0+syLvL5uQkNDVa5cuT9cVlZWlsaOHau6deuqdOnS8vHx0ZdffvmH+yUhIUFNmjRxarty/ErR0dFav369fvnlF0mXXrdt2rRxXN/z3XffaeHChU6vyaioKGVnZ+vw4cOO5Vy+X11cXBQUFJTrfeKPHDx4UOfOndPf//53p3W9+eabTj+HWbNmqVGjRipXrpx8fHw0b968fL1W/kjx4sWdtmP37t3KyspStWrVnGrasGFDvl8buDa7rj7CTXHfffcpKipKw4YN0xNPPOE0zdXVVcYYp7arXVzp7u7uNO7i4nLVtvxcPHbmzBm1bdtWEydOzDUt5wNCkry9vfO8zILwZ7fresv4xz/+oaZNmzqm3XHHHY7/P/3000pKStL27dudLg48c+aMRo8erUcffTTXujw9PR3/L4h91a1bN504cULTp09XaGioPDw8FBYW5rho08vL60+vQ7r0ARgREaHU1FStXr1aXl5eat26tWN627ZtFRoaqtdff10hISHKzs5WnTp1btrFo5f/zHLudMv5mY0aNUr//ve/tXz5cq1YsUIjR47UkiVL9M9//rNAa8jLz2/y5MmaPn26pk2bprp168rb21uxsbEFvl/uuece3XnnnVqyZImefvppffLJJ053R505c0ZPPfWU+vXrl2veihUrOv5fEO8TkrR8+XKn3xVJ8vDwkCQtWbJEgwcP1pQpUxQWFiZfX19NnjxZ27Zt+8Nl5/zRdfl74NXe/7y8vJzufjxz5ozc3Ny0Y8cOubm5OfX18fHJ87bhjxFQ/iImTJig+vXrq3r16k7t5cqVU3Jysowxjl/AgnzuwNatW3XfffdJki5evKgdO3aoT58+kqSGDRvqo48+UqVKlf7Ulfp+fn4KCQnR5s2bFRER4WjfvHnzdf9KzK+aNWsqKSlJSUlJjr+G9+7dq9OnT6tWrVp5Woavr6/T0ZUcU6dO1fvvv68tW7bkupuiYcOGSkhI0F133ZXvejdv3qxu3bo52jZv3vyHtW7evFmzZ8/WQw89JElKSkrS8ePHHdPvvvtu/fTTT0pMTLzqUZTixYsrKyvrurWFh4erQoUKeu+997RixQp16NDB8WF24sQJJSQk6PXXX9ff/vY3SdKmTZtyrUfSH67rzjvvVPHixbV582aFhoZKuvQBFB8fr9jY2OvWeLlq1aqpWrVqGjBggDp37qwFCxbkOaAUxOsmx+bNm/XII4+oS5cuki6FqMTExD9cTvXq1RUfH+/UduX41URHR2vx4sUqX768XF1d1aZNG8e0hg0bau/evfl+TeZXrVq15OHhoaNHjzr9fl9u8+bNCg8P1zPPPONou/JIxtVelzlHq44dO6ZSpUpJytv7X4MGDZSVlaXU1FTH6xMFj1M8fxF169ZVdHS0ZsyY4dTeokUL/frrr5o0aZIOHTqkWbNmacWKFQW23lmzZumTTz7R/v37FRMTo1OnTqlHjx6SpJiYGJ08eVKdO3dWfHy8Dh06pC+//FLdu3fP0wfc5YYMGaKJEyfqvffeU0JCgoYOHapdu3apf//+BbYtkhQZGenYlzt37tT27dvVtWtXRURE5Dq9kh9fffWVnn32WU2ePFlly5ZVcnKykpOTlZaWJkkaMWKE3nzzTY0ePVp79uzRvn37tGTJEr3wwgt/uNwhQ4Zo4cKFmjNnjg4cOKCpU6fq448/1uDBg685T9WqVfXWW29p37592rZtm6Kjo52OmkREROi+++5T+/bttXr1ah0+fFgrVqzQypUrJV06vXXmzBmtWbNGx48fdzoFdaV///vfmjt3rlavXu10eqdUqVIqU6aM5s2bp4MHD2rt2rUaOHCg07wBAQHy8vLSypUrlZKS4thXl/P29tbTTz+tIUOGaOXKldq7d6969eqlc+fOqWfPnn+473L8/vvv6tOnj9avX68jR45o8+bNio+PV82aNfM0v1Swr5uqVatq9erV2rJli/bt26ennnpKKSkpfzhP37599cUXX2jq1Kk6cOCAXnvtNa1YseK6z0TKqXfcuHH617/+5ThiIUnPPfectmzZoj59+mjXrl06cOCAPvvsM8cfIAXF19dXgwcP1oABA7Ro0SIdOnRIO3fu1MyZM7Vo0SJJl/bJN998oy+//FKJiYkaPnx4rgBWqVIl/e///q8SEhJ0/PhxZWZm6q677lKFChU0atQoHThwQMuXL9eUKVOuW1O1atUUHR2trl276uOPP9bhw4e1fft2jR8/XsuXLy/Q7f8rI6D8hYwZMybXodWaNWtq9uzZmjVrlurVq6ft27f/4YdXfk2YMEETJkxQvXr1tGnTJn3++ecqW7asJDmOemRlZalVq1aqW7euYmNjVbJkSafrXfKiX79+GjhwoAYNGqS6detq5cqV+vzzz1W1atUC2xbp0uHpzz77TKVKldJ9992nyMhIValSRe+9996fWu6mTZuUlZWl//znPwoODnYMOQErKipKy5Yt06pVq3TPPfeoWbNmeuWVVxxHBa6lXbt2mj59ul5++WXVrl1br732mhYsWKAWLVpcc5758+fr1KlTatiwoR5//HH169cv17M8PvroI91zzz3q3LmzatWqpWeffdYRKsPDw/Wf//xHjz32mMqVK6dJkyZdc13R0dHau3ev7rjjDqfrRFxdXbVkyRLt2LFDderU0YABAzR58mSneYsVK6YZM2botddeU0hIiB555JGrrmPChAlq3769Hn/8cTVs2FAHDx7Ul19+6fiL+Xrc3Nx04sQJde3aVdWqVVPHjh314IMPavTo0XmaXyrY180LL7yghg0bKioqSi1atFBQUNB1H0DWvHlzzZ07V1OnTlW9evW0cuVKDRgwwOn04NXcddddatKkif73f//XKUBKl46kbdiwQYmJifrb3/6mBg0aaMSIEQoJCcn3Nl3P2LFjNXz4cI0fP141a9ZU69attXz5clWuXFmS9NRTT+nRRx/VY489pqZNm+rEiRNOR1MkqVevXqpevboaN26scuXKafPmzXJ3d9e7776r/fv36+6779bEiRP14osv5qmmBQsWqGvXrho0aJCqV6+udu3aKT4+3un0Fv4cF3PlBQgAgCKvV69e2r9/v9NF2oBNuAYFAP4CXn75Zcf3A61YsUKLFi3S7NmzC7ss4Jo4ggIAfwEdO3bU+vXr9dtvv6lKlSrq27ev/vOf/xR2WcA1EVAAAIB1uEgWAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFjn/wN+epBf/XOZugAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot as hist\n",
    "plt.hist(np.minimum(nz_act, 10).numpy(), bins=20)\n",
    "plt.xlabel('Number of non-zero activations for a given feature')\n",
    "plt.title(\"Clamped at 10!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clamp nz_act to be at most 1000\n",
    "clamp_nz_act = np.minimum(nz_act, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380,\n",
       "        580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380,\n",
       "        580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380,\n",
       "        580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380,\n",
       "        580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380, 580380,\n",
       "        580380, 580378, 580376, 580376, 580375, 580326, 579563, 574563, 572567,\n",
       "        571484, 571239, 571140, 570066, 569257, 569040, 569020, 568072, 567984,\n",
       "        567942, 567941, 567936, 567727, 567236, 567014, 566994, 566798, 566063,\n",
       "        565892, 565753, 565376, 565313, 564901, 564752, 564634, 564558, 564495,\n",
       "        563876, 562940, 562734, 562701, 562448, 562203, 561838, 561835, 561810,\n",
       "        561496, 561033, 560993, 560908, 560657, 560578, 560361, 560184, 560173,\n",
       "        559803, 559795, 559414, 559252, 559157, 559146, 558797, 558619, 558597,\n",
       "        558477, 557878, 557876, 557798, 557401, 557224, 557149, 556575, 556454,\n",
       "        556333, 556281, 555664, 555652, 555611, 555537, 555523, 555497, 555483,\n",
       "        555468, 555456, 555450, 555303, 555021, 554824, 554785, 554763, 554680,\n",
       "        554624, 554571, 554412, 554402, 554395, 554360, 554356, 554279, 554145,\n",
       "        554119, 554076, 553938, 553839, 553755, 553704, 553689, 553688, 553583,\n",
       "        553573, 553485, 553433, 553352, 553291, 553257, 553135, 553057, 553013,\n",
       "        552996, 552728, 552607, 552578, 552350, 552308, 552168, 552135, 552090,\n",
       "        551830, 551816, 551762, 551671, 551634, 551628, 551626, 551606, 551336,\n",
       "        551316, 551306, 551298, 551209, 551172, 551154, 551133, 551101, 551001,\n",
       "        550983, 550966, 550909, 550888, 550882, 550809, 550742, 550722, 550697,\n",
       "        550686, 550562, 550398, 550355, 550353, 550276, 550210, 550092, 550043,\n",
       "        549991, 549967, 549897, 549530, 549504, 549441, 549437, 549318, 549045,\n",
       "        549036, 548984, 548945, 548738, 548737, 548329, 548313, 548294, 548158,\n",
       "        548148, 548109, 548090, 547915, 547873, 547861, 547800, 547585, 547472,\n",
       "        547338, 547250, 547248, 547216, 547215, 547179, 547100, 547093, 547063,\n",
       "        546930, 546845, 546677, 546626, 546609, 546573, 546440, 546352, 546292,\n",
       "        546188, 546128, 545988, 545907, 545829, 545614, 545587, 545408, 545333,\n",
       "        545288, 545264, 545049, 545018, 544965, 544933, 544668, 544657, 544618,\n",
       "        544462, 544286, 544189, 544083, 544020, 543950, 543886, 543508, 543486,\n",
       "        543387, 543325, 542789, 542750, 542633, 542349, 542171, 541966, 541678,\n",
       "        541653, 541435, 541134, 540925, 540824, 540807, 540783, 540762, 540283,\n",
       "        540280, 539867, 539855, 539685, 539598, 539395, 538961, 538562, 538451,\n",
       "        538131, 537212, 537201, 537036, 536884, 535999, 535900, 535871, 535788,\n",
       "        535761, 535281, 534713, 534583, 534209, 534166, 534130, 533682, 533278,\n",
       "        533244, 532289, 532262, 531301, 529921, 524644, 514280,  84269,  23203,\n",
       "         21495,  12490,  10943,   9870,   9756,   2850,   2592,   1509,    739,\n",
       "           623,    419,    293,    110,    100,     92,     82,     81]),\n",
       "indices=tensor([ 492, 1113,  287,  463, 1559,   31,  913, 1689,  253, 1774,  909, 1198,\n",
       "        1339, 1734, 1561,  708,  356, 1717,  890,  191,   63, 1219, 1456,  733,\n",
       "        1597, 1840, 1160,  640, 1325,  642, 1380, 1374, 1545,  414,  466, 1426,\n",
       "         139, 1530,  973,  554, 2013,  525,  235,  981, 2016,  317,  915,  293,\n",
       "        1259,  156,  479,  345,  891, 1540, 1034,  720,  747,   62,  755,  364,\n",
       "         341, 1099,  820, 1957, 1988,  372,   95, 1669, 1283, 1403,  912,  318,\n",
       "         524,  848,   87, 1803,  750, 1191, 1914,  927,  308, 1654, 1624, 1177,\n",
       "        1688, 1109, 1345, 1043,  108, 1120,  232, 1049,  899, 1483, 1702,  518,\n",
       "         437, 1982, 1786, 1952, 1608, 1709,  100,  422,  619, 1569, 1672,  351,\n",
       "        1233, 1780,  636, 1251,   82,  130,  999,  841, 1350, 1086,  587,  190,\n",
       "          24,   47, 1343,  117, 1674, 1968, 1977, 1479,  949,   77, 1649,  138,\n",
       "        1616, 1075, 1992, 1806,  421, 1122,  324,  153, 1619, 1155, 1331,  977,\n",
       "        1498, 1273,  313,  482,  402, 1370, 1817,  821,  863,  540,  858,  833,\n",
       "        1342, 1544, 1006,  536, 1020,  352,  331, 1755, 1015, 1065,  204,  705,\n",
       "         431,  295,  788,  760, 1828, 1031, 1508, 2015, 1327,  601, 1963, 1641,\n",
       "         695,  734,  908,  261,   18, 1429,  661, 1611, 1527,  532, 1148, 1552,\n",
       "         543,  741, 1323, 1054, 1264,  939,  911,  172,  125, 1537,  599,  594,\n",
       "        1014,  736,  460,  284, 1889,  633,  285, 1042,  982,  464, 1028, 1733,\n",
       "         387, 1143,  574, 1885, 1718, 2028,  627,  134,  330, 1151,  980,  743,\n",
       "         803,  136, 1105, 1369, 1243,  244, 1386, 1816, 1791, 1180, 1859, 2007,\n",
       "         321, 1081,  552,  157, 1951,  346,  596, 1570, 1274,  380, 1841, 1675,\n",
       "        1610,  579, 1093, 1304,   72,  675,  765, 1235,  894, 1305,  398, 1922,\n",
       "        2026,  732,  393, 1996, 1287,  604,   84,  333, 1030,  972, 1524, 1320,\n",
       "        1581,  476,  188, 1012, 1678, 1229, 1808,  742,    8, 1333,  508, 1041,\n",
       "         652,  323, 1268, 1853,   56,  113, 1948,  876, 1101, 1467, 1965,  794,\n",
       "        1657, 1905, 1908,  114, 1797, 2014, 1820, 1497,  963, 1063, 1881,  336,\n",
       "        1190,  237,  411,  167,  577, 1731, 1158,  860, 1707,  145, 1278,  480,\n",
       "        1584, 1664, 1353, 1587, 1532,  896,  966,  933,  551,  326,  533,  861,\n",
       "        2034, 1960, 1162, 1381,   14,  307,    5,  275, 1024, 1313,  777,  629,\n",
       "        2043,  371]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz_act.topk(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dictionary_activations[:,1734].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "minn = dictionary_activations.min(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1734)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minn.values.argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interp\n",
    "Investigate the example sentences the activate this feature.\n",
    "\n",
    "Max: show max activating (tokens,contexts)\n",
    "\n",
    "Uniform: Show range of activations from each bin (e.g. sample an example from 1-2, 2-3, etc). \n",
    "[Note: if a feature is monosemantic, then the full range of activations should be that feature, not just max-activating ones]\n",
    "\n",
    "Full_text: shows the full text example\n",
    "\n",
    "Text_list: shows up to the most activating example (try w/ max activating on a couple of examples to see)\n",
    "\n",
    "ablate_text: remove the context one token at a time, and show the decrease/increase in activation of that feature\n",
    "\n",
    "ablate_feature_direction: removes feature direction from model's activation mid-inference, showing the logit diff in the output for every token.\n",
    "\n",
    "logit_lens: show the logit lens for that feature. If matches ablate_feature_direction, then the computation path is through the residual stream, else, it's through future layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature index: 8\n",
      "MCS: 0.141135573387146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-c5d65320-da15\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-c5d65320-da15\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"An\", \" investigation\", \" of\", \" penetration\", \" depth\", \" control\", \" using\", \" parallel\", \" opposed\", \" ultrasound\", \"\\n\", \"I\", \" re\", \"-\", \"read\", \" The\", \" Gods\", \" The\", \"ms\", \"elves\", \" because\", \" it\", \" was\", \" 1972\", \"\\u2019\", \"s\", \" Hugo\", \" winner\", \",\", \" and\", \" all\", \" I\", \" could\", \" remember\", \" about\", \" it\", \" was\", \" three\", \" g\", \"endered\", \" aliens\", \",\", \" which\", \" didn\", \"\\u2019\", \"t\", \" really\", \" seem\", \" fair\", \".\", \" The\", \" part\", \" of\", \" the\", \" book\", \" I\", \" remembered\", \" is\", \" the\", \" middle\", \".\", \" It\", \"\\n\", \"Six\", \" cases\", \" of\", \" L\", \"\\n\", \"A\", \" major\", \" outbreak\", \" of\", \" who\", \"oping\", \" cough\", \" has\", \" struck\", \" a\", \" Michigan\", \" area\", \" where\", \" many\", \" people\", \" opted\", \" out\", \" of\", \" vacc\", \"inations\", \" against\", \" the\", \" disease\", \".\", \"\\\\newline\", \"\\\\newline\", \"At\", \" a\", \" single\", \" school\", \" in\", \" Grand\", \" Tra\", \"verse\", \" County\", \",\", \" which\", \" has\", \" the\", \" state\", \"\\u2019\", \"s\", \" highest\", \" rates\", \" of\", \" parents\", \" choosing\", \" not\", \" have\", \" their\", \" children\", \" vaccinated\", \",\", \" there\", \" have\", \"\\n\", \"ins\", \"urance\", \"Qu\", \"otes\", \"\\\\newline\", \"\\\\newline\", \"in\", \" the\", \" news\", \"\\\\newline\", \"\\\\newline\", \"Get\", \" My\", \" Best\", \" Adv\", \"ice\", \"\\\\newline\", \"\\\\newline\", \"L\", \"aura\", \" Adams\", \" is\", \" an\", \" award\", \"-\", \"winning\", \" personal\", \" finance\", \" expert\", \",\", \" consumer\", \" advocate\", \" and\", \" author\", \" of\", \" Money\", \" Girl\", \"\\u2019\", \"s\", \" Smart\", \" M\", \"oves\", \" to\", \" Grow\", \" Rich\", \".\", \" As\", \" insurance\", \"Qu\", \"otes\", \" spokesperson\", \"\\n\", \"package\", \" problem\", \"0\", \"958\", \"\\\\newline\", \"\\\\newline\", \"import\", \" (\", \"\\\\newline\", \"\\t\", \"\\\"\", \"testing\", \"\\\"\", \"\\\\newline\\\\newline\", \"\\t\", \"\\\"\", \"github\", \".\", \"com\", \"/\", \"a\", \"Qu\", \"\\n\", \"Q\", \"\\n\", \"A\", \" man\", \" facing\", \"\\n\", \"Management\", \" of\", \" ventricular\", \" tachy\", \"cardia\", \" in\", \" the\", \" ablation\", \" era\", \":\", \" results\", \" of\", \" the\", \" European\", \" Heart\", \" R\", \"hythm\", \" Association\", \" Survey\", \".\", \"\\\\newline\", \"Patients\", \" with\", \" sustained\", \" ventricular\", \" tachy\", \"cardia\", \" (\", \"VT\", \")\", \" are\", \" at\", \" risk\", \" of\", \" sudden\", \"\\n\", \"Ox\", \"id\", \"ative\", \" stress\", \" in\", \" stable\", \" cystic\", \"\\n\", \"Is\", \"otype\", \"-\", \"specific\", \" activation\", \" of\", \" cystic\", \"\\n\"], \"activations\": [[[-0.5347746014595032]], [[-0.8596842885017395]], [[-0.8214035630226135]], [[-0.5691791772842407]], [[-1.2466756105422974]], [[-0.8149211406707764]], [[-0.6169969439506531]], [[-1.1139599084854126]], [[-0.1442575752735138]], [[-0.0985436961054802]], [[0.0]], [[-0.7536373138427734]], [[-0.8607405424118042]], [[-0.6805848479270935]], [[-0.3449147939682007]], [[-0.3599744439125061]], [[-0.5836899876594543]], [[-0.6668251752853394]], [[-0.38736194372177124]], [[0.0]], [[-0.3242819905281067]], [[-0.7348379492759705]], [[-0.7706719040870667]], [[-0.8746841549873352]], [[-0.38168078660964966]], [[-0.7190046906471252]], [[-0.4571307599544525]], [[-0.5445494651794434]], [[-0.7497583031654358]], [[-0.2682342529296875]], [[0.0]], [[-0.6047703623771667]], [[-0.6919623613357544]], [[-1.1814227104187012]], [[-0.6158837676048279]], [[-0.3568471372127533]], [[-0.4503789246082306]], [[-0.703525960445404]], [[-0.48048603534698486]], [[-0.7814308404922485]], [[-0.5507070422172546]], [[-1.0121008157730103]], [[-0.3724437654018402]], [[-1.0228794813156128]], [[-0.19158527255058289]], [[-0.45921656489372253]], [[-0.6326132416725159]], [[-0.4786085784435272]], [[-0.7970715761184692]], [[-0.113946333527565]], [[-0.5163028240203857]], [[-0.7295278906822205]], [[-0.5323269367218018]], [[-0.5190249681472778]], [[-0.6544814109802246]], [[-0.6381992101669312]], [[-1.153673529624939]], [[-0.3415542542934418]], [[-0.5289303064346313]], [[-1.0642879009246826]], [[-0.36127957701683044]], [[-0.3975864052772522]], [[0.0]], [[-1.0694934129714966]], [[-1.5234684944152832]], [[-1.1990467309951782]], [[-0.6736522912979126]], [[0.0]], [[-0.6434338092803955]], [[-1.6276086568832397]], [[-1.3873182535171509]], [[-1.3583028316497803]], [[-0.8922545909881592]], [[-1.1207364797592163]], [[-1.107973337173462]], [[-1.3253942728042603]], [[-1.2278680801391602]], [[-0.46339017152786255]], [[-0.5949389338493347]], [[-0.4923214614391327]], [[-0.47908738255500793]], [[-0.4480147063732147]], [[-0.25342419743537903]], [[-0.9083605408668518]], [[-0.4644073247909546]], [[-0.8437942266464233]], [[-0.4701079726219177]], [[0.0]], [[-0.6008161902427673]], [[-0.6381850838661194]], [[-0.9932390451431274]], [[-0.04629122465848923]], [[-0.2979852557182312]], [[-0.12700949609279633]], [[-0.9502356648445129]], [[-0.14868150651454926]], [[-1.369170069694519]], [[-0.04883900657296181]], [[-0.5083339214324951]], [[-1.121988296508789]], [[-0.5424542427062988]], [[-0.5699174404144287]], [[-0.77793949842453]], [[-0.908123254776001]], [[-0.7229596972465515]], [[-0.9361046552658081]], [[-0.6159905195236206]], [[-1.029727578163147]], [[-0.5162164568901062]], [[-0.8077868223190308]], [[-0.5009250044822693]], [[-0.6008471846580505]], [[-0.7296142578125]], [[-0.19534103572368622]], [[-0.949134111404419]], [[-0.47639158368110657]], [[-0.8267544507980347]], [[-0.8487973213195801]], [[-0.05055158585309982]], [[0.0]], [[-0.7770946025848389]], [[-0.7769407629966736]], [[-0.7722501158714294]], [[0.0]], [[-0.6528830528259277]], [[-0.824016809463501]], [[-1.5120792388916016]], [[-1.1098662614822388]], [[-0.37422317266464233]], [[-0.5167765617370605]], [[-0.7097108364105225]], [[-0.5789244771003723]], [[-0.6094382405281067]], [[-0.24080274999141693]], [[-0.36196669936180115]], [[-0.9620930552482605]], [[-0.7834005355834961]], [[-0.29163819551467896]], [[0.0]], [[-0.31129971146583557]], [[-0.04234158620238304]], [[-0.218960702419281]], [[-0.3950400650501251]], [[-0.2820121645927429]], [[-0.12194022536277771]], [[-0.4721111059188843]], [[-0.03614811599254608]], [[-0.2377559244632721]], [[-0.46288102865219116]], [[-0.4077930152416229]], [[-0.4962766468524933]], [[-0.07758831977844238]], [[-0.6951674222946167]], [[-0.5103999972343445]], [[-0.12547148764133453]], [[-0.20203495025634766]], [[0.0]], [[-0.4053557217121124]], [[-0.1901969313621521]], [[-0.07411988079547882]], [[-0.1878696233034134]], [[-0.13403239846229553]], [[0.0]], [[-0.2802259922027588]], [[0.0]], [[0.0]], [[-0.3649694323539734]], [[0.0]], [[-0.49835819005966187]], [[-0.2571570575237274]], [[-0.48721620440483093]], [[-0.17889943718910217]], [[-0.7033945322036743]], [[-0.9803029298782349]], [[-1.0985769033432007]], [[0.0]], [[-0.3073727488517761]], [[-1.4453651905059814]], [[-0.888310432434082]], [[-0.12241771817207336]], [[-0.2557603716850281]], [[-0.21044808626174927]], [[-0.047424428164958954]], [[-0.7781672477722168]], [[-0.5598117709159851]], [[-0.49334007501602173]], [[-0.7673019766807556]], [[-0.0911923423409462]], [[-0.009020480327308178]], [[-0.6222584843635559]], [[-0.432966947555542]], [[-0.48733195662498474]], [[-0.32367318868637085]], [[-0.43145284056663513]], [[-0.8649982213973999]], [[-1.599010705947876]], [[-0.2572116553783417]], [[-1.4134047031402588]], [[0.0]], [[-1.621924638748169]], [[0.0]], [[-0.6434330940246582]], [[-1.0825201272964478]], [[-1.840989112854004]], [[0.0]], [[-1.1905893087387085]], [[-0.7706681489944458]], [[-1.0888739824295044]], [[-1.1798371076583862]], [[-1.4283034801483154]], [[-0.6831533312797546]], [[-0.7349103093147278]], [[-0.6539373397827148]], [[-0.8080466985702515]], [[-0.6796669363975525]], [[-0.202498659491539]], [[-0.6556844711303711]], [[-0.8772046566009521]], [[-0.6781644821166992]], [[-0.37898606061935425]], [[-0.4809565842151642]], [[-0.9067529439926147]], [[-0.47190627455711365]], [[-0.7586601972579956]], [[0.0]], [[-0.2201656550168991]], [[-0.8386788368225098]], [[-1.2547149658203125]], [[-1.2581862211227417]], [[-0.7078711986541748]], [[-0.8257668018341064]], [[-1.0910210609436035]], [[-1.16586172580719]], [[-1.0412392616271973]], [[-1.2706888914108276]], [[-0.6152937412261963]], [[-0.8707994818687439]], [[-0.9860463738441467]], [[-1.231773018836975]], [[-2.0576934814453125]], [[0.0]], [[-0.5539510846138]], [[-1.0606646537780762]], [[-1.1903166770935059]], [[-1.312605381011963]], [[-1.0043879747390747]], [[-1.7013254165649414]], [[-2.3193395137786865]], [[0.0]], [[-0.5098806023597717]], [[-0.9167181849479675]], [[-1.049119472503662]], [[-1.0128523111343384]], [[-1.2962523698806763]], [[-1.1698598861694336]], [[-2.561502456665039]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7feafcf8a260>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 0\n",
    "best_feature = int(max_indices[N])\n",
    "ll = [1717, 1998, 1415, 1536, 1287, 1284, 1291,  636, 1746, 1724, 1375, 1220,\n",
    "         509, 1614, 1129,  413, 1225,  322,   94,  133,  177,  300,  617, 1599,\n",
    "        1602, 1719, 1940, 1411, 1886,  666, 1697,  733,  814, 1515, 2024, 1156,\n",
    "         797, 1817,  907,  256,   65,  298,  919, 1469, 2028,  430,  449, 2041,\n",
    "         111, 1971,  513, 1308, 1297,  458,    4,  875,   11, 1993, 1519,  949,\n",
    "        1619,  326, 1951,  401, 1528, 1497,  962, 1502,   40,  143,  836, 1215,\n",
    "        1961, 1526,   57, 1849, 1190,  632,  237,  337, 1417,  101, 1844,  532,\n",
    "        1776, 2039, 1002,  903,  820, 1270,  883,  107,  973,  924,   43,  288,\n",
    "         528, 1590, 1181, 1871]\n",
    "# ll =  [1717, 1828, 1291, 1998, 1284, 1415, 1820, 4, 1672, 401, 1752, 2028, 1154, 1971, 1746, 1454, 636, 1287, 1536, 1220, 509, 1156, 1602, 1375, 1129, 413, 1817, 1599, 1724, 1951, 648, 741, 962, 1270, 949, 453, 57, 900, 1394, 1619, 322, 814, 2043, 182, 1474, 1469, 2031, 300, 1576, 1225, 2041, 706, 1614, 177, 814, 133, 1515, 1886, 31]\n",
    "# best_feature = ll[99]\n",
    "# 568,  516, 1894,\n",
    "# 7231, 4726, 3885, 2909\n",
    "# best_feature = 5277#, 5277, 7231, 7041,\n",
    "# indices=tensor([2908, 7231, 5277, 2177, 4612, 3885, 3450, 5991,  695, 1651]))\n",
    "# tensor([[ 0],\n",
    "#         [ 3],\n",
    "#         [ 4],\n",
    "#         [ 5],\n",
    "#         [ 6],\n",
    "#         [ 8],\n",
    "#         [10],\n",
    "#         [12],\n",
    "#         [14],\n",
    "#         [16]])\n",
    "best_feature = 8#, 2908, 6207\n",
    "# print(\"bias:\", smaller_auto_encoder.encoder_bias.detach().cpu().numpy()[best_feature])\n",
    "print(f\"Feature index: {best_feature}\")\n",
    "print(f\"MCS: {max_cosine_similarities[best_feature]}\")\n",
    "text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"uniform\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"max\")\n",
    "visualize_text(text_list, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ce7e01e7-465e\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.40.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ce7e01e7-465e\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Is\", \" there\", \" a\", \" way\", \" to\", \" get\", \" a\", \" non\", \"\\n\", \"L\", \"oyal\", \"ty\", \" points\", \",\", \" concert\", \" tickets\", \" and\", \" in\", \"-\", \"game\", \" items\", \" will\", \" soon\", \" appear\", \" on\", \" a\", \" new\", \" protocol\", \" built\", \" on\", \" the\", \" mon\", \"ero\", \" network\", \".\", \"\\\\newline\", \"\\\\newline\", \"Call\", \"ed\", \" T\", \"ari\", \",\", \" this\", \" new\", \" digital\", \" assets\", \" protocol\", \" will\", \" help\", \" support\", \" non\", \"\\n\", \"Introduction\", \"\\\\newline\", \"============\", \"\\\\newline\", \"\\\\newline\", \"R\", \"ational\", \"e\", \"\\\\newline\", \"---------\", \"\\\\newline\", \"\\\\newline\", \"Early\", \" life\", \" experience\", \" is\", \" a\", \" determinant\", \" of\", \" social\", \" inequalities\", \" in\", \" physical\", \",\", \" mental\", \",\", \" and\", \" social\", \" well\", \"-\", \"being\", \" \\\\[[@\", \"ref\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"using\", \" pointers\", \" in\", \" c\", \" programming\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" a\", \" problem\", \" with\", \" pointers\", \" in\", \" c\", \" programming\", \".\", \" I\", \" would\", \" love\", \" to\", \" get\", \" variables\", \" from\", \" the\", \" command\", \" line\", \" in\", \" Linux\", \" and\", \" than\", \" be\", \" assigning\", \" the\", \" value\", \" of\", \" the\", \" variables\", \" from\", \" command\", \" line\", \" to\", \" a\", \" non\", \"\\n\", \"0\", \" minutes\", \" after\", \" 12\", \":\", \"38\", \" PM\", \"?\", \"\\\\newline\", \"11\", \":\", \"48\", \" PM\", \"\\\\newline\", \"How\", \" many\", \" minutes\", \" are\", \" there\", \" between\", \" 1\", \":\", \"09\", \" AM\", \" and\", \" 1\", \":\", \"48\", \" AM\", \"?\", \"\\n\", \"He\", \"ather\", \"'s\", \" turn\", \" By\", \" nitro\", \"fi\", \"ren\", \"itro\", \"fire\", \" Watch\", \"\\\\newline\", \"\\\\newline\", \"53\", \" F\", \"avour\", \"ites\", \" 0\", \" Comments\", \" 3\", \"K\", \" Views\", \"\\\\newline\", \"\\\\newline\", \"\\\"\", \"Oh\", \" and\", \" I\", \"'m\", \" sure\", \" this\", \" whole\", \" time\", \" you\", \"'ve\", \" been\", \" away\", \" at\", \" college\", \" you\", \" have\", \" not\", \" gained\", \" a\", \" pound\", \" either\", \".\\\"\", \" Amanda\", \" j\", \"oked\", \" on\", \" the\", \" other\", \" end\", \" of\", \" the\", \" line\", \".\", \"\\\\newline\\\\newline\\\\newline\", \"\\n\", \"All\", \" relevant\", \" data\", \" are\", \" within\", \" the\", \" paper\", \" and\", \" its\", \" Supporting\", \" Information\", \" files\", \".\", \"\\\\newline\", \"\\\\newline\", \"Introduction\", \" {#\", \"sec\", \"001\", \"}\", \"\\\\newline\", \"============\", \"\\\\newline\", \"\\\\newline\", \"E\", \"cosystem\", \" services\", \" are\", \" defined\", \" as\", \" the\", \" benefits\", \" that\", \" nature\", \" provides\", \" to\", \" society\", \" \\\\[[@\", \"pone\", \".\", \"023\", \"53\", \"20\", \".\", \"ref\", \"\\n\", \"Summer\", \" of\", \" R\", \"ift\", \" is\", \" all\", \" about\", \" celebrating\", \" the\", \" VR\", \" community\", \" and\", \" encouraging\", \" new\", \" people\", \" to\", \" get\", \" in\", \" the\", \" game\", \".\", \" Today\", \",\", \" we\", \"\\u2019\", \"re\", \" excited\", \" to\", \" announce\", \" that\", \" R\", \"ift\", \" +\", \" Touch\", \" are\", \" now\", \" available\", \" at\", \" $\", \"399\", \" for\", \" a\", \" limited\", \"\\n\", \"Stone\", \" fl\", \"aming\", \"\\\\newline\", \"\\\\newline\", \"Stone\", \" fl\", \"aming\", \" or\", \" therm\", \"aling\", \" is\", \" the\", \" application\", \" of\", \" high\", \" temperature\", \" to\", \" the\", \" surface\", \" of\", \" stone\", \" to\", \" make\", \" it\", \" look\", \" like\", \" natural\", \" weather\", \"ing\", \".\", \"  \", \"\\n\", \"Mat\", \"hes\", \"on\", \" No\", \" Long\", \"er\", \" An\", \" Option\", \" For\", \" \\u201c\", \"Old\", \" Joe\", \"\\u201d\", \"\\\\newline\", \"\\\\newline\", \"After\", \" months\", \" and\", \" months\", \" of\", \" negotiations\", \",\", \" Al\", \"ach\", \"ua\", \" County\", \" is\", \" no\", \" longer\", \" considering\", \" moving\", \" the\", \" conf\", \"ed\", \"erate\", \" statue\", \" to\", \" the\", \" Mat\", \"hes\", \"on\", \" History\", \" Museum\", \".\", \"\\\\newline\", \"\\n\", \"WN\", \"CO\", \" (\", \"AM\", \")\", \"\\\\newline\", \"\\\\newline\", \"WN\", \"CO\", \" (\", \"13\", \"40\", \" AM\", \")\", \"  \", \"\\u2014\", \" branded\", \" Fox\", \" Sports\", \" 13\", \"40\", \" \\u2014\", \" is\", \" a\", \" commercial\", \" radio\", \" station\", \" licensed\", \" to\", \" Ash\", \"land\", \",\", \" Ohio\", \".\", \" The\", \" station\", \" serves\", \" the\", \" Ash\", \"land\", \",\", \" Mans\", \"field\", \"\\n\"], \"activations\": [[[0.06580144166946411]], [[0.012370109558105469]], [[-0.013183355331420898]], [[-0.013183355331420898]], [[0.021126151084899902]], [[0.0028634071350097656]], [[-0.016658663749694824]], [[-0.14209848642349243]], [[-0.1877974271774292]], [[-0.1727192997932434]], [[0.04087120294570923]], [[-0.8654499053955078]], [[0.0]], [[-0.005083799362182617]], [[0.021232545375823975]], [[-0.037627577781677246]], [[0.009242057800292969]], [[0.017258405685424805]], [[-0.02927762269973755]], [[0.022286534309387207]], [[-0.04848945140838623]], [[-0.10221236944198608]], [[-0.13623559474945068]], [[-0.07823145389556885]], [[-0.016539573669433594]], [[0.0058089494705200195]], [[-0.01047128438949585]], [[0.001409292221069336]], [[-0.015543103218078613]], [[-0.043348848819732666]], [[0.0034161806106567383]], [[0.05301380157470703]], [[-0.012321114540100098]], [[-0.015693485736846924]], [[-0.0446169376373291]], [[0.0030143260955810547]], [[-0.021850168704986572]], [[-0.002792060375213623]], [[-0.08817249536514282]], [[-0.02916741371154785]], [[-0.02916741371154785]], [[-0.04362225532531738]], [[-0.038502275943756104]], [[-0.03160184621810913]], [[-0.007354140281677246]], [[-0.05665457248687744]], [[-0.02080821990966797]], [[-0.013129830360412598]], [[-0.04850780963897705]], [[0.02779853343963623]], [[0.011671781539916992]], [[0.01232290267944336]], [[-0.029892444610595703]], [[-0.019718170166015625]], [[-0.6961098909378052]], [[0.0]], [[0.11730796098709106]], [[0.03735244274139404]], [[-0.07887536287307739]], [[0.004362523555755615]], [[0.004362523555755615]], [[0.005316972732543945]], [[0.014696061611175537]], [[0.0014297962188720703]], [[0.014930248260498047]], [[-0.00882500410079956]], [[0.00791996717453003]], [[0.00791996717453003]], [[-0.032750487327575684]], [[-0.01477581262588501]], [[-0.014721572399139404]], [[-0.021706759929656982]], [[-0.01119452714920044]], [[-0.013007104396820068]], [[-0.04136770963668823]], [[0.00041222572326660156]], [[-0.03257089853286743]], [[0.0008975863456726074]], [[0.00626450777053833]], [[0.0024856925010681152]], [[-0.01726377010345459]], [[0.004821062088012695]], [[-0.01919722557067871]], [[-0.02425515651702881]], [[0.013540267944335938]], [[-0.08321893215179443]], [[-0.05697590112686157]], [[-0.6591617465019226]], [[-0.6591617465019226]], [[0.0]], [[-0.06769555807113647]], [[-0.010075092315673828]], [[-0.0012165307998657227]], [[-0.0012165307998657227]], [[-0.00916522741317749]], [[0.027857959270477295]], [[-0.01650106906890869]], [[-0.006662964820861816]], [[-0.015797972679138184]], [[-0.012255370616912842]], [[-0.012255370616912842]], [[-0.01612788438796997]], [[-0.006656050682067871]], [[-0.01766073703765869]], [[-0.013486981391906738]], [[-0.016536831855773926]], [[0.01877439022064209]], [[-0.010920166969299316]], [[0.004303395748138428]], [[-0.028055548667907715]], [[-0.01832890510559082]], [[-0.017511725425720215]], [[-0.006959736347198486]], [[-0.0375826358795166]], [[-0.014733552932739258]], [[-0.004717469215393066]], [[0.01534658670425415]], [[-0.014515459537506104]], [[-0.020756423473358154]], [[0.010500133037567139]], [[0.005789518356323242]], [[-0.013713598251342773]], [[-0.010347247123718262]], [[-0.007744550704956055]], [[-0.0011713504791259766]], [[0.016282081604003906]], [[-0.03350621461868286]], [[0.018370389938354492]], [[0.06962579488754272]], [[-0.04300034046173096]], [[-0.00973445177078247]], [[0.058275699615478516]], [[0.002738356590270996]], [[-0.1130107045173645]], [[-0.1331029236316681]], [[-0.006981253623962402]], [[-0.19019454717636108]], [[-0.5612106323242188]], [[0.0]], [[0.031036972999572754]], [[0.04457196593284607]], [[0.019318461418151855]], [[0.01194906234741211]], [[0.020666539669036865]], [[-0.010033965110778809]], [[-0.04657095670700073]], [[-0.02071934938430786]], [[0.048656463623046875]], [[-0.001353621482849121]], [[-0.0005480051040649414]], [[-0.004034668207168579]], [[-0.033088743686676025]], [[0.020345091819763184]], [[-0.09365370869636536]], [[0.053386539220809937]], [[-0.06598210334777832]], [[0.01643320918083191]], [[0.009565412998199463]], [[-0.07244279980659485]], [[0.009059369564056396]], [[0.020399034023284912]], [[-0.016982465982437134]], [[-0.03518033027648926]], [[-0.054560184478759766]], [[0.021718591451644897]], [[-0.1293278932571411]], [[-0.021565526723861694]], [[-0.09951275587081909]], [[-0.4337695240974426]], [[0.0]], [[-0.0029428303241729736]], [[-0.010620951652526855]], [[-0.033825963735580444]], [[-0.011650800704956055]], [[0.004134237766265869]], [[-0.005704045295715332]], [[-0.009198218584060669]], [[-0.011774778366088867]], [[-0.024438917636871338]], [[-0.011456042528152466]], [[0.022806137800216675]], [[-0.003375232219696045]], [[-0.003375232219696045]], [[0.0035042762756347656]], [[0.016358494758605957]], [[-0.22736956179141998]], [[0.008121907711029053]], [[0.022525131702423096]], [[-0.09019830822944641]], [[0.015837490558624268]], [[-0.032573550939559937]], [[-0.27638617157936096]], [[-0.05358231067657471]], [[-0.05358231067657471]], [[0.018812119960784912]], [[-0.003292262554168701]], [[0.010587990283966064]], [[0.012168347835540771]], [[0.016129791736602783]], [[0.014938950538635254]], [[0.02975490689277649]], [[0.02515929937362671]], [[0.005566239356994629]], [[0.028790593147277832]], [[0.010396361351013184]], [[0.013958513736724854]], [[-6.946921348571777e-05]], [[0.0063634514808654785]], [[0.00810086727142334]], [[0.021390825510025024]], [[0.011834710836410522]], [[0.0051310062408447266]], [[0.014144808053970337]], [[0.00024703145027160645]], [[0.0055387020111083984]], [[0.0024345219135284424]], [[-0.033388763666152954]], [[0.07864576578140259]], [[0.03537416458129883]], [[-0.01421976089477539]], [[-0.015819698572158813]], [[0.007280945777893066]], [[-0.023195236921310425]], [[0.08503681421279907]], [[0.006500333547592163]], [[0.01767122745513916]], [[-0.036808133125305176]], [[-0.34881138801574707]], [[-0.34881138801574707]], [[0.0]], [[-0.015457600355148315]], [[-0.004975467920303345]], [[-0.007151156663894653]], [[-0.008081287145614624]], [[0.0017102062702178955]], [[-0.013769030570983887]], [[-0.011603206396102905]], [[-0.0027115046977996826]], [[-0.004703134298324585]], [[-0.025305569171905518]], [[-0.020223766565322876]], [[-0.026148736476898193]], [[0.011452019214630127]], [[0.024117857217788696]], [[0.024117857217788696]], [[0.018434107303619385]], [[-0.06305506825447083]], [[0.05450397729873657]], [[0.024245232343673706]], [[0.008570462465286255]], [[0.013950496912002563]], [[-0.005318939685821533]], [[-0.012773901224136353]], [[-0.012773901224136353]], [[0.023648470640182495]], [[-0.023845762014389038]], [[-0.018169492483139038]], [[-0.01840171217918396]], [[0.021453410387039185]], [[0.011858314275741577]], [[0.039187878370285034]], [[0.03992900252342224]], [[0.04161563515663147]], [[-0.003251194953918457]], [[0.03779870271682739]], [[0.01902177929878235]], [[0.027704060077667236]], [[-0.30785003304481506]], [[-0.23583239316940308]], [[0.13889560103416443]], [[-0.10650235414505005]], [[-0.1246899664402008]], [[-0.13981664180755615]], [[-0.08844754099845886]], [[-0.30785003304481506]], [[0.0]], [[-0.052925482392311096]], [[-0.03203773498535156]], [[0.019383907318115234]], [[0.0031950920820236206]], [[0.006913363933563232]], [[0.0008212774991989136]], [[0.013216406106948853]], [[-0.02917875349521637]], [[-0.006352543830871582]], [[-0.007321715354919434]], [[-0.009310901165008545]], [[-0.008093610405921936]], [[-0.02292659878730774]], [[-0.0063336193561553955]], [[0.0032489001750946045]], [[-0.006171107292175293]], [[-0.008611291646957397]], [[0.006980448961257935]], [[-0.00905492901802063]], [[-0.064077228307724]], [[-0.04421025514602661]], [[-0.03527897596359253]], [[-0.021783560514450073]], [[0.011250972747802734]], [[-0.01061674952507019]], [[0.0010725855827331543]], [[-0.04053260385990143]], [[0.0021037757396698]], [[-0.057016193866729736]], [[0.005738973617553711]], [[0.002008184790611267]], [[-0.0013212859630584717]], [[-0.0019562095403671265]], [[-0.043331071734428406]], [[-0.05652956664562225]], [[-0.029265522956848145]], [[-0.05088892579078674]], [[-0.08334232121706009]], [[0.016597986221313477]], [[-0.0067525506019592285]], [[-0.04906758666038513]], [[0.2301211655139923]], [[-0.2049499750137329]], [[0.0]], [[-0.0034050866961479187]], [[-0.03494131565093994]], [[0.05797514319419861]], [[-0.0611453652381897]], [[-0.0611453652381897]], [[0.022852249443531036]], [[-0.002766907215118408]], [[0.010408997535705566]], [[0.027403146028518677]], [[-0.012627124786376953]], [[0.013446792960166931]], [[-0.0014002621173858643]], [[-0.004769444465637207]], [[0.03240750730037689]], [[0.020600177347660065]], [[-0.00432276725769043]], [[-0.037579163908958435]], [[-0.008756227791309357]], [[0.007272012531757355]], [[-0.024724438786506653]], [[-0.012471005320549011]], [[0.019482702016830444]], [[-0.016896933317184448]], [[-0.0017508268356323242]], [[-0.007273316383361816]], [[-0.013292886316776276]], [[0.006279200315475464]], [[0.024709045886993408]], [[-0.03675469011068344]], [[-0.011030986905097961]], [[-0.10151100158691406]], [[-0.10151100158691406]], [[0.0]], [[0.0010017827153205872]], [[-2.492964267730713e-05]], [[-0.0002383887767791748]], [[-0.0027091652154922485]], [[-0.005090758204460144]], [[0.003498159348964691]], [[0.004524752497673035]], [[0.02783845365047455]], [[0.004825621843338013]], [[0.004093363881111145]], [[-0.02146580070257187]], [[0.011987850069999695]], [[-0.042941752821207047]], [[-0.08060075342655182]], [[-0.08060075342655182]], [[-0.009133711457252502]], [[0.026128701865673065]], [[0.02476615458726883]], [[0.008978769183158875]], [[0.008640721440315247]], [[-0.028354626148939133]], [[0.003515183925628662]], [[0.01423141360282898]], [[0.00781811773777008]], [[0.008375652134418488]], [[0.04932044446468353]], [[-0.04367687553167343]], [[0.004097595810890198]], [[-0.010035917162895203]], [[-0.002542927861213684]], [[-0.005569927394390106]], [[-0.02758949249982834]], [[0.012575618922710419]], [[-0.007131204009056091]], [[-0.00774761289358139]], [[0.04715713858604431]], [[-0.041293732821941376]], [[-0.05467451363801956]], [[-0.022334296256303787]], [[-0.022312097251415253]], [[-0.0076062679290771484]], [[0.02442203462123871]], [[-0.08060075342655182]], [[-0.08060075342655182]], [[-0.08060075342655182]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fec9475da20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_text(text_list, best_feature, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LISTADenoisingSAE' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ablate_feature_direction_display(full_text, best_feature)\n",
      "Cell \u001b[0;32mIn[6], line 172\u001b[0m, in \u001b[0;36mablate_feature_direction_display\u001b[0;34m(text, features, setting, verbose)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    171\u001b[0m     original_logits \u001b[39m=\u001b[39m model(tokens)\u001b[39m.\u001b[39mlog_softmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m--> 172\u001b[0m     ablated_logits \u001b[39m=\u001b[39m ablate_feature_direction(tokens, features, model, smaller_auto_encoder)\u001b[39m.\u001b[39mlog_softmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    173\u001b[0m diff_logits \u001b[39m=\u001b[39m ablated_logits  \u001b[39m-\u001b[39m original_logits\u001b[39m# ablated > original -> negative diff\u001b[39;00m\n\u001b[1;32m    174\u001b[0m tokens \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mcpu()\n",
      "Cell \u001b[0;32mIn[6], line 138\u001b[0m, in \u001b[0;36mablate_feature_direction\u001b[0;34m(tokens, feature, model, autoencoder)\u001b[0m\n\u001b[1;32m    135\u001b[0m     value \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m feature_direction\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m--> 138\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mrun_with_hooks(tokens, \n\u001b[1;32m    139\u001b[0m     fwd_hooks\u001b[39m=\u001b[39;49m[(\n\u001b[1;32m    140\u001b[0m         cache_name, \n\u001b[1;32m    141\u001b[0m         mlp_ablation_hook\n\u001b[1;32m    142\u001b[0m         )]\n\u001b[1;32m    143\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/transformer_lens/hook_points.py:355\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    349\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    353\u001b[0m     fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts\n\u001b[1;32m    354\u001b[0m ) \u001b[39mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mreturn\u001b[39;00m hooked_model\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/transformer_lens/components.py:944\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    940\u001b[0m     normalized_resid_pre_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(resid_pre)\n\u001b[1;32m    941\u001b[0m     mlp_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_mlp_out(\n\u001b[1;32m    942\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(normalized_resid_pre_2)\n\u001b[1;32m    943\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[0;32m--> 944\u001b[0m     resid_post \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_post(\n\u001b[1;32m    945\u001b[0m         resid_pre \u001b[39m+\u001b[39;49m attn_out \u001b[39m+\u001b[39;49m mlp_out\n\u001b[1;32m    946\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m     resid_post \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_post(\n\u001b[1;32m    949\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m    950\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/torch/nn/modules/module.py:1547\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1546\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1547\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[1;32m   1549\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/logan/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 128\u001b[0m, in \u001b[0;36mablate_feature_direction.<locals>.mlp_ablation_hook\u001b[0;34m(value, hook)\u001b[0m\n\u001b[1;32m    125\u001b[0m int_val \u001b[39m=\u001b[39m rearrange(value, \u001b[39m'\u001b[39m\u001b[39mb s h -> (b s) h\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[39m# Run through the autoencoder\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m _, act \u001b[39m=\u001b[39m autoencoder(int_val)\n\u001b[1;32m    129\u001b[0m feature_to_ablate \u001b[39m=\u001b[39m feature \u001b[39m# TODO: bring this out of the function\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m# Subtract value with feature direction*act_of_feature\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LISTADenoisingSAE' object is not callable"
     ]
    }
   ],
   "source": [
    "ablate_feature_direction_display(full_text, best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_lens(model,best_feature, smaller_dict, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_text = [\n",
    "    \"I can count up to: 2 4 8 16 32 64 128 256 512 1024 2048 4096 8192 16384 32768 7 6 12 16 18 20 22 24\",\n",
    "]\n",
    "visualize_text(custom_text, best_feature, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Centric Viewpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through datapoints & see if the features that activate on them make sense.\n",
    "d_point = 0\n",
    "# text = tokens_dataset[d_point]\n",
    "data_ind, sequence_pos = np.unravel_index(d_point, (datapoints, token_amount))\n",
    "feature_val, feature_ind = dictionary_activations[d_point].topk(10)\n",
    "data_ind = int(data_ind)\n",
    "sequence_pos = int(sequence_pos)\n",
    "full_tok = torch.tensor(dataset[data_ind][\"input_ids\"])\n",
    "full_text = []\n",
    "full_text.append(model.tokenizer.decode(full_tok))\n",
    "visualize_text(full_text, feature_ind, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the neuron/residual basis\n",
    "When we look at the weights of a feature, we are seeing the literal dimensions from the residual stream/neurons being read from the feature. \n",
    "\n",
    "Here I'm visualizing the weight values for the residual stream. If there are outliers, then it's mainly reading from that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(weights*max_activation).topk(20), (weights*max_activation).topk(20, largest=False).values, (weights*max_activation > 0.2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepend/Append tokens\n",
    "We can iterate over all tokens to check which ones activate a feature a lot to more rigorously test a hypothesis on what a feature means.\n",
    "\n",
    "Note: I'm literately running the model through all 50k tokens prepended to the text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_all_tokens_and_get_feature_activation(model, minimal_activating_example, feature, setting=\"prepend\"):\n",
    "    tokens = model.to_tokens(minimal_activating_example, prepend_bos=False)\n",
    "\n",
    "    # Run through every number up to vocab size\n",
    "    vocab_size = model.cfg.d_vocab\n",
    "    batch_size = 256*2 # Define your desired batch size\n",
    "\n",
    "    dollar_feature_activations = torch.zeros(vocab_size)\n",
    "    for start in range(0, vocab_size, batch_size):\n",
    "        end = min(start + batch_size, vocab_size)\n",
    "\n",
    "        token_prep = torch.arange(start, end).to(device)\n",
    "        token_prep = token_prep.unsqueeze(1)  # Add a dimension for concatenation\n",
    "\n",
    "        # 1. Prepend to the tokens\n",
    "        if setting == \"prepend\":\n",
    "            tokens_catted = torch.cat((token_prep, tokens.repeat(end - start, 1)), dim=1).long()\n",
    "        elif setting == \"append\":\n",
    "            tokens_catted = torch.cat((tokens.repeat(end - start, 1), token_prep), dim=1).long()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown setting: {setting}\")\n",
    "\n",
    "        # 2. Run through the model\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens_catted.to(device))\n",
    "            neuron_act_batch = cache[cache_name]\n",
    "            _, act = smaller_auto_encoder(neuron_act_batch)\n",
    "\n",
    "        # 3. Get the feature\n",
    "        dollar_feature_activations[start:end] = act[:, -1, feature].cpu().squeeze()\n",
    "\n",
    "    k = 20\n",
    "    k_increasing_val, k_increasing_ind = dollar_feature_activations.topk(k)\n",
    "    k_decreasing_val, k_decreasing_ind = dollar_feature_activations.topk(k, largest=False)\n",
    "    if(setting == \"prepend\"):\n",
    "        print(f\"[token]{minimal_activating_example}\")\n",
    "    elif(setting == \"append\"):\n",
    "        print(f\"{minimal_activating_example}[token]\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown setting: {setting}\")\n",
    "    # Print indices converted to tokens\n",
    "    print(f\"Top-{k} increasing: {model.to_str_tokens(k_increasing_ind)}\")\n",
    "    # Print values\n",
    "    print(f\"Top-{k} increasing: {[f'{val:.2f}' for val in k_increasing_val]}\")\n",
    "    print(f\"Top-{k} decreasing: {model.to_str_tokens(k_decreasing_ind)}\")\n",
    "    print(f\"Top-{k} decreasing: {[f'{val:.2f}' for val in k_decreasing_val]}\")\n",
    "    print(f\"Number of 0 activations: {torch.sum(dollar_feature_activations == 0)}\")\n",
    "    if(setting == \"prepend\"):\n",
    "        best_text = \"\".join(model.to_str_tokens(dollar_feature_activations.argmax()) + [minimal_activating_example])\n",
    "    else:\n",
    "        best_text = \"\".join([minimal_activating_example] + model.to_str_tokens(dollar_feature_activations.argmax()))\n",
    "    return best_text\n",
    "\n",
    "best_text = \"\"\n",
    "for x in range(3):\n",
    "    # best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" for all $\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_text = \"\"\n",
    "for x in range(3):\n",
    "    best_text = prepend_all_tokens_and_get_feature_activation(model, best_text, best_feature, setting=\"prepend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepend_all_tokens_and_get_feature_activation(model, \" for all $\", best_feature, setting=\"prepend\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \"The\", best_feature, setting=\"append\")\n",
    "# prepend_all_tokens_and_get_feature_activation(model, \" tree\", best_feature, setting=\"append\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
