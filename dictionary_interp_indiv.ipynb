{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"Elriggs/pythia-70m-deduped\"\n",
    "\n",
    "# The layer around which we want to interpret by looking at the last layer\n",
    "layer_focus = 1\n",
    "assert layer_focus > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 9.999999747378752e-05, 0.00019306977628730237, 0.000372759357560426, 0.0007196856895461679, 0.0013894954463467002, 0.0026826958637684584, 0.005179474595934153, 0.009999999776482582]\n",
      "{'dict_size': 3072, 'l1_alpha': 0.0013894954463467002}\n",
      "[0.0, 9.999999747378752e-05, 0.00019306977628730237, 0.000372759357560426, 0.0007196856895461679, 0.0013894954463467002, 0.0026826958637684584, 0.005179474595934153, 0.009999999776482582]\n",
      "{'dict_size': 3072, 'l1_alpha': 0.0013894954463467002}\n"
     ]
    }
   ],
   "source": [
    "def load_autoencoder(layer: int):\n",
    "\tae_download_location_main = hf_hub_download(repo_id=model_id, filename=f\"tied_residual_l{layer_focus}_r6/_63/learned_dicts.pt\")\n",
    "\tall_autoencoders = torch.load(ae_download_location_main)\n",
    "\tall_l1s = [hyperparams[\"l1_alpha\"] for autoencoder, hyperparams in all_autoencoders]\n",
    "\t# TODO: choose best one???\n",
    "\tprint(all_l1s)\n",
    "\tauto_num = 5\n",
    "\tautoencoder, hyperparams = all_autoencoders[auto_num]\n",
    "\t# You want a hyperparam around 1e-3. Higher is less features/datapoint (at the cost of reconstruction error); lower is more features/datapoint (at the cost of polysemanticity)\n",
    "\tautoencoder.to_device(device)\n",
    "\tprint(hyperparams)\n",
    "\treturn autoencoder, hyperparams\n",
    "\n",
    "autoencoder_main, hyperparams_main = load_autoencoder(layer_focus)\n",
    "autoencoder_prior, hyperparams_prior = load_autoencoder(layer_focus - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "def get_cache_name_neurons(layer: int):\n",
    "    if setting == \"residual\":\n",
    "        cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp\":\n",
    "        cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "        neurons = model.cfg.d_mlp\n",
    "    elif setting == \"attention\":\n",
    "        cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp_out\":\n",
    "        cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return cache_name, neurons\n",
    "\n",
    "cache_name, neurons  = get_cache_name_neurons(layer_focus)\n",
    "cache_name_prior, _  = get_cache_name_neurons(layer_focus - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-33be347e89413cee.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f6085a72c3f9dd17.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-904da2d039ac9ca3.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "# dataset_name = \"NeelNanda/pile-10k\"\n",
    "# dataset_name = \"NeelNanda/pile-10k\"\n",
    "dataset_name = \"JeanKaddour/minipile\"\n",
    "token_amount= 40\n",
    "#TODO: change train[:1000] to train if you want whole dataset\n",
    "# 100_000 datasets\n",
    "dataset = load_dataset(dataset_name, split=\"train[:100000]\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")\n",
    "# TODO: we can maybe make this faster for the larger dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf6399274f54c52a84c5e1154c99ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: in chunks...\n",
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "# MAX_CHUNK_SIZE = 1_000\n",
    "\n",
    "def get_activations(autoencoder):\n",
    "    num_features, d_model = autoencoder.encoder.shape\n",
    "    datapoints = dataset.num_rows\n",
    "    dictionary_activations_final = np.memmap('dict.mymemmap', dtype='float32', mode='w+', shape=(datapoints * token_amount, num_features))\n",
    "    # neuron_activations = torch.zeros((datapoints*token_amount, d_model))\n",
    "    neuron_activations = np.memmap('neur.mymemmap', dtype='float32', mode='w+', shape=(datapoints * token_amount, d_model))\n",
    "    batch_size = 32\n",
    "    # dictionary_activations = np.zeros((datapoints*token_amount, num_features))\n",
    "    smaller_auto_encoder = autoencoder\n",
    "    smaller_auto_encoder.to_device(device)\n",
    "\n",
    "    # print(\"Creating data dict\")\n",
    "    # d = DatasetDict(Dataset(dataset[ind_low:ind_max]))\n",
    "    # print(\"Got data dict\")\n",
    "    # with torch.no_grad(), d.formatted_as(\"pt\"):\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            # print(batch)\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "            neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "            batched_dictionary_activations = smaller_auto_encoder.encode(batched_neuron_activations)\n",
    "            dictionary_activations_final[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu().numpy()\n",
    "    del neuron_activations\n",
    "        # dictionary_activations_final[ind_low:ind_max] = dictionary_activations\n",
    "        # chunks.append(dictionary_activations)\n",
    "    return dictionary_activations_final\n",
    "\n",
    "dict_activations_main = get_activations(autoencoder_main)\n",
    "dict_activations_prior = get_activations(autoencoder_prior)\n",
    "# TODO: I think that we may want to save this to disk for the future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interp\n",
    "Investigate the example sentences the activate this feature.\n",
    "\n",
    "Max: show max activating (tokens,contexts)\n",
    "\n",
    "Uniform: Show range of activations from each bin (e.g. sample an example from 1-2, 2-3, etc). \n",
    "[Note: if a feature is monosemantic, then the full range of activations should be that feature, not just max-activating ones]\n",
    "\n",
    "Full_text: shows the full text example\n",
    "\n",
    "Text_list: shows up to the most activating example (try w/ max activating on a couple of examples to see)\n",
    "\n",
    "ablate_text: remove the context one token at a time, and show the decrease/increase in activation of that feature\n",
    "\n",
    "ablate_feature_direction: removes feature direction from model's activation mid-inference, showing the logit diff in the output for every token.\n",
    "\n",
    "logit_lens: show the logit lens for that feature. If matches ablate_feature_direction, then the computation path is through the residual stream, else, it's through future layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39320, 3072])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_activations_prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints_with_idx(feature_index, dictionary_activations, tokenizer, token_amount, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        # min_value = torch.min(best_feature_activations)\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            if(bin_idx==0): # Skip the first one. This is below the median\n",
    "                continue\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    num_datapoints = int(dictionary_activations.shape[0]/token_amount)\n",
    "    datapoint_indices =[np.unravel_index(i, (num_datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list, found_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Index tensor([ 6738, 38178, 14329, 31726, 31759, 15716, 18798, 25789, 25622, 35054])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-30363543-a90c\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-30363543-a90c\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"R\", \"ent\", \"z\", \" R\", \"Vs\", \" Inc\", \".\", \" (\", \"RR\", \"V\", \")\", \"\\\\newline\", \"\\\\newline\", \"1\", \".\", \" A\", \" stock\", \" is\", \" expected\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"Un\", \"precedented\", \"ly\", \" precise\", \" cosmic\", \" microwave\", \" background\", \" (\", \"CM\", \"B\", \")\", \" data\", \" are\", \" expected\", \"\\n\", \"Visual\", \" attention\", \" to\", \" features\", \" by\", \" associative\", \" learning\", \".\", \"\\\\newline\", \"Expect\", \"\\n\", \"General\", \" Election\", \" 10\", \" generally\", \" went\", \" the\", \" predicted\", \"\\n\", \"General\", \" Election\", \" 10\", \" generally\", \" went\", \" the\", \" predicted\", \" way\", \",\", \" albeit\", \" for\", \" a\", \" few\", \" surprises\", \" such\", \" as\", \" Labour\", \" not\", \" being\", \" dec\", \"imated\", \" even\", \" after\", \" a\", \" major\", \" scandal\", \" /\", \" poor\", \" term\", \",\", \" and\", \" New\", \" Britain\", \"\\u2019\", \"s\", \" electoral\", \" failure\", \".\", \" As\", \" predicted\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"In\", \" state\", \" space\", \" models\", \",\", \" smoothing\", \" refers\", \" to\", \" the\", \" task\", \" of\", \" estimating\", \" a\", \" latent\", \" stochastic\", \" process\", \" given\", \" noisy\", \" measurements\", \" related\", \" to\", \" the\", \" process\", \".\", \" We\", \" propose\", \" an\", \" unbiased\", \" estimator\", \" of\", \" smoothing\", \" expectations\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" measure\", \" g\", \" using\", \" a\", \" met\", \"re\", \" stick\", \" and\", \" a\", \" ball\", \"\\\\newline\", \"\\\\newline\", \"Can\", \" I\", \" measure\", \" the\", \" value\", \" of\", \" g\", \" using\", \" only\", \" a\", \" met\", \"re\", \" stick\", \" and\", \" a\", \" ball\", \"?\", \" I\", \" am\", \" not\", \" supposed\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" do\", \" I\", \" access\", \" an\", \" instance\", \" of\", \" a\", \" class\", \" that\", \" is\", \" inside\", \" an\", \" array\", \"List\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" doing\", \" a\", \" MO\", \"OC\", \" and\", \" am\", \" supposed\", \"\\n\", \"Hy\", \"unda\", \"i\", \" Motor\", \" Co\", \".\", \" officially\", \" launched\", \" Santa\", \" Fe\", \" in\", \" New\", \" York\", \" Auto\", \" Show\", \" 2012\", \" some\", \" time\", \" ago\", \".\", \" Santa\", \" Fe\", \" planned\", \"\\n\", \"2020\", \" Australian\", \" S\", \"5000\", \" Championship\", \"\\\\newline\", \"\\\\newline\", \"The\", \" 2020\", \" Australian\", \" S\", \"5000\", \" Championship\", \" is\", \" planned\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.053551435470581]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9013288021087646]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5892302989959717]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6434235572814941]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6434228420257568]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5225279331207275]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.804863691329956]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.0896680355072021]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9648325443267822]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5490217208862305]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7815039157867432]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f14a7f02c50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from interp_utils import *\n",
    "# Can sort by MMCS w/ the larger dictionary\n",
    "# indexed_feature = 13\n",
    "# best_feature = int(max_indices[indexed_feature])\n",
    "# Or just random index \n",
    "feature_idx = 13\n",
    "\n",
    "text_list, full_text, token_list, full_token_list, sampled_indices = get_feature_datapoints_with_idx(feature_idx, dict_activations_prior, model.tokenizer, token_amount, dataset, setting=\"max\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(feature_idx, dict_activations_prior, model.tokenizer, token_amount, dataset, setting=\"max\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"max\")\n",
    "# visualize_text(full_text, best_feature, model, autoencoder, layer)\n",
    "print(\"Sampled Index\", sampled_indices)\n",
    "visualize_text(text_list, feature_idx, model, autoencoder_prior, layer_focus - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the **downstream** feature in the next layer that is most activated by random subset of this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1287)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1287, 1321, 2985,  621,  833, 2221, 1483, 1067,  189, 1926, 1104, 1002,\n",
       "          12,  271,  663, 1414, 1515, 1112, 1343, 1101])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_feature_idx = 12\n",
    "k = 100\n",
    "\n",
    "text_list, full_text, token_list, full_token_list, sampled_indices = get_feature_datapoints_with_idx(prior_feature_idx, dict_activations_prior, model.tokenizer, token_amount, dataset, setting=\"max\", k=k)\n",
    "\n",
    "# print(\"Sampled Index\", sampled_indices)\n",
    "# dict_activations_prior.shape, dict_activations_main.shape\n",
    "TOP_FEATURES = 20\n",
    "# We do not need absolute value as I think that everything is positive b/c ReLU\n",
    "summed = dict_activations_main[sampled_indices].sum(dim=0)\n",
    "summed_abs = summed.abs()\n",
    "print(summed.argmax())\n",
    "main_features = summed.argsort(descending=True)[:TOP_FEATURES]\n",
    "main_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-3c161634-e4d0\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-3c161634-e4d0\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"So\", \",\", \" now\", \" that\", \" the\", \" X\", \"oom\", \" has\", \" flo\", \"pped\", \",\", \" can\", \" we\", \" finally\", \" agree\", \" that\", \" \\\"\", \"table\", \"t\", \"\\\"\", \" means\", \" \\\"\", \"i\", \"Pad\", \"\\\"?\", \"\\\\newline\", \"\\\\newline\", \"Yeah\", \",\", \" I\", \" don\", \"'t\", \" see\", \" how\", \" the\", \" Android\", \" tablet\", \" makers\", \" are\", \" going\", \"\\n\", \"Very\", \" well\", \",\", \" this\", \" is\", \" not\", \" the\", \" first\", \" story\", \" that\", \" I\", \" write\", \" in\", \" English\", \",\", \" but\", \" is\", \" the\", \" first\", \" that\", \" is\", \" going\", \"\\n\", \"\\\\newline\", \"\\\\newline\", \"Z\", \"UR\", \"B\", \" T\", \"avern\", \" -\", \" j\", \"ac\", \"ob\", \"wg\", \"\\\\newline\", \"http\", \"://\", \"z\", \"urb\", \".\", \"com\", \"/\", \"ta\", \"vern\", \"\\\\newline\", \"\\\\newline\", \"======\", \"\\\\newline\", \"pe\", \"psi\", \"\\\\newline\", \"By\", \" the\", \" name\", \",\", \" I\", \" thought\", \" that\", \" this\", \" was\", \" going\", \"\\n\", \"I\", \"'m\", \" definitely\", \" going\", \"\\n\", \"\\\\newline\\\\newline\\\\newline\", \"\\\\newline\", \"I\", \" was\", \" talking\", \" to\", \" a\", \" Je\", \"hov\", \"ah\", \"\\u2019\", \"s\", \" Witness\", \" the\", \" other\", \" day\", \" and\", \" found\", \" out\", \" their\", \" idea\", \" of\", \" heaven\", \" is\", \" the\", \" same\", \" ut\", \"opia\", \" that\", \" liberals\", \" are\", \" trying\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Under\", \"standing\", \" difference\", \" in\", \" un\", \"ix\", \" epoch\", \" time\", \" via\", \" Python\", \" vs\", \".\", \" In\", \"flux\", \"DB\", \"\\\\newline\", \"\\\\newline\", \"I\", \"'ve\", \" been\", \" trying\", \"\\n\", \"Meet\", \" Dr\", \".\", \" Carol\", \" Ford\", \"\\\\newline\", \"\\\\newline\", \"Dr\", \".\", \" Carol\", \" Ford\", \" has\", \" been\", \" working\", \"\\n\", \"Monday\", \"'s\", \" are\", \" never\", \" fun\", \"...\", \"except\", \" when\", \" it\", \"'s\", \" West\", \" Ham\", \" Q\", \"&\", \"A\", \" day\", \" and\", \" that\", \"'s\", \" exactly\", \" what\", \" it\", \" is\", \" today\", \"!\", \"\\\\newline\", \"\\\\newline\", \"To\", \" say\", \" there\", \" is\", \" a\", \" lot\", \" going\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Binding\", \" value\", \" to\", \" select\", \" in\", \" angular\", \" js\", \" across\", \" 2\", \" controllers\", \"\\\\newline\", \"\\\\newline\", \"Working\", \" with\", \" angular\", \"JS\", \" I\", \" am\", \" trying\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Node\", \" JS\", \" Express\", \" Bo\", \"iler\", \"plate\", \" and\", \" rendering\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" trying\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02554967999458313]], [[0.26761674880981445]], [[0.05465430021286011]], [[0.05130976438522339]], [[0.2388601005077362]], [[0.0]], [[0.0]], [[0.14184293150901794]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.04043757915496826]], [[0.0]], [[0.004578545689582825]], [[0.11854834854602814]], [[0.1877664476633072]], [[0.04511850327253342]], [[0.676803469657898]], [[0.19483119249343872]], [[0.18430037796497345]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.10009695589542389]], [[0.30141106247901917]], [[0.09558391571044922]], [[0.0]], [[0.05119243264198303]], [[0.13252362608909607]], [[0.0]], [[0.0]], [[0.28978103399276733]], [[0.043035127222537994]], [[0.0]], [[0.0]], [[0.345833957195282]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2838343381881714]], [[0.0]], [[0.0]], [[0.12475376576185226]], [[0.0]], [[0.2518509328365326]], [[0.1967340111732483]], [[0.07866804301738739]], [[0.15882325172424316]], [[0.0]], [[0.0]], [[0.34050843119621277]], [[0.0]], [[0.060410276055336]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.1494164913892746]], [[0.3054475784301758]], [[0.34331655502319336]], [[0.1994335651397705]], [[0.1074039489030838]], [[0.07796728610992432]], [[0.07520181685686111]], [[0.2906854748725891]], [[0.19099321961402893]], [[0.0]], [[0.0]], [[0.0]], [[0.07711296528577805]], [[0.28588101267814636]], [[0.0]], [[0.0]], [[0.0]], [[0.24892905354499817]], [[0.11188550293445587]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.02786758542060852]], [[0.0]], [[0.05809599161148071]], [[0.0]], [[0.13911181688308716]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5439971685409546]], [[0.19443918764591217]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2025274634361267]], [[0.0]], [[0.0]], [[0.004082128405570984]], [[0.13834506273269653]], [[0.45895901322364807]], [[0.0]], [[0.0]], [[0.23232592642307281]], [[0.10612478107213974]], [[0.39732643961906433]], [[0.2903577387332916]], [[0.36871206760406494]], [[0.2922235131263733]], [[0.0]], [[0.0]], [[0.010274738073348999]], [[0.007827192544937134]], [[0.08136239647865295]], [[0.37780237197875977]], [[0.16612409055233002]], [[0.06253588944673538]], [[0.0]], [[0.0]], [[0.1329449564218521]], [[0.0]], [[0.08073699474334717]], [[0.19371846318244934]], [[0.221792533993721]], [[0.08408860862255096]], [[0.0]], [[0.02381289005279541]], [[0.0]], [[0.0]], [[0.0]], [[0.022257506847381592]], [[0.0]], [[0.09149882197380066]], [[0.37732216715812683]], [[0.1108819842338562]], [[0.0]], [[0.15145382285118103]], [[0.22093817591667175]], [[0.07324501872062683]], [[0.18726001679897308]], [[0.0]], [[0.0]], [[0.36775827407836914]], [[0.0]], [[0.10700720548629761]], [[0.0]], [[0.0]], [[0.0]], [[0.052030667662620544]], [[0.0]], [[0.0]], [[0.0]], [[0.45722126960754395]], [[0.4787807762622833]], [[0.0]], [[0.4103517532348633]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2666173279285431]], [[0.009658396244049072]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.12378402054309845]], [[0.0]], [[0.0]], [[0.0]], [[0.4100027084350586]], [[0.0]], [[0.0]], [[0.359311044216156]], [[0.0]], [[0.14958254992961884]], [[0.0]], [[0.1517811268568039]], [[0.45987668633461]], [[0.0]], [[0.0]], [[0.1828007698059082]], [[0.38892093300819397]], [[0.23277758061885834]], [[0.13503408432006836]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.14376387000083923]], [[0.0]], [[0.037742555141448975]], [[0.0]], [[0.0]], [[0.0]], [[0.02381289005279541]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.17161771655082703]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.23715418577194214]], [[0.27167007327079773]], [[0.0]], [[0.20821909606456757]], [[0.04311901330947876]], [[0.0]], [[0.0]], [[0.02381289005279541]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.15752428770065308]], [[0.0]], [[0.0]], [[0.0]], [[0.017752930521965027]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f14a2f39c90>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_text(text_list[:10], feature_idx, model, autoencoder_prior, layer_focus - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-67d259b4-5493\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-67d259b4-5493\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Using\", \" \\\"\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"apache\", \" rewrite\", \" rules\", \",\", \" non\", \"-\", \"www\", \",\", \" https\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" two\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Re\", \"act\", \" types\", \"cript\", \" ref\", \" return\", \" null\", \" in\", \" conditional\", \" rendering\", \"\\\\newline\", \"\\\\newline\", \"I\", \" want\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"jav\", \"afx\", \":\", \" How\", \" to\", \" bind\", \" the\", \" Enter\", \" key\", \" to\", \" a\", \" button\", \" and\", \" fire\", \" off\", \" an\", \" event\", \" when\", \" it\", \" is\", \" clicked\", \"?\", \"\\\\newline\", \"\\\\newline\", \"Basically\", \",\", \" I\", \" have\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"in\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Execute\", \" HTTP\", \" Post\", \" automatically\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" a\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"CM\", \"ake\", \" link\", \" directory\", \" passing\", \" when\", \" compiling\", \" shared\", \" library\", \"\\\\newline\", \"\\\\newline\", \"Say\", \"\\n\", \"Q\", \":\", \"\\\\newline\\\\newline\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" can\", \" I\", \" use\", \" a\", \" date\", \"picker\", \" to\", \" set\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"sql\", \" queries\", \" and\", \" inserts\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" a\", \" random\", \" question\", \".\", \" If\", \" I\", \" were\", \" to\", \" do\", \" a\", \" sql\", \" select\", \" and\", \" while\", \" the\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"bootstrap\", \".\", \"min\", \".\", \"css\", \" sets\", \" transparency\", \" where\", \" not\", \" wanted\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \" a\", \" small\", \" chat\", \"box\", \" at\", \" the\", \" bottom\", \" of\", \" my\", \" page\", \" which\", \" seems\", \" to\", \" be\", \" inher\", \"iting\", \" CSS\", \" style\", \" from\", \" bootstrap\", \".\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" do\", \" you\", \" use\", \" list\", \" properties\", \" in\", \" Google\", \" App\", \" Engine\", \" dat\", \"ast\", \"ore\", \" in\", \" Java\", \"?\", \"\\\\newline\", \"\\\\newline\", \"An\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Do\", \"ctrine\", \"2\", \" entity\", \" default\", \" value\", \" for\", \" Many\", \"To\", \"One\", \" relation\", \" property\", \"\\\\newline\", \"\\\\newline\", \"I\", \"'ve\", \" got\", \" a\", \" Do\", \"ctrine\", \"2\", \" Entity\", \" called\", \" \\\"\", \"Order\", \"\\\",\", \" which\", \" has\", \" several\", \" status\", \" properties\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Binding\", \" value\", \" to\", \" select\", \" in\", \" angular\", \" js\", \" across\", \" 2\", \" controllers\", \"\\\\newline\", \"\\\\newline\", \"Working\", \" with\", \" angular\", \"JS\", \" I\", \" am\", \" trying\", \" to\", \" figure\", \" out\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"high\", \"charts\", \" red\", \"raw\", \" and\", \" ref\", \"low\", \" not\", \" working\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" trying\", \" to\", \" build\", \" a\", \" dynamic\", \" page\", \" that\", \" has\", \" any\", \" number\", \" between\", \" 1\", \"-\", \"4\", \" graphs\", \" on\", \" it\", \" that\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Assembly\", \" in\", \" Visual\", \" Studio\", \" 2013\", \" not\", \" building\", \" even\", \" after\", \" enabling\", \" Microsoft\", \" Mac\", \"ro\", \" As\", \"sembl\", \"er\", \"\\\\newline\", \"\\\\newline\", \"I\", \"'m\", \" trying\", \" to\", \" run\", \" a\", \" pretty\", \" basic\", \" assembly\", \" file\", \" to\", \" do\", \" a\", \" little\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"A\", \" j\", \"apan\", \"ese\", \" saying\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Script\", \" para\", \" mud\", \"ar\", \" jog\", \"ador\", \" (\", \"j\", \"ogo\", \" da\", \" vel\", \"ha\", \" J\", \"query\", \")\", \"\\\\newline\", \"\\\\newline\", \"Est\", \"ou\", \" tent\", \"ando\", \" fazer\", \" com\", \" que\", \" o\", \" j\", \"ogo\", \" da\", \" vel\", \"ha\", \" m\", \"ude\", \" o\", \" jog\", \"\\n\", \"import\", \" {\", \" Component\", \",\", \" In\", \"ject\", \",\", \" Input\", \" }\", \" from\", \" '@\", \"angular\", \"/\", \"core\", \"';\", \"\\\\newline\", \"\\n\", \"<?\", \"xml\", \" version\", \"=\\\"\", \"1\", \".\", \"0\", \"\\\"\", \" encoding\", \"=\\\"\", \"utf\", \"-\", \"8\", \"\\\"?>\", \"\\\\newline\", \"<\", \"LinearLayout\", \" xmlns\", \":\", \"android\", \"=\\\"\", \"http\", \"://\", \"schemas\", \".\", \"android\", \".\", \"com\", \"/\", \"apk\", \"/\", \"res\", \"/\", \"android\", \"\\\"\", \"\\\\newline\", \"    \", \"android\", \":\", \"\\n\"], \"activations\": [[[0.5216711759567261]], [[0.7156170606613159]], [[2.33268404006958]], [[2.4600906372070312]], [[2.731034755706787]], [[3.9615793228149414]], [[0.0]], [[0.5216711759567261]], [[0.7156177759170532]], [[2.332686424255371]], [[2.460092544555664]], [[2.437053680419922]], [[2.3985166549682617]], [[0.9437233209609985]], [[1.592175841331482]], [[0.8436356782913208]], [[0.9514306783676147]], [[0.0]], [[0.0]], [[0.6684943437576294]], [[1.184705376625061]], [[1.7501929998397827]], [[3.132394790649414]], [[3.246692657470703]], [[3.5956649780273438]], [[0.0]], [[0.5216711759567261]], [[0.7156177759170532]], [[2.332686424255371]], [[2.460092544555664]], [[1.7811545133590698]], [[2.2010040283203125]], [[1.231418490409851]], [[1.6265028715133667]], [[1.5866152048110962]], [[2.1278600692749023]], [[1.3121579885482788]], [[2.3807730674743652]], [[1.8903127908706665]], [[1.455363154411316]], [[1.9590562582015991]], [[2.177196502685547]], [[3.5697574615478516]], [[3.5527896881103516]], [[0.0]], [[0.5216712951660156]], [[0.7156182527542114]], [[2.332686424255371]], [[2.460092067718506]], [[2.4152607917785645]], [[1.7709838151931763]], [[1.3838194608688354]], [[1.8671513795852661]], [[2.4086217880249023]], [[2.1649298667907715]], [[1.8326019048690796]], [[1.507545828819275]], [[1.4914182424545288]], [[1.5280107259750366]], [[1.5018233060836792]], [[1.677132487297058]], [[1.6356242895126343]], [[1.7986246347427368]], [[0.7780364751815796]], [[1.4106560945510864]], [[1.6854196786880493]], [[2.297163486480713]], [[1.824835181236267]], [[1.2287813425064087]], [[1.7833653688430786]], [[2.109121322631836]], [[2.590712070465088]], [[2.506927013397217]], [[2.594412326812744]], [[2.7159600257873535]], [[3.161616325378418]], [[3.244652271270752]], [[0.0]], [[0.5216711759567261]], [[0.7156170606613159]], [[2.33268404006958]], [[2.4600906372070312]], [[3.1079769134521484]], [[0.0]], [[0.521670937538147]], [[0.7156163454055786]], [[2.3326849937438965]], [[2.4600911140441895]], [[2.779423236846924]], [[1.4157763719558716]], [[1.6311713457107544]], [[0.7828782796859741]], [[1.8060802221298218]], [[2.306110382080078]], [[3.1730237007141113]], [[3.244378089904785]], [[2.7799668312072754]], [[0.0]], [[0.521670937538147]], [[0.7156170606613159]], [[2.3326845169067383]], [[2.4600911140441895]], [[2.1298794746398926]], [[1.444374918937683]], [[0.9678131341934204]], [[0.28487658500671387]], [[2.7333154678344727]], [[2.6000218391418457]], [[1.6314162015914917]], [[2.0202059745788574]], [[0.4036290645599365]], [[1.8498812913894653]], [[1.8951843976974487]], [[2.600341320037842]], [[0.0]], [[0.5216711759567261]], [[0.7156170606613159]], [[2.6616954803466797]], [[0.0]], [[0.521670937538147]], [[0.7156163454055786]], [[2.3326849937438965]], [[2.4600911140441895]], [[2.6467370986938477]], [[3.232011318206787]], [[3.7726516723632812]], [[3.180783271789551]], [[2.8646860122680664]], [[1.7731298208236694]], [[1.738613247871399]], [[2.3314085006713867]], [[2.3107495307922363]], [[0.0]], [[0.5216712951660156]], [[0.7156182527542114]], [[2.332686424255371]], [[2.460092067718506]], [[2.3904953002929688]], [[2.128170967102051]], [[2.0779218673706055]], [[2.0527515411376953]], [[1.760677456855774]], [[2.2581849098205566]], [[3.357555389404297]], [[3.344757556915283]], [[2.8082022666931152]], [[1.60493004322052]], [[1.2676435708999634]], [[1.8510557413101196]], [[1.9116116762161255]], [[2.2676262855529785]], [[0.7799822092056274]], [[0.6499109268188477]], [[0.6109250783920288]], [[1.4262408018112183]], [[2.1060242652893066]], [[1.8891326189041138]], [[1.581337332725525]], [[1.5551704168319702]], [[2.038576126098633]], [[0.0]], [[0.5216711759567261]], [[0.7156169414520264]], [[2.3326849937438965]], [[2.460092067718506]], [[2.153522491455078]], [[2.5461974143981934]], [[0.6720304489135742]], [[1.6738661527633667]], [[1.2854472398757935]], [[1.6103843450546265]], [[0.9254437685012817]], [[0.7862416505813599]], [[0.8955343961715698]], [[0.8492083549499512]], [[1.72173011302948]], [[1.917054533958435]], [[3.2798261642456055]], [[2.893906593322754]], [[2.6822142601013184]], [[1.624419093132019]], [[0.9943372011184692]], [[0.6927067041397095]], [[0.6721622943878174]], [[0.7884976863861084]], [[0.6340198516845703]], [[0.6599429845809937]], [[2.2159271240234375]], [[1.138895869255066]], [[1.061730980873108]], [[1.950661063194275]], [[0.6957519054412842]], [[0.7579977512359619]], [[2.5658931732177734]], [[0.8239883184432983]], [[2.1036996841430664]], [[1.3534969091415405]], [[1.4837993383407593]], [[1.3639572858810425]], [[1.8215831518173218]], [[0.0]], [[0.5216712951660156]], [[0.7156182527542114]], [[2.332686424255371]], [[2.460092067718506]], [[2.6467366218566895]], [[2.2738776206970215]], [[1.07376229763031]], [[2.8620200157165527]], [[2.088226795196533]], [[2.7986598014831543]], [[2.6821374893188477]], [[2.0475635528564453]], [[0.6698997020721436]], [[0.4517906904220581]], [[0.9629546403884888]], [[1.349291205406189]], [[1.0727766752243042]], [[1.9716418981552124]], [[1.2207974195480347]], [[1.2624439001083374]], [[2.616454601287842]], [[2.298614025115967]], [[1.63986337184906]], [[0.0]], [[0.5216711759567261]], [[0.7156169414520264]], [[2.3326849937438965]], [[2.460092067718506]], [[2.52280330657959]], [[2.2746238708496094]], [[2.036457061767578]], [[1.4281059503555298]], [[0.5927354097366333]], [[1.3724998235702515]], [[1.4587424993515015]], [[1.1682075262069702]], [[0.8304461240768433]], [[0.735143780708313]], [[1.772823452949524]], [[1.9989570379257202]], [[2.2219204902648926]], [[2.2495107650756836]], [[3.395446300506592]], [[3.0039572715759277]], [[1.7995234727859497]], [[2.016955852508545]], [[1.5174118280410767]], [[2.2113537788391113]], [[1.5015348196029663]], [[1.8410013914108276]], [[1.9891985654830933]], [[1.6721845865249634]], [[0.6752011775970459]], [[1.2864230871200562]], [[0.6704167127609253]], [[1.1985896825790405]], [[1.6379262208938599]], [[1.396199345588684]], [[1.4684966802597046]], [[0.0]], [[0.5216712951660156]], [[0.7156182527542114]], [[2.332686424255371]], [[2.460092067718506]], [[2.338808536529541]], [[2.1956467628479004]], [[2.4277172088623047]], [[1.4705373048782349]], [[2.919841766357422]], [[2.090555191040039]], [[2.0661087036132812]], [[2.2922019958496094]], [[2.789336681365967]], [[2.083683490753174]], [[2.2934131622314453]], [[2.4402618408203125]], [[3.2135086059570312]], [[1.942264199256897]], [[1.9586912393569946]], [[1.6697410345077515]], [[3.1482014656066895]], [[2.7512998580932617]], [[3.720475196838379]], [[2.848219871520996]], [[2.0815820693969727]], [[1.2432862520217896]], [[0.0]], [[0.5216711759567261]], [[0.7156169414520264]], [[2.3326849937438965]], [[2.460092067718506]], [[2.0416688919067383]], [[1.895663857460022]], [[0.9436758756637573]], [[0.7127730846405029]], [[1.2120240926742554]], [[1.3047255277633667]], [[0.0]], [[1.4234310388565063]], [[2.2530593872070312]], [[1.9398740530014038]], [[2.107065200805664]], [[3.436169147491455]], [[2.7345385551452637]], [[3.622969150543213]], [[2.8974838256835938]], [[2.386258125305176]], [[2.109475612640381]], [[2.0308613777160645]], [[1.3653377294540405]], [[1.7721327543258667]], [[1.8509570360183716]], [[2.302855968475342]], [[1.3229821920394897]], [[2.3360700607299805]], [[1.1951569318771362]], [[0.6057519912719727]], [[0.6591504812240601]], [[0.9308327436447144]], [[1.052136778831482]], [[1.1259855031967163]], [[1.0028175115585327]], [[0.0]], [[0.5216711759567261]], [[0.7156169414520264]], [[2.3326849937438965]], [[2.460092067718506]], [[2.3709545135498047]], [[2.698197841644287]], [[2.4781746864318848]], [[2.1374807357788086]], [[1.337328314781189]], [[1.6111589670181274]], [[1.485880732536316]], [[1.1207512617111206]], [[0.8994148969650269]], [[2.1488986015319824]], [[1.6408904790878296]], [[1.6414111852645874]], [[0.7943800687789917]], [[0.7674350738525391]], [[0.8806208372116089]], [[1.166411280632019]], [[1.832213044166565]], [[2.182859420776367]], [[3.2359628677368164]], [[2.7464189529418945]], [[3.257676124572754]], [[2.5264358520507812]], [[2.853855609893799]], [[2.0913429260253906]], [[1.411394476890564]], [[1.6031135320663452]], [[1.4537056684494019]], [[1.2720474004745483]], [[1.6402088403701782]], [[1.498846411705017]], [[1.5915664434432983]], [[0.7993941307067871]], [[0.0]], [[0.521670937538147]], [[0.7156163454055786]], [[2.3326849937438965]], [[2.4600911140441895]], [[1.7838271856307983]], [[3.002870559692383]], [[0.9258197546005249]], [[1.3757151365280151]], [[0.7065662145614624]], [[0.0]], [[0.5216711759567261]], [[0.7156169414520264]], [[2.3326849937438965]], [[2.460092067718506]], [[2.352426528930664]], [[2.1139936447143555]], [[0.3778480291366577]], [[0.9919017553329468]], [[0.11749064922332764]], [[1.0507558584213257]], [[0.5873109102249146]], [[0.5755388736724854]], [[0.0]], [[0.4686230421066284]], [[1.1725941896438599]], [[0.005842804908752441]], [[1.6309226751327515]], [[0.9236716032028198]], [[0.28582584857940674]], [[1.819844126701355]], [[1.9342776536941528]], [[2.194457530975342]], [[0.0]], [[0.0]], [[0.592745304107666]], [[1.110026240348816]], [[1.1193159818649292]], [[1.3052996397018433]], [[1.7858656644821167]], [[1.8666871786117554]], [[0.5129594802856445]], [[1.6388510465621948]], [[1.3601468801498413]], [[0.7916891574859619]], [[1.4815858602523804]], [[0.23587775230407715]], [[1.4977525472640991]], [[0.4019625186920166]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2790219783782959]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.2509702444076538]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.018224120140075684]], [[0.6444646120071411]], [[0.7520760297775269]], [[0.05674123764038086]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.047574520111083984]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.013296246528625488]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.019739747047424316]], [[0.0]], [[0.2720431089401245]], [[0.010525226593017578]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f14a4956cd0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_idx = main_features[2].item()\n",
    " \n",
    "k = 20\n",
    "text_list, full_text, token_list, full_token_list, sampled_indices = get_feature_datapoints_with_idx(feature_idx, dict_activations_main, model.tokenizer, token_amount, dataset, setting=\"uniform\", k=k)\n",
    "visualize_text(text_list, feature_idx, model, autoencoder_prior, layer_focus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
