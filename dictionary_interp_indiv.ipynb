{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"Elriggs/pythia-70m-deduped\"\n",
    "\n",
    "# The layer around which we want to interpret by looking at the last layer\n",
    "layer_focus = 1\n",
    "assert layer_focus > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 9.999999747378752e-05, 0.00019306977628730237, 0.000372759357560426, 0.0007196856895461679, 0.0013894954463467002, 0.0026826958637684584, 0.005179474595934153, 0.009999999776482582]\n",
      "{'dict_size': 3072, 'l1_alpha': 0.0013894954463467002}\n",
      "[0.0, 9.999999747378752e-05, 0.00019306977628730237, 0.000372759357560426, 0.0007196856895461679, 0.0013894954463467002, 0.0026826958637684584, 0.005179474595934153, 0.009999999776482582]\n",
      "{'dict_size': 3072, 'l1_alpha': 0.0013894954463467002}\n"
     ]
    }
   ],
   "source": [
    "def load_autoencoder(layer: int):\n",
    "\tae_download_location_main = hf_hub_download(repo_id=model_id, filename=f\"tied_residual_l{layer_focus}_r6/_63/learned_dicts.pt\")\n",
    "\tall_autoencoders = torch.load(ae_download_location_main)\n",
    "\tall_l1s = [hyperparams[\"l1_alpha\"] for autoencoder, hyperparams in all_autoencoders]\n",
    "\t# TODO: choose best one???\n",
    "\tprint(all_l1s)\n",
    "\tauto_num = 5\n",
    "\tautoencoder, hyperparams = all_autoencoders[auto_num]\n",
    "\t# You want a hyperparam around 1e-3. Higher is less features/datapoint (at the cost of reconstruction error); lower is more features/datapoint (at the cost of polysemanticity)\n",
    "\tautoencoder.to_device(device)\n",
    "\tprint(hyperparams)\n",
    "\treturn autoencoder, hyperparams\n",
    "\n",
    "autoencoder_main, hyperparams_main = load_autoencoder(layer_focus)\n",
    "autoencoder_prior, hyperparams_prior = load_autoencoder(layer_focus - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# autoencoder_larger, larger_hyperparams = all_autoencoders[auto_num+1]\n",
    "\n",
    "# #Dictionary Comparison\n",
    "# autoencoder_features = hyperparams[\"dict_size\"]\n",
    "# autoencoder_larger_features = larger_hyperparams[\"dict_size\"]\n",
    "# autoencoder_larger.to_device(device)\n",
    "\n",
    "# # Hungary algorithm\n",
    "# # Calculate all cosine similarities and store in a 2D array\n",
    "# cos_sims = np.zeros((autoencoder_features, autoencoder_larger_features))\n",
    "# for idx, vector in enumerate(autoencoder.get_learned_dict()):\n",
    "#     cos_sims[idx] = torch.nn.functional.cosine_similarity(vector.to(device), autoencoder_larger.get_learned_dict(), dim=1).cpu().numpy()\n",
    "# # Convert to a minimization problem\n",
    "# cos_sims = 1 - cos_sims\n",
    "# # Use the Hungarian algorithm to solve the assignment problem\n",
    "# row_ind, col_ind = linear_sum_assignment(cos_sims)\n",
    "# # Retrieve the max cosine similarities and corresponding indices\n",
    "# max_cosine_similarities = 1 - cos_sims[row_ind, col_ind]\n",
    "\n",
    "# # Get the indices of the max cosine similarities in descending order\n",
    "# max_indices = np.argsort(max_cosine_similarities)[::-1]\n",
    "# max_cosine_similarities[max_indices][:20]\n",
    "# print((\"# of features above 0.9:\", (max_cosine_similarities > .9).sum()))\n",
    "# # Plot histogram of max_cosine_similarities\n",
    "# plt.hist(max_cosine_similarities, bins=20)\n",
    "# plt.xlabel(\"Cosine Similarity\")\n",
    "# plt.title(f\"Max Cos Sim between {hyperparams['l1_alpha']:.2E} & {larger_hyperparams['l1_alpha']:.2E}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "def get_cache_name_neurons(layer: int):\n",
    "    if setting == \"residual\":\n",
    "        cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp\":\n",
    "        cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "        neurons = model.cfg.d_mlp\n",
    "    elif setting == \"attention\":\n",
    "        cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp_out\":\n",
    "        cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return cache_name, neurons\n",
    "\n",
    "cache_name, neurons  = get_cache_name_neurons(layer_focus)\n",
    "cache_name_prior, _  = get_cache_name_neurons(layer_focus - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-75627fbeda83a2c4.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3182e4e6d9ffacd3.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-54571ecbcc6289aa.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "token_amount= 40\n",
    "#TODO: change train[:1000] to train if you want whole dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train[:1000]\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c479af1707b241e3aa28f2ba7978bebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f370dc9b5844637b01b4a7c775afcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "def get_activations(autoencoder):\n",
    "    num_features, d_model = autoencoder.encoder.shape\n",
    "    datapoints = dataset.num_rows\n",
    "    batch_size = 32\n",
    "    neuron_activations = torch.zeros((datapoints*token_amount, d_model))\n",
    "    dictionary_activations = torch.zeros((datapoints*token_amount, num_features))\n",
    "    smaller_auto_encoder = autoencoder\n",
    "    smaller_auto_encoder.to_device(device)\n",
    "\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "            neuron_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_neuron_activations.cpu()\n",
    "            batched_dictionary_activations = smaller_auto_encoder.encode(batched_neuron_activations)\n",
    "            dictionary_activations[i*batch_size*token_amount:(i+1)*batch_size*token_amount,:] = batched_dictionary_activations.cpu()\n",
    "    return dictionary_activations\n",
    "\n",
    "dict_activations_main = get_activations(autoencoder_main)\n",
    "dict_activations_prior = get_activations(autoencoder_prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Interp\n",
    "Investigate the example sentences the activate this feature.\n",
    "\n",
    "Max: show max activating (tokens,contexts)\n",
    "\n",
    "Uniform: Show range of activations from each bin (e.g. sample an example from 1-2, 2-3, etc). \n",
    "[Note: if a feature is monosemantic, then the full range of activations should be that feature, not just max-activating ones]\n",
    "\n",
    "Full_text: shows the full text example\n",
    "\n",
    "Text_list: shows up to the most activating example (try w/ max activating on a couple of examples to see)\n",
    "\n",
    "ablate_text: remove the context one token at a time, and show the decrease/increase in activation of that feature\n",
    "\n",
    "ablate_feature_direction: removes feature direction from model's activation mid-inference, showing the logit diff in the output for every token.\n",
    "\n",
    "logit_lens: show the logit lens for that feature. If matches ablate_feature_direction, then the computation path is through the residual stream, else, it's through future layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39320, 3072])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_activations_prior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuitsvis.activations import text_neuron_activations\n",
    "import torch\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints_with_idx(feature_index, dictionary_activations, tokenizer, token_amount, dataset, k=10, setting=\"max\"):\n",
    "    best_feature_activations = dictionary_activations[:, feature_index]\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        found_indices = torch.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        # min_value = torch.min(best_feature_activations)\n",
    "        min_value = torch.min(best_feature_activations)\n",
    "        max_value = torch.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = torch.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in torch.unique(bins):\n",
    "            if(bin_idx==0): # Skip the first one. This is below the median\n",
    "                continue\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = torch.nonzero(bins == bin_idx, as_tuple=False).squeeze(dim=1)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = torch.tensor(sampled_indices).long().flip(dims=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    num_datapoints = int(dictionary_activations.shape[0]/token_amount)\n",
    "    datapoint_indices =[np.unravel_index(i, (num_datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list, found_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Index tensor([ 6738, 38178, 14329, 31726, 31759, 15716, 18798, 25789, 25622, 35054])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-5331c2a0-6508\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-5331c2a0-6508\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"R\", \"ent\", \"z\", \" R\", \"Vs\", \" Inc\", \".\", \" (\", \"RR\", \"V\", \")\", \"\\\\newline\", \"\\\\newline\", \"1\", \".\", \" A\", \" stock\", \" is\", \" expected\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"Un\", \"precedented\", \"ly\", \" precise\", \" cosmic\", \" microwave\", \" background\", \" (\", \"CM\", \"B\", \")\", \" data\", \" are\", \" expected\", \"\\n\", \"Visual\", \" attention\", \" to\", \" features\", \" by\", \" associative\", \" learning\", \".\", \"\\\\newline\", \"Expect\", \"\\n\", \"General\", \" Election\", \" 10\", \" generally\", \" went\", \" the\", \" predicted\", \"\\n\", \"General\", \" Election\", \" 10\", \" generally\", \" went\", \" the\", \" predicted\", \" way\", \",\", \" albeit\", \" for\", \" a\", \" few\", \" surprises\", \" such\", \" as\", \" Labour\", \" not\", \" being\", \" dec\", \"imated\", \" even\", \" after\", \" a\", \" major\", \" scandal\", \" /\", \" poor\", \" term\", \",\", \" and\", \" New\", \" Britain\", \"\\u2019\", \"s\", \" electoral\", \" failure\", \".\", \" As\", \" predicted\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"In\", \" state\", \" space\", \" models\", \",\", \" smoothing\", \" refers\", \" to\", \" the\", \" task\", \" of\", \" estimating\", \" a\", \" latent\", \" stochastic\", \" process\", \" given\", \" noisy\", \" measurements\", \" related\", \" to\", \" the\", \" process\", \".\", \" We\", \" propose\", \" an\", \" unbiased\", \" estimator\", \" of\", \" smoothing\", \" expectations\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" measure\", \" g\", \" using\", \" a\", \" met\", \"re\", \" stick\", \" and\", \" a\", \" ball\", \"\\\\newline\", \"\\\\newline\", \"Can\", \" I\", \" measure\", \" the\", \" value\", \" of\", \" g\", \" using\", \" only\", \" a\", \" met\", \"re\", \" stick\", \" and\", \" a\", \" ball\", \"?\", \" I\", \" am\", \" not\", \" supposed\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" do\", \" I\", \" access\", \" an\", \" instance\", \" of\", \" a\", \" class\", \" that\", \" is\", \" inside\", \" an\", \" array\", \"List\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" doing\", \" a\", \" MO\", \"OC\", \" and\", \" am\", \" supposed\", \"\\n\", \"Hy\", \"unda\", \"i\", \" Motor\", \" Co\", \".\", \" officially\", \" launched\", \" Santa\", \" Fe\", \" in\", \" New\", \" York\", \" Auto\", \" Show\", \" 2012\", \" some\", \" time\", \" ago\", \".\", \" Santa\", \" Fe\", \" planned\", \"\\n\", \"2020\", \" Australian\", \" S\", \"5000\", \" Championship\", \"\\\\newline\", \"\\\\newline\", \"The\", \" 2020\", \" Australian\", \" S\", \"5000\", \" Championship\", \" is\", \" planned\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.053551435470581]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9013288021087646]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5892302989959717]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6434235572814941]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6434228420257568]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5225279331207275]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.804863691329956]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.0896680355072021]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9648325443267822]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5490217208862305]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7815039157867432]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f3088e2e510>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from interp_utils import *\n",
    "# Can sort by MMCS w/ the larger dictionary\n",
    "# indexed_feature = 13\n",
    "# best_feature = int(max_indices[indexed_feature])\n",
    "# Or just random index \n",
    "feature_idx = 13\n",
    "\n",
    "text_list, full_text, token_list, full_token_list, sampled_indices = get_feature_datapoints_with_idx(feature_idx, dict_activations_prior, model.tokenizer, token_amount, dataset, setting=\"max\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(feature_idx, dict_activations_prior, model.tokenizer, token_amount, dataset, setting=\"max\")\n",
    "# text_list, full_text, token_list, full_token_list = get_feature_datapoints(best_feature, dictionary_activations, dataset, setting=\"max\")\n",
    "# visualize_text(full_text, best_feature, model, autoencoder, layer)\n",
    "print(\"Sampled Index\", sampled_indices)\n",
    "visualize_text(text_list, feature_idx, model, autoencoder_prior, layer_focus - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the **downstream** feature in the next layer that is most activated by random subset of this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Index tensor([ 6738, 38178, 14329, 31726, 31759, 15716, 18798, 25789, 25622, 35054])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-7d44ad68-4ee2\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-7d44ad68-4ee2\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"R\", \"ent\", \"z\", \" R\", \"Vs\", \" Inc\", \".\", \" (\", \"RR\", \"V\", \")\", \"\\\\newline\", \"\\\\newline\", \"1\", \".\", \" A\", \" stock\", \" is\", \" expected\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"Un\", \"precedented\", \"ly\", \" precise\", \" cosmic\", \" microwave\", \" background\", \" (\", \"CM\", \"B\", \")\", \" data\", \" are\", \" expected\", \"\\n\", \"Visual\", \" attention\", \" to\", \" features\", \" by\", \" associative\", \" learning\", \".\", \"\\\\newline\", \"Expect\", \"\\n\", \"General\", \" Election\", \" 10\", \" generally\", \" went\", \" the\", \" predicted\", \"\\n\", \"General\", \" Election\", \" 10\", \" generally\", \" went\", \" the\", \" predicted\", \" way\", \",\", \" albeit\", \" for\", \" a\", \" few\", \" surprises\", \" such\", \" as\", \" Labour\", \" not\", \" being\", \" dec\", \"imated\", \" even\", \" after\", \" a\", \" major\", \" scandal\", \" /\", \" poor\", \" term\", \",\", \" and\", \" New\", \" Britain\", \"\\u2019\", \"s\", \" electoral\", \" failure\", \".\", \" As\", \" predicted\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" '\", \"In\", \" state\", \" space\", \" models\", \",\", \" smoothing\", \" refers\", \" to\", \" the\", \" task\", \" of\", \" estimating\", \" a\", \" latent\", \" stochastic\", \" process\", \" given\", \" noisy\", \" measurements\", \" related\", \" to\", \" the\", \" process\", \".\", \" We\", \" propose\", \" an\", \" unbiased\", \" estimator\", \" of\", \" smoothing\", \" expectations\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" to\", \" measure\", \" g\", \" using\", \" a\", \" met\", \"re\", \" stick\", \" and\", \" a\", \" ball\", \"\\\\newline\", \"\\\\newline\", \"Can\", \" I\", \" measure\", \" the\", \" value\", \" of\", \" g\", \" using\", \" only\", \" a\", \" met\", \"re\", \" stick\", \" and\", \" a\", \" ball\", \"?\", \" I\", \" am\", \" not\", \" supposed\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"How\", \" do\", \" I\", \" access\", \" an\", \" instance\", \" of\", \" a\", \" class\", \" that\", \" is\", \" inside\", \" an\", \" array\", \"List\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" doing\", \" a\", \" MO\", \"OC\", \" and\", \" am\", \" supposed\", \"\\n\", \"Hy\", \"unda\", \"i\", \" Motor\", \" Co\", \".\", \" officially\", \" launched\", \" Santa\", \" Fe\", \" in\", \" New\", \" York\", \" Auto\", \" Show\", \" 2012\", \" some\", \" time\", \" ago\", \".\", \" Santa\", \" Fe\", \" planned\", \"\\n\", \"2020\", \" Australian\", \" S\", \"5000\", \" Championship\", \"\\\\newline\", \"\\\\newline\", \"The\", \" 2020\", \" Australian\", \" S\", \"5000\", \" Championship\", \" is\", \" planned\", \"\\n\"], \"activations\": [[[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[3.053551435470581]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[2.9013288021087646]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.5892302989959717]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6434235572814941]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.6434228420257568]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5225279331207275]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.804863691329956]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[1.0896680355072021]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.9648325443267822]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.5490217208862305]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.7815039157867432]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f308977efd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_feature_idx = 13\n",
    "\n",
    "text_list, full_text, token_list, full_token_list, sampled_indices = get_feature_datapoints_with_idx(feature_idx, dict_activations_prior, model.tokenizer, token_amount, dataset, setting=\"max\")\n",
    "\n",
    "print(\"Sampled Index\", sampled_indices)\n",
    "visualize_text(text_list, feature_idx, model, autoencoder_prior, layer_focus - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1287)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1287,   13, 1515, 1926, 2919, 2274,  621, 2221,  715, 1261]),\n",
       " tensor([1287,   13, 1515, 1926, 2919, 2274,  621, 2221,  715, 1261]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict_activations_prior.shape, dict_activations_main.shape\n",
    "TOP_FEATURES = 10\n",
    "# We do not need absolute value as I think that everything is positive b/c ReLU\n",
    "summed = dict_activations_main[sampled_indices].sum(dim=0)\n",
    "summed_abs = summed.abs()\n",
    "print(summed.argmax())\n",
    "main_features = summed.argsort(descending=True)[:TOP_FEATURES]\n",
    "main_features, summed_abs.argsort(descending=True)[:TOP_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = main_features[0]\n",
    " \n",
    "text_list, full_text, token_list, full_token_list, sampled_indices = get_feature_datapoints_with_idx(feature_idx, dict_activations_main, model.tokenizer, token_amount, dataset, setting=\"max\")\n",
    "visualize_text(text_list, feature_idx, model, autoencoder_prior, layer_focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ec31c19c-2b9b\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ec31c19c-2b9b\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"M\", \"oss\", \" (\", \"Phys\", \"comit\", \"rella\", \" pat\", \"ens\", \")\", \" GH\", \"\\n\", \"(\", \"Sports\", \"Network\", \".\", \"com\", \")\", \" -\", \" Bruce\", \"\\n\", \"AR\", \"MED\", \" SERVICES\", \" BO\", \"ARD\", \" OF\", \" CONTRACT\", \" APPEALS\", \"\\\\newline\", \"\\\\newline\", \"Appeal\", \" of\", \" --\", \"\\\\newline\", \"\\\\newline\", \")\", \"\\\\newline\", \"\\\\newline\", \")\", \"\\\\newline\", \"_\", \" )\", \" AS\", \"BC\", \"A\", \" N\", \"\\u00b0\", \"'\", \" 6\", \"03\", \"15\", \"\\\\newline\", \"\\\\newline\", \")\", \"\\\\newline\\\\newline\", \"\\n\", \"[\", \"Pl\", \"ant\", \"ar\", \"-\", \"pal\", \"mar\", \" eryth\", \"rod\", \"ys\", \"est\", \"hesia\", \".\", \" A\", \" new\", \" and\", \" relatively\", \" frequent\", \" side\", \" effect\", \" in\", \" ant\", \"ine\", \"oplastic\", \" treatment\", \"].\", \"\\\\newline\", \"Pal\", \"\\n\", \"---\", \"\\\\newline\", \"abstract\", \":\", \" |\", \"\\\\newline\", \"    \", \"We\", \" give\", \" a\", \" general\", \" construction\", \" of\", \" de\", \"biased\", \"/\", \"loc\", \"ally\", \" robust\", \"/\", \"orth\", \"ogonal\", \" (\", \"LR\", \")\", \" moment\", \"\\n\", \"Ti\", \"O\", \"\\n\"], \"activations\": [[[0.24689960479736328]], [[0.0897212028503418]], [[2.6524481773376465]], [[0.0598750114440918]], [[-0.25872278213500977]], [[0.040201663970947266]], [[-0.18775081634521484]], [[0.22060251235961914]], [[-1.7578802108764648]], [[-1.7578802108764648]], [[0.0]], [[0.6930880546569824]], [[-0.0614469051361084]], [[0.022599458694458008]], [[0.49445533752441406]], [[0.10129499435424805]], [[-1.5296683311462402]], [[0.7536129951477051]], [[-1.1315927505493164]], [[0.0]], [[-0.002390146255493164]], [[-0.019585609436035156]], [[0.0626680850982666]], [[0.01507711410522461]], [[-0.010713338851928711]], [[-0.01215052604675293]], [[0.0914452075958252]], [[-0.5644659996032715]], [[-0.029992103576660156]], [[-0.029992103576660156]], [[-0.13664555549621582]], [[-0.08668875694274902]], [[0.009149551391601562]], [[0.012486696243286133]], [[0.012486696243286133]], [[-0.23746371269226074]], [[0.01725935935974121]], [[0.01725935935974121]], [[-0.24169182777404785]], [[0.012551546096801758]], [[-0.006418466567993164]], [[-0.3426370620727539]], [[0.004926443099975586]], [[0.03546929359436035]], [[0.036377668380737305]], [[0.007082462310791016]], [[0.0013239383697509766]], [[-0.01601433753967285]], [[-0.009234189987182617]], [[0.010329961776733398]], [[-0.0054204463958740234]], [[0.12570476531982422]], [[0.12570476531982422]], [[-0.9440203309059143]], [[-0.8374548554420471]], [[0.0]], [[0.1358931064605713]], [[-0.007337450981140137]], [[-0.022840142250061035]], [[-0.006342172622680664]], [[0.038091301918029785]], [[-0.21573948860168457]], [[-0.034105539321899414]], [[-0.010980963706970215]], [[-0.03535306453704834]], [[-0.02920389175415039]], [[-0.031029939651489258]], [[-0.061017751693725586]], [[-0.0311739444732666]], [[-0.02724325656890869]], [[-0.03085780143737793]], [[-0.06025528907775879]], [[-0.07039248943328857]], [[0.00608515739440918]], [[-0.0333399772644043]], [[-0.04230666160583496]], [[0.006704807281494141]], [[-0.050559043884277344]], [[-0.058719515800476074]], [[-0.06669056415557861]], [[-0.04005730152130127]], [[-0.6327149271965027]], [[0.24984610080718994]], [[-0.6327149271965027]], [[0.0]], [[-0.07528567314147949]], [[-0.06558245420455933]], [[-0.11050355434417725]], [[0.06705820560455322]], [[0.022358417510986328]], [[0.0030308961868286133]], [[0.055732131004333496]], [[0.01424705982208252]], [[-0.0026488304138183594]], [[0.03798484802246094]], [[0.014093756675720215]], [[0.014198064804077148]], [[0.043434977531433105]], [[0.058271169662475586]], [[-0.020761549472808838]], [[0.010192155838012695]], [[-0.013418793678283691]], [[-0.00689244270324707]], [[-0.032557904720306396]], [[0.044960856437683105]], [[-0.013056516647338867]], [[-0.03069019317626953]], [[1.101832628250122]], [[-0.12966716289520264]], [[-0.2630557417869568]], [[-0.2630557417869568]], [[0.0]], [[-0.1295558214187622]], [[0.6736994385719299]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fc51db37110>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_text(text_list, feature_idx, model, autoencoder, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-5b61fa18-e3e7\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-5b61fa18-e3e7\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"\\\\newline\", \"7\", \"\\\\newline\", \"Let\", \" o\", \"(\", \"g\", \")\", \" be\", \" the\", \" third\", \" derivative\", \" of\", \" -\", \"g\", \"**\", \"4\", \"/\", \"12\", \" +\", \" g\", \"**\", \"3\", \"/\", \"2\", \" +\", \" 20\", \"*\", \"g\", \"**\", \"2\", \".\", \" Suppose\", \" -\", \"b\", \" -\", \" 2\", \"*\", \"b\", \"\\n\", \"oss\", \" (\", \"Phys\", \"comit\", \"rella\", \" pat\", \"ens\", \")\", \" GH\", \"3\", \" proteins\", \" act\", \" in\", \" aux\", \"in\", \" homeostasis\", \".\", \"\\\\newline\", \"A\", \"ux\", \"ins\", \" are\", \" hormones\", \" involved\", \" in\", \" many\", \" cellular\", \",\", \" physiological\", \" and\", \" developmental\", \" processes\", \" in\", \" seed\", \" plants\", \" and\", \" in\", \" moss\", \"es\", \"\\n\", \"Sports\", \"Network\", \".\", \"com\", \")\", \" -\", \" Bruce\", \" Ari\", \"ans\", \" was\", \" a\", \" candidate\", \" for\", \" the\", \" vacant\", \" Philadelphia\", \"\\\\newline\", \"E\", \"agles\", \" job\", \",\", \" but\", \" the\", \" powers\", \" that\", \" be\", \" decided\", \" Chip\", \" Kelly\", \",\", \" with\", \" no\", \" professional\", \"\\\\newline\", \"experience\", \",\", \" was\", \" better\", \" equipped\", \"\\n\", \"valence\", \" of\", \" var\", \"ic\", \"oc\", \"oe\", \"le\", \" and\", \" its\", \" association\", \" with\", \" body\", \" mass\", \" index\", \" among\", \" 39\", \",\", \"559\", \" rural\", \" men\", \" in\", \" eastern\", \" China\", \":\", \" a\", \" population\", \"-\", \"based\", \" cross\", \"-\", \"sectional\", \" study\", \".\", \"\\\\newline\", \"Var\", \"ic\", \"oc\", \"oe\", \"le\", \"\\n\", \"MED\", \" SERVICES\", \" BO\", \"ARD\", \" OF\", \" CONTRACT\", \" APPEALS\", \"\\\\newline\", \"\\\\newline\", \"Appeal\", \" of\", \" --\", \"\\\\newline\", \"\\\\newline\", \")\", \"\\\\newline\", \"\\\\newline\", \")\", \"\\\\newline\", \"_\", \" )\", \" AS\", \"BC\", \"A\", \" N\", \"\\u00b0\", \"'\", \" 6\", \"03\", \"15\", \"\\\\newline\", \"\\\\newline\", \")\", \"\\\\newline\", \"\\\\newline\", \")\", \"\\\\newline\", \"\\\\newline\", \"Under\", \"\\n\", \" is\", \" done\", \",\", \" and\", \" submitted\", \".\", \" You\", \" can\", \" play\", \" \\u201c\", \"Sur\", \"vival\", \" of\", \" the\", \" T\", \"ast\", \"iest\", \"\\u201d\", \" on\", \" Android\", \",\", \" and\", \" on\", \" the\", \" web\", \".\", \" Playing\", \" on\", \" the\", \" web\", \" works\", \",\", \" but\", \" you\", \" have\", \" to\", \" simulate\", \" multi\", \"-\", \"\\n\", \"Pl\", \"ant\", \"ar\", \"-\", \"pal\", \"mar\", \" eryth\", \"rod\", \"ys\", \"est\", \"hesia\", \".\", \" A\", \" new\", \" and\", \" relatively\", \" frequent\", \" side\", \" effect\", \" in\", \" ant\", \"ine\", \"oplastic\", \" treatment\", \"].\", \"\\\\newline\", \"Pal\", \"mar\", \"-\", \"plant\", \"ar\", \" eryth\", \"rod\", \"ys\", \"est\", \"hesia\", \" (\", \"P\", \"PE\", \"\\n\", \"\\\\newline\", \"abstract\", \":\", \" |\", \"\\\\newline\", \"    \", \"We\", \" give\", \" a\", \" general\", \" construction\", \" of\", \" de\", \"biased\", \"/\", \"loc\", \"ally\", \" robust\", \"/\", \"orth\", \"ogonal\", \" (\", \"LR\", \")\", \" moment\", \" functions\", \" for\", \" G\", \"MM\", \",\", \" where\", \" the\", \" derivative\", \" with\", \" respect\", \" to\", \" first\", \" step\", \" non\", \"\\n\", \"O\", \"2\", \" nanot\", \"ubes\", \" for\", \" bone\", \" regeneration\", \".\", \"\\\\newline\", \"Nan\", \"ost\", \"ruct\", \"ured\", \" materials\", \" are\", \" believed\", \" to\", \" play\", \" a\", \" fundamental\", \" role\", \" in\", \" orth\", \"opedic\", \" research\", \" because\", \" bone\", \" itself\", \" has\", \" a\", \" structural\", \" hierarchy\", \" at\", \" the\", \" first\", \" level\", \" in\", \" the\", \" nan\", \"\\n\"], \"activations\": [[[0.0027931928634643555]], [[-0.5259890556335449]], [[0.1352858543395996]], [[-0.2214488983154297]], [[-0.05758190155029297]], [[-0.04184317588806152]], [[-0.006076335906982422]], [[-0.0002850303426384926]], [[-0.002045273780822754]], [[0.00011105541489087045]], [[0.05493885278701782]], [[0.0008373633027076721]], [[-0.00013679670519195497]], [[-0.002021491527557373]], [[-0.033882468938827515]], [[0.0004131537862122059]], [[-0.09208536148071289]], [[7.280707359313965e-05]], [[0.008099794387817383]], [[0.0009974241256713867]], [[0.0020844340324401855]], [[4.219589754939079e-05]], [[-0.003220096230506897]], [[-0.002619713544845581]], [[-0.006478548049926758]], [[-0.02241230010986328]], [[0.059261322021484375]], [[-0.0029408633708953857]], [[3.0992778192739934e-06]], [[-8.209515362977982e-05]], [[7.495831232517958e-05]], [[-0.006335735321044922]], [[-0.04373908042907715]], [[0.0019893646240234375]], [[0.0013051033020019531]], [[0.08870339393615723]], [[0.007049918174743652]], [[0.007260456681251526]], [[0.01796555519104004]], [[0.0]], [[-0.0057544708251953125]], [[-0.002837657928466797]], [[0.0011625289916992188]], [[0.013393878936767578]], [[1.348741352558136e-05]], [[0.015789270401000977]], [[-0.001781463623046875]], [[4.3511390686035156e-05]], [[-0.008016586303710938]], [[-0.13794231414794922]], [[-0.10773563385009766]], [[0.041090965270996094]], [[0.053392648696899414]], [[-0.021859169006347656]], [[-0.05430591106414795]], [[0.1352977752685547]], [[0.0158846378326416]], [[-0.011973023414611816]], [[-0.16318130493164062]], [[0.0023899078369140625]], [[0.015848875045776367]], [[-0.0213778018951416]], [[-0.039316654205322266]], [[-0.004374027252197266]], [[-0.00027068424969911575]], [[-0.010439395904541016]], [[-0.01835179328918457]], [[-0.009447097778320312]], [[-0.024901866912841797]], [[0.009296238422393799]], [[-0.00226593017578125]], [[0.0030358731746673584]], [[-0.011806011199951172]], [[0.01473236083984375]], [[-0.020123004913330078]], [[-0.0007529258728027344]], [[-0.00401759147644043]], [[0.007092475891113281]], [[-0.003640890121459961]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-0.00018405914306640625]], [[0.08493804931640625]], [[0.11588430404663086]], [[0.001966673880815506]], [[0.009066581726074219]], [[0.03158426284790039]], [[0.0728292465209961]], [[0.024105310440063477]], [[0.029546141624450684]], [[-0.026078224182128906]], [[0.017818450927734375]], [[0.10539722442626953]], [[0.009621620178222656]], [[0.0014257431030273438]], [[0.01940774917602539]], [[0.015224218368530273]], [[0.018239736557006836]], [[-0.002253293991088867]], [[-0.0011453628540039062]], [[-0.00011014938354492188]], [[-0.012832224369049072]], [[0.0003094673156738281]], [[0.020792007446289062]], [[-0.015118122100830078]], [[-0.00043511390686035156]], [[0.007717132568359375]], [[-0.013509273529052734]], [[0.004721641540527344]], [[-0.020253419876098633]], [[0.00013446807861328125]], [[-0.0024856925010681152]], [[0.016417980194091797]], [[0.006566047668457031]], [[-0.04349374771118164]], [[0.0]], [[-0.09728813171386719]], [[0.029720544815063477]], [[-0.005354881286621094]], [[-0.0395737886428833]], [[-0.00177764892578125]], [[-0.02353382110595703]], [[0.01881122589111328]], [[-0.018909215927124023]], [[0.008939743041992188]], [[-0.015549659729003906]], [[-0.0001313239336013794]], [[0.0030689239501953125]], [[-0.0035445690155029297]], [[-0.0012871026992797852]], [[-0.0004634857177734375]], [[0.002071380615234375]], [[-0.027057886123657227]], [[-0.00646209716796875]], [[-0.024474143981933594]], [[0.009751319885253906]], [[-0.007018685340881348]], [[-0.0038428306579589844]], [[0.00707244873046875]], [[0.01251077651977539]], [[-0.003191351890563965]], [[0.009732484817504883]], [[-0.007266998291015625]], [[0.00026794523000717163]], [[0.0009982585906982422]], [[-0.0009660273790359497]], [[2.8318725526332855e-05]], [[0.0016540884971618652]], [[-0.0007790327072143555]], [[-0.0033257007598876953]], [[-0.003688812255859375]], [[3.7128105759620667e-05]], [[0.0005719587206840515]], [[-0.00011024065315723419]], [[-3.4444965422153473e-06]], [[0.0]], [[-0.051278114318847656]], [[-0.007405281066894531]], [[-0.00412750244140625]], [[-0.00739598274230957]], [[-0.002005338668823242]], [[-0.0874183177947998]], [[-0.00988742709159851]], [[-0.00027063488960266113]], [[0.0013039112091064453]], [[-0.0049419403076171875]], [[0.02191448211669922]], [[-0.013330459594726562]], [[0.018311500549316406]], [[0.0023822784423828125]], [[-0.0016169548034667969]], [[-0.0038841962814331055]], [[-0.00024263374507427216]], [[-0.05888700485229492]], [[0.01601940393447876]], [[0.10923004150390625]], [[-0.36765432357788086]], [[-0.28269433975219727]], [[0.2266845703125]], [[-0.01060187816619873]], [[0.48488616943359375]], [[0.032500267028808594]], [[-0.06454277038574219]], [[0.16173839569091797]], [[-0.10922861099243164]], [[-0.09154224395751953]], [[0.009713172912597656]], [[0.07483401894569397]], [[-0.5204166769981384]], [[0.014981195330619812]], [[0.05434896796941757]], [[-0.0071357786655426025]], [[0.01068263128399849]], [[0.027761787176132202]], [[0.24684858322143555]], [[0.0]], [[0.005954265594482422]], [[0.01346588134765625]], [[-0.009594202041625977]], [[0.004911899566650391]], [[-0.01190948486328125]], [[-0.012398719787597656]], [[-0.0019378662109375]], [[0.0001043081283569336]], [[0.0034856796264648438]], [[0.011346817016601562]], [[-0.0011653900146484375]], [[0.001356363296508789]], [[-0.002468585968017578]], [[-0.0001201927661895752]], [[-0.002871990203857422]], [[0.010920047760009766]], [[-0.0012559890747070312]], [[-0.0007059574127197266]], [[-0.0010211467742919922]], [[-0.00027561187744140625]], [[0.0025529861450195312]], [[0.0018742084503173828]], [[-0.0036668777465820312]], [[7.557868957519531e-05]], [[0.004194498062133789]], [[0.0005030632019042969]], [[-0.01215362548828125]], [[-0.002361297607421875]], [[0.0011398792266845703]], [[0.005106449127197266]], [[-0.0004968643188476562]], [[0.0032558441162109375]], [[0.0009063482284545898]], [[-0.001173853874206543]], [[-0.00010180473327636719]], [[0.0007520020008087158]], [[0.0031957626342773438]], [[0.000576019287109375]], [[0.0003062039613723755]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-3.5762786865234375e-06]], [[0.0]], [[0.0]], [[-0.0009136199951171875]], [[0.0]], [[-0.0004608035087585449]], [[7.62939453125e-06]], [[1.7881393432617188e-06]], [[-5.555152893066406e-05]], [[-1.1920928955078125e-06]], [[0.00012922286987304688]], [[0.0]], [[-2.86102294921875e-06]], [[0.0]], [[0.0]], [[0.0]], [[6.866455078125e-05]], [[0.0007681846618652344]], [[0.0]], [[-9.447336196899414e-05]], [[2.384185791015625e-07]], [[0.0011692047119140625]], [[-0.005760163068771362]], [[-0.08550071716308594]], [[-0.000666874460875988]], [[0.020827770233154297]], [[-0.05501747131347656]], [[0.00857427716255188]], [[0.004625767469406128]], [[0.004411563277244568]], [[-4.990026354789734e-06]], [[-0.00047031231224536896]], [[0.0021588001400232315]], [[-0.05224442481994629]], [[-0.0012259483337402344]], [[-0.001335740089416504]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[0.0]], [[-1.1734664440155029e-07]], [[0.0]], [[0.0]], [[0.0]], [[-3.129243850708008e-05]], [[0.00014209747314453125]], [[-0.0016880035400390625]], [[-0.00027805566787719727]], [[-0.0010433197021484375]], [[0.0005979537963867188]], [[-0.0002541542053222656]], [[0.0010366439819335938]], [[0.00015997886657714844]], [[0.0005812644958496094]], [[-0.0011196136474609375]], [[0.00048828125]], [[4.813075065612793e-06]], [[0.0006895065307617188]], [[0.0012826919555664062]], [[7.43567943572998e-05]], [[0.00031566619873046875]], [[-0.009891271591186523]], [[-0.0196915864944458]], [[0.0024061203002929688]], [[-0.002598285675048828]], [[0.015047550201416016]], [[0.01625823974609375]], [[-0.0007382631301879883]], [[0.016935348510742188]], [[0.02142953872680664]], [[0.00028433650732040405]], [[-3.5453587770462036e-05]], [[0.004261970520019531]], [[0.0012364387512207031]], [[-0.0034761428833007812]], [[0.0]], [[-0.07542276382446289]], [[0.009362936019897461]], [[-0.028693199157714844]], [[0.004706621170043945]], [[-0.002815723419189453]], [[-0.0050201416015625]], [[0.0029456615447998047]], [[0.005443930625915527]], [[-0.0014821290969848633]], [[-0.00412750244140625]], [[0.01177072525024414]], [[-7.197260856628418e-05]], [[-0.00015845149755477905]], [[-0.0011076927185058594]], [[-0.0012830495834350586]], [[-0.0004901885986328125]], [[-1.58543698489666e-05]], [[-0.0026760101318359375]], [[0.0005075931549072266]], [[0.0002906322479248047]], [[-1.584365963935852e-05]], [[-2.485141158103943e-05]], [[0.004399776458740234]], [[0.0028553009033203125]], [[-0.0008335113525390625]], [[0.0032320022583007812]], [[-0.002365589141845703]], [[0.0026488304138183594]], [[-0.0012726783752441406]], [[0.0010150671005249023]], [[0.0009784698486328125]], [[-0.0005407333374023438]], [[-0.0002384185791015625]], [[-1.6391277313232422e-05]], [[0.005349159240722656]], [[0.0031218528747558594]], [[-0.00029277801513671875]], [[-0.0005259513854980469]], [[-0.025058746337890625]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fc463fe2ad0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablate_feature_direction_display(full_text, autoencoder, model, layer, features=feature_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\";', ' respectively', ' {¶', '()\"', 'wise', '<>();', 'woke', 'essential', 'ients', 'uple', 'rost', '\\n\\t\\n', 'Delegate', 'free', '.\";', 'tetra', 'append', ' AFFIRMED', 'rocy', '>\";']\n",
      "tensor([1.4627, 1.3649, 1.3501, 1.3298, 1.2833, 1.2640, 1.2484, 1.2394, 1.2345,\n",
      "        1.2338, 1.2326, 1.2159, 1.2127, 1.2017, 1.1831, 1.1810, 1.1771, 1.1766,\n",
      "        1.1545, 1.1515])\n"
     ]
    }
   ],
   "source": [
    "logit_lens(model, feature_idx, autoencoder.get_learned_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
