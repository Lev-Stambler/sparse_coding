{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ad2ecfd158f710eb.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-d173f762c78db5b8.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/JeanKaddour___parquet/JeanKaddour--minipile-0d7d2d1ff79d1d36/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bd49b6b2e3856158.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"JeanKaddour/minipile\"\n",
    "token_amount= 40\n",
    "#TODO: change train[:1000] to train if you want whole dataset\n",
    "# 100_000 datasets\n",
    "# I think that we want to use the full 100_000 at some point...\n",
    "# dataset = load_dataset(dataset_name, split=\"train[:100000]\").map(\n",
    "dataset = load_dataset(dataset_name, split=\"train[:10000]\").map( # 1_000 to get started\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")\n",
    "# TODO: we can maybe make this faster for the larger dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = \"residual\"\n",
    "\n",
    "def get_cache_name_neurons(layer: int):\n",
    "    if setting == \"residual\":\n",
    "        cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp\":\n",
    "        cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "        neurons = model.cfg.d_mlp\n",
    "    elif setting == \"attention\":\n",
    "        cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp_out\":\n",
    "        cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return cache_name, neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "model.cfg.d_model, n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Dictionary Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc23339a0984afdba923c29bfb68914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00cc59391bb4a3aa3928c033a7a4a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe65da56d1445db903cd092b7b5c39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a42b3d957c4595a3275932bf4af5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5141a0546a4286be80798ce777668f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf2d9eedf444b468ceb2d25e1ced4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: in chunks...\n",
    "# Now we can use the model to get the activations\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "# MAX_CHUNK_SIZE = 1_000\n",
    "\n",
    "# TODO: move to a separate file or something\n",
    "def get_activations(layer: int):\n",
    "    datapoints = dataset.num_rows\n",
    "    embedding_size = model.cfg.d_model\n",
    "    activations_final = np.memmap(f'layer-{layer}.mymemmap', dtype='float32', mode='w+', shape=(datapoints, token_amount, embedding_size))\n",
    "    batch_size = 32\n",
    "\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        cache_name = get_cache_name_neurons(layer)[0]\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            # print(batch)\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            # print(\"AA\", cache[cache_name].shape)\n",
    "            # batched_neuron_activations = rearrange(cache[cache_name], \"b s n -> (b s) n\" )\n",
    "\n",
    "            real_batch_size = batch.shape[0]\n",
    "            activations_final[i*batch_size:i*batch_size + real_batch_size, :, :] = cache[cache_name].cpu().numpy()\n",
    "    return activations_final\n",
    "\n",
    "model_activations = [get_activations(layer) for layer in range(n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get activations for a specific feature and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9909, 40, 512), (396360, 512))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 0\n",
    "model_activations[0].shape, model_activations[layer].reshape(-1, model_activations[layer].shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interp_utils import *\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Get the activations for the best dict features\n",
    "def get_feature_datapoints_with_idx(feature_index, dictionary_activations, tokenizer, token_amount, dataset, k=10, setting=\"max\"):\n",
    "    if len(dictionary_activations.shape) == 3:\n",
    "        best_feature_activations = dictionary_activations[:, :, feature_index].flatten()\n",
    "    else:\n",
    "        best_feature_activations = dictionary_activations\n",
    "    # Sort the features by activation, get the indices\n",
    "    if setting==\"max\":\n",
    "        # TODO:! Urrr.... is this backwards? CHECK IF ::-1 is correct but I think that it is\n",
    "        found_indices = np.argsort(best_feature_activations)[::-1][:k]\n",
    "        # found_indices = np.argsort(best_feature_activations, descending=True)[:k]\n",
    "    elif setting==\"uniform\":\n",
    "        # min_value = torch.min(best_feature_activations)\n",
    "        min_value = np.min(best_feature_activations)\n",
    "        max_value = np.max(best_feature_activations)\n",
    "\n",
    "        # Define the number of bins\n",
    "        num_bins = k\n",
    "\n",
    "        # Calculate the bin boundaries as linear interpolation between min and max\n",
    "        bin_boundaries = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "        # Assign each activation to its respective bin\n",
    "        # TODO: hmm\n",
    "        # np bucketize?\n",
    "        # bins = torch.bucketize(best_feature_activations, bin_boundaries)\n",
    "        bins = np.digitize(best_feature_activations, bin_boundaries)\n",
    "\n",
    "        # Initialize a list to store the sampled indices\n",
    "        sampled_indices = []\n",
    "\n",
    "        # Sample from each bin\n",
    "        for bin_idx in np.unique(bins):\n",
    "            if(bin_idx==0): # Skip the first one. This is below the median\n",
    "                continue\n",
    "            # Get the indices corresponding to the current bin\n",
    "            bin_indices = np.array(np.nonzero(bins == bin_idx)).squeeze(axis=0)\n",
    "            # print(bin_indices.shape)\n",
    "            \n",
    "            # Randomly sample from the current bin\n",
    "            sampled_indices.extend(np.random.choice(bin_indices, size=1, replace=False))\n",
    "\n",
    "        # Convert the sampled indices to a PyTorch tensor & reverse order\n",
    "        found_indices = np.flip(np.array(sampled_indices), axis=[0])\n",
    "    else: # random\n",
    "        # get nonzero indices\n",
    "        nonzero_indices = torch.nonzero(best_feature_activations)[:, 0]\n",
    "        # shuffle\n",
    "        shuffled_indices = nonzero_indices[torch.randperm(nonzero_indices.shape[0])]\n",
    "        found_indices = shuffled_indices[:k]\n",
    "    num_datapoints = int(dictionary_activations.shape[0])\n",
    "    datapoint_indices =[np.unravel_index(i, (num_datapoints, token_amount)) for i in found_indices]\n",
    "    text_list = []\n",
    "    full_text = []\n",
    "    token_list = []\n",
    "    full_token_list = []\n",
    "    for md, s_ind in datapoint_indices:\n",
    "        md = int(md)\n",
    "        s_ind = int(s_ind)\n",
    "        full_tok = torch.tensor(dataset[md][\"input_ids\"])\n",
    "        full_text.append(tokenizer.decode(full_tok))\n",
    "        tok = dataset[md][\"input_ids\"][:s_ind+1]\n",
    "        text = tokenizer.decode(tok)\n",
    "        text_list.append(text)\n",
    "        token_list.append(tok)\n",
    "        full_token_list.append(full_tok)\n",
    "    return text_list, full_text, token_list, full_token_list, found_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline before looking at \"deconstructive interference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-47570b60-4b67\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-47570b60-4b67\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"Comp\", \"etitive\", \" enzyme\", \"-\", \"linked\", \" immuno\", \"assay\", \" for\", \" s\", \"ialog\", \"\\n\", \"In\", \" the\", \" medical\", \" field\", \",\", \" doctors\", \" display\", \" medical\", \" images\", \" obtained\", \" by\", \" imaging\", \" patients\", \" on\", \" monitors\", \",\", \" interpret\", \" the\", \" displayed\", \" medical\", \" images\", \",\", \" and\", \" observe\", \" the\", \"\\n\", \"Abstract\", \"\\\\newline\", \"\\\\newline\", \"This\", \" article\", \" is\", \" the\", \" fore\", \"word\", \" to\", \" a\", \" sym\", \"posium\", \",\", \" \\\"\", \"Can\", \" the\", \" Se\", \"am\", \"less\", \" Gar\", \"ment\", \" Be\", \" Se\", \"wn\", \"?\", \" The\", \"\\n\", \"CH\", \"IC\", \"AG\", \"\\n\", \"Pop\", \"!\", \" Harry\", \" Potter\", \" 27\", \"\\\\newline\", \"\\\\newline\", \"3\", \" 3\", \"/\", \"4\", \"\\\"\", \" tall\", \"\\\\newline\", \"\\\\newline\", \"V\", \"inyl\", \"\\\\newline\", \"\\\\newline\", \"Im\", \"ported\", \"\\\\newline\", \"\\\\newline\", \"By\", \" Funk\", \"o\", \"\\\\newline\\\\newline\\\\newline\", \"\\\\newline\", \"Harry\", \" Potter\", \" is\", \" given\", \" a\", \" fun\", \",\", \" and\", \" fun\", \"ky\", \",\", \" styl\", \"\\n\", \"Trans\", \"c\", \"athe\", \"ter\", \" mitral\", \" \\\"\", \"val\", \"ve\", \"-\", \"in\", \"-\", \"ring\", \"\\\"\", \"\\n\", \"Twitter\", \" Del\", \"inqu\", \"ent\", \" Disorder\", \" (\", \"TD\", \"D\", \")\", \" |\", \"\\n\", \"320\", \" F\", \".\", \"2\", \"d\", \" 7\", \"95\", \"\\\\newline\", \"IM\", \"MAC\", \"UL\", \"ATE\", \" CON\", \"CEPT\", \"ION\", \" CH\", \"UR\", \"CH\", \" OF\", \" LOS\", \" AN\", \"G\", \"EL\", \"ES\", \" and\", \" Lake\", \" Cong\", \"reg\", \"ational\", \" Church\", \"\\n\", \"On\", \" Tue\", \"\\n\", \"Last\", \" year\", \",\", \" over\", \" 30\", \",\", \"400\", \" events\", \" were\", \" organised\", \" as\", \" part\", \" of\", \" the\", \" festival\", \" and\", \" over\", \" 750\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"\\\"\", \"Maybe\", \" Mon\", \"\\n\"], \"activations\": [[[-0.04152888059616089]], [[0.5242021679878235]], [[0.21585237979888916]], [[0.35495954751968384]], [[0.002543121576309204]], [[-0.18748079240322113]], [[-0.019493162631988525]], [[-0.037040889263153076]], [[0.0608527809381485]], [[0.9230337142944336]], [[0.0]], [[0.5197615623474121]], [[0.7400957345962524]], [[0.08623554557561874]], [[0.31371885538101196]], [[-0.2884203791618347]], [[-0.10334022343158722]], [[0.0779980942606926]], [[-0.0065562110394239426]], [[0.33464863896369934]], [[-0.4329555630683899]], [[-0.34551802277565]], [[0.2368410974740982]], [[-0.11882033199071884]], [[-0.23460590839385986]], [[-0.4033105671405792]], [[-0.32046201825141907]], [[-0.11964555829763412]], [[0.7301771640777588]], [[-0.06419669091701508]], [[-0.1088266596198082]], [[0.28609341382980347]], [[-0.2932179272174835]], [[0.04202213138341904]], [[-0.2064429670572281]], [[0.7491986155509949]], [[0.0]], [[0.2388153225183487]], [[-0.2603626251220703]], [[-0.24602824449539185]], [[0.3702227473258972]], [[0.36897793412208557]], [[-0.011492308229207993]], [[0.7654796242713928]], [[-0.5107218623161316]], [[0.4452332556247711]], [[-0.20659670233726501]], [[0.2086825668811798]], [[0.1299966275691986]], [[0.5824370980262756]], [[-0.4227767288684845]], [[-0.22572365403175354]], [[-0.16723334789276123]], [[0.7480008602142334]], [[-0.24457690119743347]], [[-0.33211803436279297]], [[-0.15378418564796448]], [[-0.6389805674552917]], [[-0.13667795062065125]], [[-0.17571140825748444]], [[-0.31704381108283997]], [[0.023793049156665802]], [[-0.5405967831611633]], [[0.5055346488952637]], [[0.0]], [[-0.2534915804862976]], [[-0.3000970482826233]], [[0.3792160153388977]], [[0.0]], [[0.4652099907398224]], [[0.27695319056510925]], [[0.039296090602874756]], [[0.2380380779504776]], [[0.012206068262457848]], [[-0.1938275843858719]], [[-0.15628746151924133]], [[-0.06134516000747681]], [[-0.19052566587924957]], [[0.32720375061035156]], [[0.15137968957424164]], [[-0.19721361994743347]], [[-0.8576329350471497]], [[-0.25244206190109253]], [[-0.20726586878299713]], [[-0.19278907775878906]], [[0.05149238556623459]], [[-0.15107452869415283]], [[-0.14387747645378113]], [[-0.2997715473175049]], [[-0.341967910528183]], [[-0.14016571640968323]], [[-0.15242230892181396]], [[0.03526078164577484]], [[0.2679021954536438]], [[0.5705810189247131]], [[-0.1950787901878357]], [[-0.12728166580200195]], [[0.0994303822517395]], [[0.15206892788410187]], [[-0.07849745452404022]], [[-0.038017578423023224]], [[0.2801763415336609]], [[-0.16042360663414001]], [[-0.27987396717071533]], [[0.06568562239408493]], [[-0.16448456048965454]], [[-0.0009922832250595093]], [[-0.23134654760360718]], [[0.08212091028690338]], [[0.0]], [[0.10152100771665573]], [[-0.5823038816452026]], [[0.01725510135293007]], [[-0.3343515396118164]], [[-0.6177705526351929]], [[-0.40984484553337097]], [[-0.33931246399879456]], [[0.0953056663274765]], [[0.24626798927783966]], [[-0.005576640367507935]], [[0.3650546967983246]], [[-0.36170095205307007]], [[-0.23013970255851746]], [[0.0]], [[-0.14322206377983093]], [[-0.1464162915945053]], [[0.06880570948123932]], [[0.2639023959636688]], [[0.0593668594956398]], [[-1.1290751695632935]], [[-0.5652979016304016]], [[0.3387499451637268]], [[-0.35533052682876587]], [[-0.44584566354751587]], [[0.0]], [[0.13252593576908112]], [[0.29728808999061584]], [[0.15050803124904633]], [[0.35799816250801086]], [[0.28226596117019653]], [[-0.8029429912567139]], [[0.09999774396419525]], [[-0.03661353886127472]], [[-0.33450111746788025]], [[-0.58026522397995]], [[-0.257760226726532]], [[-0.08082827180624008]], [[-0.5000669360160828]], [[-0.052246347069740295]], [[-0.16946016252040863]], [[-0.6710805892944336]], [[-0.3926166892051697]], [[-0.4505975842475891]], [[-0.12157410383224487]], [[-0.6901507377624512]], [[-0.12789148092269897]], [[0.2554038465023041]], [[0.27658653259277344]], [[-0.30553072690963745]], [[-0.05612218379974365]], [[-0.21829432249069214]], [[0.05982401221990585]], [[0.3130948543548584]], [[0.27789822220802307]], [[-0.6132107377052307]], [[0.0]], [[0.42115330696105957]], [[-0.6199280619621277]], [[0.0]], [[0.05375101417303085]], [[-0.5801280736923218]], [[-0.45715078711509705]], [[-0.166973277926445]], [[-0.07299414277076721]], [[-0.1587299257516861]], [[-0.3906071186065674]], [[0.1032654345035553]], [[-0.2434808611869812]], [[0.11148975044488907]], [[-0.08212490379810333]], [[-0.16086922585964203]], [[-0.07129010558128357]], [[0.7416059970855713]], [[0.1785675585269928]], [[-0.04851722717285156]], [[-0.05545007809996605]], [[-0.7921035289764404]], [[0.0]], [[0.3025246262550354]], [[0.1550891399383545]], [[-0.09055060893297195]], [[-0.09148449450731277]], [[-0.09119405597448349]], [[-0.11515185236930847]], [[-1.2310266494750977]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fa06c191950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import interp_utils\n",
    "import importlib\n",
    "importlib.reload(interp_utils)\n",
    "\n",
    "feature = 10\n",
    "layer = 0\n",
    "\n",
    "text_list, full_text, token_list, full_token_list, indices = get_feature_datapoints_with_idx(feature, model_activations[layer], model.tokenizer, token_amount, dataset, setting=\"uniform\")\n",
    "interp_utils.visualize_text(text_list, feature, model, None, layer=layer, setting=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at constructive interference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5000\n",
    "neuron_index = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9909,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_relevant_other_neurons(neuron_index: int, layer=0, k=100, weight_cutoff=1.2): # TODO: check weight cutoff vis a vis using quantified models\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\tbest_feature_activations = model_activations[layer][:, :, neuron_index]\n",
    "\tsummed_along_sentence = best_feature_activations.sum(axis=1)\n",
    "\tprint(summed_along_sentence.shape)\n",
    "\t# Find the input data-points that most activate the neuron\n",
    "\tfound_indices = np.argsort(summed_along_sentence)[::-1][:k]\n",
    "\n",
    "\tdef get_activated_neurons(layer: int):\n",
    "\t\tneurons = set()\n",
    "\t\tfor i in found_indices:\n",
    "\t\t\tcutoff_n = model_activations[layer][i, :, :] > weight_cutoff\n",
    "\t\t\t_pos_nonzero, neuron_nonzero = np.nonzero(cutoff_n)\n",
    "\t\t\t# print(\"LEN\", neuron_nonzero.shape)\n",
    "\t\t\tneurons.update(neuron_nonzero)\n",
    "\t\treturn list(neurons)\n",
    "\t\n",
    "\tother_layer_neurons = []\n",
    "\tfor i in range(n_layers):\n",
    "\t\tif i != layer:\n",
    "\t\t\tother_layer_neurons.append((i, get_activated_neurons(i)))\n",
    "\treturn other_layer_neurons\n",
    "\n",
    "other_neurons = get_relevant_other_neurons(neuron_index, layer=0, k=k, weight_cutoff=2)\n",
    "len(other_neurons[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 832)\n",
      "(5000,) 6541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_auxiliary_data(neuron_index: int, layer: int):\n",
    "\t# TODO: this should be a different function!! (UTILS)\n",
    "\t# Get the input data-points that most activate the neuron\n",
    "\tbest_feature_activations = model_activations[layer][:, :, neuron_index]\n",
    "\tsummed_along_sentence = best_feature_activations.sum(axis=1)\n",
    "\t# Find the input data-points that most activate the neuron\n",
    "\tfound_indices = np.argsort(summed_along_sentence)[::-1][:k]\n",
    "\ttotal_other_neurons = sum([len(i[1]) for i in other_neurons])\n",
    "\n",
    "\tconcatenated = np.zeros((len(found_indices), total_other_neurons))\n",
    "\n",
    "\tcounter = 0\n",
    "\tfor other_neur in other_neurons:\n",
    "\t\tother_layer, neurons = other_neur\n",
    "\t\tr = model_activations[other_layer][:, :, neurons][found_indices].sum(axis=1) # Sum over the entire sentence/ text input\n",
    "\t\tconcatenated[:, counter:counter+len(neurons)] = r\n",
    "\t\tcounter += len(neurons)\n",
    "\t\t\n",
    "\treturn concatenated, found_indices\n",
    "\n",
    "\n",
    "aux_data, datapoints_used = get_auxiliary_data(neuron_index, layer=0)\n",
    "# TODO: CONSIDER ONLY USING THE CLOSER LAYERS...\n",
    "print(aux_data.shape), print(datapoints_used.shape, datapoints_used[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gaussian_mixture_model():\n",
    "\t# TODO:\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with iteration 0\n",
      "Done with iteration 1\n",
      "Done with iteration 2\n",
      "Done with iteration 3\n",
      "Done with iteration 4\n",
      "Done with iteration 5\n",
      "Done with iteration 6\n",
      "Done with iteration 7\n",
      "Done with iteration 8\n",
      "Done with iteration 9\n",
      "Done with iteration 10\n",
      "Done with iteration 11\n",
      "Done with iteration 12\n",
      "Done with iteration 13\n",
      "Done with iteration 14\n",
      "Done with iteration 15\n",
      "Done with iteration 16\n",
      "Done with iteration 17\n",
      "Done with iteration 18\n",
      "Done with iteration 19\n",
      "Done with iteration 20\n",
      "Done with iteration 21\n",
      "Done with iteration 22\n",
      "Done with iteration 23\n",
      "Done with iteration 24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# KMeans with cosine similarity\n",
    "def kmeans_cosine(X, n_clusters:int, iterations=100):\n",
    "    k = n_clusters\n",
    "    # Normalize input data\n",
    "    X_normalized = normalize(X, axis=1)\n",
    "\n",
    "    # Randomly initialize centroids\n",
    "    n_samples, n_features = X_normalized.shape\n",
    "    centroids = X_normalized[np.random.choice(n_samples, k, replace=False)]\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        # Cluster assignment step\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for idx, x in enumerate(X_normalized):\n",
    "            # similarities = [cosine_similarity(x, centroid) for centroid in centroids]\n",
    "            similarities = [np.dot(x, centroid) for centroid in centroids]\n",
    "            closest = np.argmax(similarities)\n",
    "            clusters[closest].append(idx)\n",
    "\n",
    "        # Update centroids\n",
    "        # TODO: we maybe able to just **not use** PCA at all here.... slow it may be\n",
    "        new_centroids = []\n",
    "        for cluster in clusters:\n",
    "            if cluster:  # Check if cluster is not empty\n",
    "                new_centroid = np.mean(X_normalized[cluster], axis=0)\n",
    "                new_centroids.append(new_centroid)\n",
    "            else:\n",
    "                new_centroids.append(np.random.rand(n_features))  # Reinitialize empty clusters\n",
    "\n",
    "        new_centroids = np.array(new_centroids)\n",
    "        if np.allclose(centroids, new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        print(\"Done with iteration\", iter)\n",
    "\n",
    "    return centroids, clusters\n",
    "\n",
    "# TODO: no function. Just on global so we can stop middway etc etc\n",
    "# TODO: can we speed this up??? Maybe we use PCA\n",
    "_, cluster_by_idx = kmeans_cosine(aux_data, iterations=400, n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4761, 27, 8, 202, 2]\n"
     ]
    }
   ],
   "source": [
    "print([len(c) for c in cluster_by_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "outf = open(\"cluster_by_idx.json\", \"w\")\n",
    "json.dump(cluster_by_idx, outf, cls=NumpyArrayEncoder)\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6541"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints_used[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The European Elections Monitor\n",
      "\n",
      "In the polls the Socialists just ahead of the outgoing right-wing co\n"
     ]
    }
   ],
   "source": [
    "cluster_idx = 0\n",
    "\n",
    "cluster_inds = datapoints_used[cluster_by_idx[cluster_idx]]\n",
    "print(dataset[int(cluster_inds[4])]['text'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4761, 40, 512)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-3c00d6a9-8a50\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, TextNeuronActivations } from \"https://unpkg.com/circuitsvis@1.41.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-3c00d6a9-8a50\",\n",
       "      TextNeuronActivations,\n",
       "      {\"tokens\": [\"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Setting\", \" timer\", \" interval\", \" based\", \" on\", \" stop\", \"\\n\", \"An\", \"ch\", \"ors\", \" are\", \" used\", \" in\", \"\\n\", \"A\", \"rom\", \"at\", \"isation\", \" of\", \" and\", \"rost\", \"ened\", \"ione\", \" by\", \" human\", \" breast\", \" cancer\", \" tissue\", \":\", \" correlation\", \" with\", \" hormone\", \" receptor\", \" activity\", \" and\", \" possible\", \" bi\", \"ologic\", \" significance\", \"\\n\", \"---\", \"\\\\newline\", \"-\", \"api\", \"-\", \"id\", \":\", \" P\", \":\", \"Windows\", \".\", \"UI\", \".\", \"X\", \"aml\", \".\", \"UI\", \"Element\", \".\", \"Exit\", \"Display\", \"Mode\", \"On\", \"Access\", \"Key\", \"Inv\", \"oked\", \"\\\\newline\", \"-\", \"api\", \"-\", \"\\n\", \"H\", \"aha\", \" I\", \" love\", \" this\", \"!\", \" ALL\", \" THE\", \"\\n\", \"Field\", \" of\", \" the\", \" Invention\", \"\\\\newline\", \"The\", \" present\", \" invention\", \" relates\", \" to\", \"\\n\", \"\\ufeff\", \"Active\", \" Record\", \"\\\\newline\", \"=============\", \"\\\\newline\", \"\\\\newline\", \"\\u05dc\", \"\\u05de\", \"\\u05e8\", \"\\u05d5\\u05ea\", \"\\n\", \"/*\", \"\\\\newline\", \" *\", \" Copyright\", \" 2000\", \"\\n\", \"N\", \"ike\", \" Hyper\", \"f\", \"use\", \" \\u2013\", \" Met\", \"\\n\", \"use\", \" super\", \"::\", \"{\", \"pat\", \"::\", \"Pat\", \"Type\", \",\", \" util\", \"::\", \"Expr\", \"Ext\", \",\", \" *\", \"};\", \"\\\\newline\", \"use\", \" crate\", \"::\", \"{\", \"lex\", \"er\", \"::\", \"Token\", \"Context\", \",\", \" token\", \"::\", \"Assign\", \"Op\", \"Token\", \"};\", \"\\\\newline\", \"use\", \" either\", \"::\", \"Either\", \";\", \"\\n\", \"Ext\", \"\\n\", \"This\", \" paper\", \" offers\", \" a\", \" new\", \" interpretation\", \" of\", \" John\", \" Austin\", \"\\u2019\", \"s\", \" views\", \" both\", \" on\", \" assertion\", \" and\", \" on\", \" ad\", \"ver\", \"bs\", \",\", \" as\", \" result\", \" of\", \" which\", \" an\", \" express\", \"ivist\", \" thesis\", \" concerning\", \" the\", \" semantics\", \" for\", \" action\", \" sentences\", \" is\", \" advanced\", \".\", \" First\", \",\", \"\\n\", \"One\", \" of\", \" the\", \" biggest\", \" women\", \" with\", \" download\", \" Sams\", \" Te\", \"ach\", \" Your\", \"self\", \" countries\", \" was\", \" the\", \" history\", \" itself\", \",\", \" which\", \" was\", \" a\", \" expanded\", \" field\", \" of\", \" access\", \";\", \" a\", \" website\", \" of\", \" physicians\", \" turned\", \"\\n\", \"/*\", \"\\\\newline\", \" *\", \" @\", \"BEGIN\", \" LICENSE\", \"\\\\newline\", \" *\", \"\\\\newline\", \"\\n\", \"2016\", \" Nova\", \" Scotia\", \" Scott\", \"ies\", \" Tournament\", \" of\", \" He\", \"arts\", \"\\\\newline\", \"\\\\newline\", \"The\", \" 2016\", \" Nova\", \" Scotia\", \" Scott\", \"ies\", \" Tournament\", \" of\", \" He\", \"arts\", \",\", \" the\", \" provincial\", \" women\", \"'s\", \" cur\", \"ling\", \" championship\", \" of\", \"\\n\", \"1\", \".\", \" Field\", \" of\", \" the\", \" Invention\", \"\\\\newline\", \"The\", \" present\", \" invention\", \" relates\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"error\", \" returned\", \" in\", \" function\", \" to\", \" find\", \" l\", \"ogn\", \"ormal\", \" for\", \" calculation\", \"\\\\newline\", \"\\\\newline\", \"Finding\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Good\", \" No\", \"SQL\", \" database\", \" for\", \" Write\", \"-\", \"Read\", \" Int\", \"ensive\", \" Site\", \"\\\\newline\", \"\\\\newline\", \"Ok\", \" I\", \" do\", \" have\", \" a\", \" small\", \" messaging\", \" site\", \" for\", \" my\", \" client\", \".\", \" Well\", \" its\", \" more\", \" likely\", \" a\", \" post\", \"\\n\", \"Many\", \" organizations\", \" are\", \" moving\", \" toward\", \" cloud\", \"-\", \"based\", \" services\", \" and\", \" infrastructure\", \" as\", \" to\", \" provide\", \" on\", \"\\n\", \"I\", \" am\", \" sorry\", \" that\", \" it\", \"'s\", \"\\n\", \"\\\\newline\", \"558\", \" P\", \".\", \"2\", \"d\", \" 517\", \" (\", \"1976\", \")\", \"\\\\newline\", \"EX\", \"BER\", \",\", \" INC\", \".,\", \" a\", \" Nevada\", \" corporation\", \",\", \"\\n\", \"1\", \".\", \" Field\", \" of\", \" the\", \" Invention\", \"\\\\newline\", \"The\", \" invention\", \" relates\", \" to\", \" methods\", \" of\", \" inhibiting\", \" tumor\", \" cell\", \" proliferation\", \" by\", \" inhibiting\", \" Fox\", \"M\", \"1\", \"B\", \" activity\", \".\", \"\\n\", \"Despite\", \" the\", \" brief\", \" awakening\", \" offered\", \" by\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Def\", \"ining\", \" equilibrium\", \" points\", \" of\", \" a\", \" 2\", \"D\", \" system\", \" with\", \" a\", \" variable\", \" as\", \" an\", \" equilibrium\", \"\\\\newline\", \"\\\\newline\", \"Given\", \" the\", \" nonlinear\", \" system\", \",\", \" \", \"\\\\newline\", \"$$\", \" x\", \"'\", \" =\", \" x\", \"y\", \" =\", \" f\", \"(\", \"x\", \",\", \"y\", \"\\n\", \"S\", \"olar\", \" Ther\", \"mal\", \"Har\", \"vest\", \"ing\", \" and\", \" storing\", \" the\", \" sun\", \"'s\", \" heat\", \" energy\", \"\\\\newline\", \"\\\\newline\", \"This\", \" is\", \" what\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"finding\", \" last\", \" row\", \" in\", \" excel\", \" and\", \" using\", \" excel\", \".\", \"range\", \"\\\\newline\", \"\\\\newline\", \"I\", \" am\", \" attempting\", \" to\", \" use\", \" a\", \" reference\", \" to\", \" inter\", \"op\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"View\", \"port\", \" for\", \" ip\", \"ad\", \"\\n\", \"Effect\", \" of\", \" acute\", \" and\", \" chronic\", \" psych\", \"ogenic\", \" stress\", \" on\", \" cortic\", \"oad\", \"ren\", \"al\", \" and\", \" pituitary\", \"-\", \"thy\", \"roid\", \" hormones\", \" in\", \" male\", \" rats\", \".\", \"\\\\newline\", \"The\", \" effects\", \" of\", \" acute\", \" and\", \" chronic\", \" stress\", \" on\", \" serum\", \" cortic\", \"osterone\", \" and\", \"\\n\", \"Q\", \":\", \"\\\\newline\", \"\\\\newline\", \"Display\", \" value\", \" of\", \" a\", \" variable\", \" using\", \" X\", \"code\", \" break\", \"point\", \" logging\", \" (\", \"LL\", \"DB\", \")\", \"\\\\newline\", \"\\\\newline\", \"I\", \" have\", \"\\n\", \"Par\", \"ad\", \"ox\", \"ical\", \" migrating\", \" cyst\", \":\", \" an\", \" unusual\", \" presentation\", \" of\", \" intra\", \"vent\", \"ricular\", \" neuro\", \"cy\", \"stic\", \"erc\", \"osis\", \" with\", \" a\", \" coinc\", \"idental\", \"\\n\", \"Syn\", \"opsis\", \"\\\\newline\", \"\\\\newline\", \"G\", \"one\", \" Girl\", \"\\n\"], \"activations\": [[[0.3025246858596802]], [[0.15508921444416046]], [[-0.0905504897236824]], [[-0.091484434902668]], [[0.3090210258960724]], [[0.48964235186576843]], [[0.02116525173187256]], [[-0.35681766271591187]], [[-0.10066448152065277]], [[0.1322018951177597]], [[0.0]], [[0.9029842615127563]], [[0.24841052293777466]], [[-0.25154465436935425]], [[-0.27433907985687256]], [[-0.2350027859210968]], [[-0.2707443833351135]], [[0.0]], [[0.3441028296947479]], [[-0.34631112217903137]], [[-0.21365401148796082]], [[0.38542452454566956]], [[-0.14253482222557068]], [[0.07878934592008591]], [[-0.25272202491760254]], [[0.11425676941871643]], [[-0.13723726570606232]], [[-0.36270520091056824]], [[-0.12616559863090515]], [[-0.22882498800754547]], [[-0.6241117119789124]], [[-0.16728080809116364]], [[-0.06105419248342514]], [[-0.07784336060285568]], [[-0.3297567367553711]], [[0.05716046690940857]], [[-0.09882880747318268]], [[-0.4287419617176056]], [[-0.04454955458641052]], [[-0.3288106322288513]], [[0.1983710676431656]], [[0.04400286078453064]], [[0.04446319490671158]], [[0.0]], [[0.03575419634580612]], [[-0.35658442974090576]], [[0.44375190138816833]], [[0.2711430788040161]], [[0.33731740713119507]], [[-0.049032583832740784]], [[-0.2259511947631836]], [[-0.49498921632766724]], [[0.045621804893016815]], [[-0.050747066736221313]], [[-0.2811937630176544]], [[-0.05239899456501007]], [[-0.16993048787117004]], [[0.10147695988416672]], [[0.5698838829994202]], [[-0.18088963627815247]], [[-0.08096611499786377]], [[0.06101354956626892]], [[-0.09120048582553864]], [[0.0484587699174881]], [[0.04527944326400757]], [[-0.08696962147951126]], [[-0.02313902974128723]], [[-0.14244303107261658]], [[0.28381821513175964]], [[-0.3366313576698303]], [[-0.3025785982608795]], [[-0.055418163537979126]], [[0.4411020278930664]], [[0.30444952845573425]], [[0.35705241560935974]], [[0.0]], [[0.02686239778995514]], [[0.1891184151172638]], [[-0.14745688438415527]], [[-0.3261348605155945]], [[0.5197146534919739]], [[0.07286937534809113]], [[-0.010724633932113647]], [[0.5185350179672241]], [[0.0]], [[0.19332091510295868]], [[-0.06746695935726166]], [[0.8221369385719299]], [[0.9329766035079956]], [[-0.018795542418956757]], [[0.7031450867652893]], [[0.25044190883636475]], [[0.4490179121494293]], [[-0.20338329672813416]], [[-0.2744583487510681]], [[0.0]], [[-0.013006225228309631]], [[-0.13792850077152252]], [[-0.18134154379367828]], [[-0.2247965931892395]], [[0.17655888199806213]], [[-0.2678470015525818]], [[-0.2881026268005371]], [[0.11866722255945206]], [[-0.16499260067939758]], [[-0.13630662858486176]], [[0.2821941375732422]], [[0.0]], [[0.45482176542282104]], [[-0.06383586674928665]], [[0.010380029678344727]], [[0.01989789307117462]], [[-0.04186432808637619]], [[0.0]], [[0.1167663037776947]], [[-0.009642105549573898]], [[-0.22670386731624603]], [[-0.011213801801204681]], [[0.20773380994796753]], [[-0.2592167556285858]], [[-0.6267069578170776]], [[0.0]], [[0.3868870735168457]], [[0.13605830073356628]], [[0.08325211703777313]], [[0.26698195934295654]], [[-0.25985103845596313]], [[-0.0004940181970596313]], [[-0.12115958333015442]], [[0.5622766017913818]], [[-0.41965192556381226]], [[-0.3875570297241211]], [[0.10259483754634857]], [[0.05696897208690643]], [[0.2625529170036316]], [[-0.23766674101352692]], [[-0.0563417449593544]], [[0.01360727846622467]], [[-0.043971315026283264]], [[0.0885087326169014]], [[0.12583768367767334]], [[0.10999040305614471]], [[0.24123160541057587]], [[-0.37820178270339966]], [[-0.04931175708770752]], [[0.01374431699514389]], [[0.2591424286365509]], [[-0.07169477641582489]], [[-0.2853717803955078]], [[0.14891083538532257]], [[0.15184760093688965]], [[-0.4782438278198242]], [[0.5899885892868042]], [[0.4096124470233917]], [[0.08761408179998398]], [[-0.021565377712249756]], [[0.20245596766471863]], [[-0.4061039686203003]], [[0.14193251729011536]], [[-0.273761123418808]], [[-0.3206716775894165]], [[0.0]], [[0.28471291065216064]], [[0.0]], [[0.6082229018211365]], [[0.8619211912155151]], [[0.11639697104692459]], [[0.3540814518928528]], [[0.12792477011680603]], [[0.3764592409133911]], [[-0.056874312460422516]], [[0.030491143465042114]], [[-0.12530222535133362]], [[-0.0690271258354187]], [[0.28927913308143616]], [[0.5376453399658203]], [[0.10608017444610596]], [[-0.21258506178855896]], [[-0.11831855773925781]], [[-0.021155856549739838]], [[-0.08086338639259338]], [[0.22494664788246155]], [[0.09707186371088028]], [[-0.20574772357940674]], [[-0.4960164427757263]], [[-0.2016691118478775]], [[-0.4957563877105713]], [[-0.0285593643784523]], [[-0.06681451201438904]], [[0.4565177261829376]], [[-0.16110247373580933]], [[0.15461795032024384]], [[0.07744300365447998]], [[-0.19821049273014069]], [[0.8307632803916931]], [[-0.02996949851512909]], [[-0.1498914659023285]], [[-0.1694062501192093]], [[-0.23716817796230316]], [[-0.13431768119335175]], [[0.1688690036535263]], [[-0.2680027186870575]], [[-0.152554452419281]], [[-0.2964382767677307]], [[0.0]], [[-0.05925832688808441]], [[-0.07428505271673203]], [[0.7465323805809021]], [[0.12923291325569153]], [[-0.2583758533000946]], [[-0.7016459107398987]], [[0.046779438853263855]], [[-0.4773671627044678]], [[-0.4732910692691803]], [[-0.4553181529045105]], [[0.2816619873046875]], [[0.18520422279834747]], [[-0.32839107513427734]], [[-0.2925253212451935]], [[0.7619228363037109]], [[-0.07691971212625504]], [[-0.3050116300582886]], [[-0.4644542932510376]], [[-0.27881693840026855]], [[-0.23620015382766724]], [[0.18152615427970886]], [[-0.1438877135515213]], [[0.03727240860462189]], [[-0.11817600578069687]], [[-0.024906717240810394]], [[-0.4028019905090332]], [[0.0031767338514328003]], [[-0.3993881344795227]], [[-0.2805071473121643]], [[-0.20747943222522736]], [[-0.35957300662994385]], [[0.0]], [[0.4548216462135315]], [[-0.06383583694696426]], [[0.010379984974861145]], [[0.01537930965423584]], [[0.18330854177474976]], [[-0.11218316853046417]], [[-0.062260858714580536]], [[0.031201470643281937]], [[-0.05379775911569595]], [[0.0]], [[0.436856210231781]], [[-0.06562726199626923]], [[-0.33593305945396423]], [[0.09912645816802979]], [[0.05262455344200134]], [[0.707050085067749]], [[0.2105420082807541]], [[0.09631510078907013]], [[0.22115758061408997]], [[-0.155364528298378]], [[-0.16052834689617157]], [[0.6593371629714966]], [[-0.11766927689313889]], [[-0.22300000488758087]], [[-0.2821432054042816]], [[-0.1706608235836029]], [[0.10269318521022797]], [[0.5593740940093994]], [[0.18768678605556488]], [[0.08106061071157455]], [[0.17769969999790192]], [[-0.3893766701221466]], [[0.661685049533844]], [[0.043730951845645905]], [[-0.09861647337675095]], [[0.2662709951400757]], [[-0.32349324226379395]], [[0.44118279218673706]], [[0.1831790655851364]], [[-0.17366254329681396]], [[0.0]], [[0.14727163314819336]], [[0.030856356024742126]], [[-0.2835325002670288]], [[-0.047851577401161194]], [[0.8011259436607361]], [[0.865570068359375]], [[-0.0015654489398002625]], [[0.6412440538406372]], [[0.20639334619045258]], [[0.3791280686855316]], [[-0.296397864818573]], [[0.0]], [[0.30252474546432495]], [[0.1550893783569336]], [[-0.09055043756961823]], [[-0.09148456156253815]], [[0.4035993814468384]], [[-0.21047461032867432]], [[-0.03914130851626396]], [[0.38662856817245483]], [[-0.1795109510421753]], [[-0.08963319659233093]], [[-0.12149321287870407]], [[0.1817598044872284]], [[0.5349403023719788]], [[-0.17536062002182007]], [[0.11605964601039886]], [[-0.10687752068042755]], [[-0.1396626979112625]], [[0.10172127187252045]], [[0.0]], [[0.30252477526664734]], [[0.15508942306041718]], [[-0.0905504822731018]], [[-0.09148451685905457]], [[0.2523230016231537]], [[-0.5154755711555481]], [[0.12659281492233276]], [[0.005087018013000488]], [[-0.16429275274276733]], [[0.4172542095184326]], [[0.5818702578544617]], [[-0.31834107637405396]], [[-0.4145793914794922]], [[-0.22930888831615448]], [[-0.34369897842407227]], [[-0.16036102175712585]], [[-0.17022360861301422]], [[0.2331305742263794]], [[-0.1961098462343216]], [[-0.1831074357032776]], [[-0.2065916359424591]], [[0.2652767300605774]], [[0.03881418704986572]], [[0.20932398736476898]], [[-0.6399415135383606]], [[-0.11579269170761108]], [[0.1906304508447647]], [[-0.13172394037246704]], [[-0.3243940770626068]], [[-0.15419667959213257]], [[0.21287167072296143]], [[-0.09288196265697479]], [[-0.7067685127258301]], [[0.04541982710361481]], [[-0.040376171469688416]], [[0.0]], [[0.16072505712509155]], [[-0.10312611609697342]], [[-0.20175965130329132]], [[-0.17686820030212402]], [[-0.1788286715745926]], [[-0.2831460237503052]], [[0.32040244340896606]], [[-0.2009749859571457]], [[-0.03602065145969391]], [[-0.10341396927833557]], [[-0.25381579995155334]], [[-0.235809788107872]], [[-0.5900988578796387]], [[-0.02036701887845993]], [[-0.10138215124607086]], [[0.0]], [[0.27923208475112915]], [[-0.1862521916627884]], [[0.3362733721733093]], [[-0.3401203453540802]], [[0.0014729592949151993]], [[0.41270747780799866]], [[0.0]], [[-0.03506879508495331]], [[-0.35732802748680115]], [[-0.5227445960044861]], [[0.016037646681070328]], [[0.3551902770996094]], [[0.20815439522266388]], [[-0.46120887994766235]], [[-1.06486177444458]], [[-0.2950170636177063]], [[-0.37689995765686035]], [[-0.09119842946529388]], [[-0.26735106110572815]], [[-0.0036440640687942505]], [[-0.38107821345329285]], [[-0.7315911650657654]], [[-0.29416245222091675]], [[0.13720154762268066]], [[-0.10821108520030975]], [[-0.23658278584480286]], [[-0.22263123095035553]], [[0.0]], [[0.14727160334587097]], [[0.030856192111968994]], [[-0.28353267908096313]], [[-0.04785142093896866]], [[0.8011257648468018]], [[0.8655703663825989]], [[-0.0015654414892196655]], [[0.6412436962127686]], [[0.3501368761062622]], [[-0.30919012427330017]], [[-0.2578475773334503]], [[0.3181559443473816]], [[0.03669343888759613]], [[0.1191120594739914]], [[-0.2380572259426117]], [[-0.24973931908607483]], [[0.01870625838637352]], [[-0.3151751160621643]], [[-0.043017782270908356]], [[-0.5223090648651123]], [[-0.10850569605827332]], [[0.011534221470355988]], [[0.043139807879924774]], [[-0.38010069727897644]], [[-0.18665142357349396]], [[0.0]], [[0.15213200449943542]], [[0.6267786026000977]], [[-0.10189615190029144]], [[0.23431316018104553]], [[-0.13157345354557037]], [[-0.28803279995918274]], [[0.0]], [[0.30252477526664734]], [[0.15508942306041718]], [[-0.0905504822731018]], [[-0.09148451685905457]], [[-0.014394424855709076]], [[0.5937632918357849]], [[0.4541374444961548]], [[0.09190888702869415]], [[-0.06594869494438171]], [[0.35858747363090515]], [[0.05135362595319748]], [[0.5057104825973511]], [[0.02986079454421997]], [[-0.4600673019886017]], [[0.23862504959106445]], [[0.08879761397838593]], [[-0.16906513273715973]], [[0.5128213167190552]], [[0.23042871057987213]], [[-0.06583921611309052]], [[-0.06438775360584259]], [[-0.00629887729883194]], [[0.7836233377456665]], [[0.5019286274909973]], [[0.08841288089752197]], [[-0.46439820528030396]], [[0.2531130909919739]], [[-0.11464759707450867]], [[0.24290664494037628]], [[-0.12127025425434113]], [[0.006373226642608643]], [[-0.10629020631313324]], [[-0.1490546613931656]], [[-0.23379400372505188]], [[-0.1321849524974823]], [[-0.19857801496982574]], [[-0.5197203159332275]], [[-0.1701253056526184]], [[-0.4510383903980255]], [[-0.3371737003326416]], [[0.0]], [[-0.01116211712360382]], [[-0.25327980518341064]], [[-0.02427859604358673]], [[0.23099112510681152]], [[0.46770697832107544]], [[0.25528645515441895]], [[0.23784416913986206]], [[-0.03130471706390381]], [[0.19144415855407715]], [[0.7293278574943542]], [[-0.1933356672525406]], [[0.3808459937572479]], [[-0.08131960034370422]], [[0.5502817630767822]], [[-0.033621326088905334]], [[-0.05550950765609741]], [[0.31985440850257874]], [[-0.008789803832769394]], [[0.3842293918132782]], [[0.0]], [[0.30252474546432495]], [[0.1550893783569336]], [[-0.09055043756961823]], [[-0.09148456156253815]], [[0.19050955772399902]], [[0.1294361650943756]], [[0.01050613820552826]], [[-0.0346844382584095]], [[0.6594467163085938]], [[0.06045181304216385]], [[-0.05551861971616745]], [[0.49988892674446106]], [[-0.15028202533721924]], [[-0.3319835066795349]], [[-0.1398811787366867]], [[-0.09210802614688873]], [[-0.1349395215511322]], [[-0.24747423827648163]], [[0.005639709532260895]], [[0.06131783127784729]], [[0.2076825052499771]], [[0.20201808214187622]], [[-0.18115749955177307]], [[-0.1740606725215912]], [[-0.5548009872436523]], [[-0.02691977471113205]], [[0.0]], [[0.3025246858596802]], [[0.15508921444416046]], [[-0.0905504897236824]], [[-0.091484434902668]], [[0.41298285126686096]], [[-0.251408189535141]], [[-0.08027340471744537]], [[-0.37185096740722656]], [[0.21974629163742065]], [[0.0]], [[0.748410701751709]], [[0.18833087384700775]], [[-0.6896728873252869]], [[-0.0477302223443985]], [[-0.3451690673828125]], [[-0.11933353543281555]], [[0.5112870335578918]], [[-0.1366269439458847]], [[-0.037871867418289185]], [[-0.5158543586730957]], [[-0.30300629138946533]], [[0.0951734185218811]], [[-0.11499133706092834]], [[-0.05357973277568817]], [[-0.7122127413749695]], [[0.20193864405155182]], [[-0.5734095573425293]], [[-0.5333420038223267]], [[0.29532551765441895]], [[-0.3584934175014496]], [[-0.6094406843185425]], [[-0.2338075488805771]], [[-0.3244933784008026]], [[-0.13744497299194336]], [[0.5634564161300659]], [[0.16619855165481567]], [[-0.04200250655412674]], [[-0.7250421643257141]], [[-0.06314294040203094]], [[-0.3981192708015442]], [[-0.25080716609954834]], [[-0.08027362823486328]], [[-0.329982191324234]], [[-0.5013070106506348]], [[-0.3913715183734894]], [[-0.07059936225414276]], [[0.0]], [[0.30252474546432495]], [[0.1550893783569336]], [[-0.09055043756961823]], [[-0.09148456156253815]], [[0.28297507762908936]], [[0.10987482964992523]], [[0.17231176793575287]], [[0.38383421301841736]], [[0.24005897343158722]], [[-0.13123232126235962]], [[-0.04356318712234497]], [[0.17056652903556824]], [[0.14736391603946686]], [[0.18386200070381165]], [[0.28980153799057007]], [[-1.0132108926773071]], [[-0.6103817224502563]], [[-0.4474128484725952]], [[-0.400518000125885]], [[-0.13232989609241486]], [[-0.14706958830356598]], [[-0.21744826436042786]], [[-0.31248992681503296]], [[0.0]], [[-0.4552721381187439]], [[-0.08918159455060959]], [[-0.28959184885025024]], [[0.27405065298080444]], [[0.23741964995861053]], [[0.04274943470954895]], [[-0.13320788741111755]], [[0.3284966051578522]], [[-0.10040710121393204]], [[0.366988867521286]], [[0.00027254968881607056]], [[-0.2965084910392761]], [[-0.13397230207920074]], [[0.2607268691062927]], [[0.4138701558113098]], [[-0.34548425674438477]], [[0.3343089818954468]], [[-0.19219352304935455]], [[-0.1583755761384964]], [[-0.5608475804328918]], [[0.19970211386680603]], [[-0.41830894351005554]], [[-0.378297358751297]], [[0.0]], [[0.1328752040863037]], [[0.5215644836425781]], [[-0.06533905118703842]], [[-0.03508395701646805]], [[0.04602564871311188]], [[-0.17818132042884827]], [[0.2734972834587097]], [[0.0]]], \"firstDimensionName\": \"Layer\", \"secondDimensionName\": \"Neuron\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fa055d43510>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Somehow this no good enough...\n",
    "\n",
    "# TODO: this now wrong?\n",
    "activations = model_activations[layer][cluster_inds]\n",
    "# .reshape(-1, model_activations[layer].shape[-1])[cluster, :], 1)\n",
    "print(activations.shape)\n",
    "text_list, full_text, token_list, full_token_list, indices = get_feature_datapoints_with_idx(feature, activations, model.tokenizer, token_amount, dataset, setting=\"uniform\", k=30)\n",
    "interp_utils.visualize_text(text_list, feature, model, None, layer=layer, setting=\"model\")\n",
    "# TODO: maybe do everything on MLP side where we get only positive activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
