{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install auto-gptq --quiet\n",
    "!pip install --upgrade optimum --quiet\n",
    "!pip install --upgrade accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Found cached dataset json (/home/lev/.cache/huggingface/datasets/allenai___json/allenai--c4-6fbe877195f42de5/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (0, 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/lev/code/research/ai/play_around/sparse_coding/LEV_GP_Kmeans.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/play_around/sparse_coding/LEV_GP_Kmeans.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/play_around/sparse_coding/LEV_GP_Kmeans.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/play_around/sparse_coding/LEV_GP_Kmeans.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m quantized_model \u001b[39m=\u001b[39m quantizer\u001b[39m.\u001b[39;49mquantize_model(model, tokenizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/play_around/sparse_coding/LEV_GP_Kmeans.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# TODO: save model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optimum/gptq/quantizer.py:344\u001b[0m, in \u001b[0;36mGPTQQuantizer.quantize_model\u001b[0;34m(self, model, tokenizer)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou need to pass `dataset` in order to quantize your model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     dataset \u001b[39m=\u001b[39m get_dataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, tokenizer, seqlen\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_seqlen, split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    345\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    346\u001b[0m     dataset \u001b[39m=\u001b[39m [tokenizer(data, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optimum/gptq/data.py:263\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(dataset_name, tokenizer, nsamples, seqlen, seed, split)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a value in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(get_dataset_map\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m but found \u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m get_dataset_fn \u001b[39m=\u001b[39m get_dataset_map[dataset_name]\n\u001b[0;32m--> 263\u001b[0m \u001b[39mreturn\u001b[39;00m get_dataset_fn(tokenizer\u001b[39m=\u001b[39;49mtokenizer, nsamples\u001b[39m=\u001b[39;49mnsamples, seqlen\u001b[39m=\u001b[39;49mseqlen, split\u001b[39m=\u001b[39;49msplit)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optimum/gptq/data.py:152\u001b[0m, in \u001b[0;36mget_c4\u001b[0;34m(tokenizer, seqlen, nsamples, split)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m enc\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m seqlen:\n\u001b[1;32m    151\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m i \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, enc\u001b[39m.\u001b[39;49minput_ids\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39;49m seqlen \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    153\u001b[0m j \u001b[39m=\u001b[39m i \u001b[39m+\u001b[39m seqlen\n\u001b[1;32m    154\u001b[0m inp \u001b[39m=\u001b[39m enc\u001b[39m.\u001b[39minput_ids[:, i:j]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/random.py:362\u001b[0m, in \u001b[0;36mRandom.randint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandint\u001b[39m(\u001b[39mself\u001b[39m, a, b):\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return random integer in range [a, b], including both end points.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandrange(a, b\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/random.py:345\u001b[0m, in \u001b[0;36mRandom.randrange\u001b[0;34m(self, start, stop, step)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m width \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    344\u001b[0m         \u001b[39mreturn\u001b[39;00m istart \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow(width)\n\u001b[0;32m--> 345\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mempty range for randrange() (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (istart, istop, width))\n\u001b[1;32m    347\u001b[0m \u001b[39m# Non-unit step argument supplied.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m istep \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: empty range for randrange() (0, 0, 0)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from transformers import AwqConfig, AutoConfig\n",
    "from optimum.gptq import GPTQQuantizer, load_quantized_model\n",
    "\n",
    "quantizer = GPTQQuantizer(bits=4, dataset=\"c4\", block_name_to_quantize = \"model.decoder.layers\", model_seqlen = 2048)\n",
    "\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "quantized_model = quantizer.quantize_model(model, tokenizer)\n",
    "# TODO: save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer.save(model, \"quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"Elriggs/pythia-70m-deduped\"\n",
    "\n",
    "# The layer around which we want to interpret by looking at the last layer\n",
    "layer_focus = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformer_lens import HookedTransformer\n",
    "setting = \"residual\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(model_name, device=device)\n",
    "\n",
    "def get_cache_name_neurons(layer: int):\n",
    "    if setting == \"residual\":\n",
    "        cache_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp\":\n",
    "        cache_name = f\"blocks.{layer}.mlp.hook_post\"\n",
    "        neurons = model.cfg.d_mlp\n",
    "    elif setting == \"attention\":\n",
    "        cache_name = f\"blocks.{layer}.hook_attn_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    elif setting == \"mlp_out\":\n",
    "        cache_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "        neurons = model.cfg.d_model\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return cache_name, neurons\n",
    "\n",
    "cache_name, neurons  = get_cache_name_neurons(layer_focus)\n",
    "cache_name_prior, _  = get_cache_name_neurons(layer_focus - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3d37cf93bf4eabea.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-fb7554683ae70ef0.arrow\n",
      "Loading cached processed dataset at /home/lev/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-980bfa4a4898f16b.arrow\n"
     ]
    }
   ],
   "source": [
    "# Downnload dataset\n",
    "from datasets import Dataset, load_dataset\n",
    "dataset_name = \"NeelNanda/pile-10k\"\n",
    "# dataset_name = \"NeelNanda/pile-10k\"\n",
    "# dataset_name = \"JeanKaddour/minipile\"\n",
    "token_amount= 60\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"train[:1000]\").map(\n",
    "    lambda x: model.tokenizer(x['text']),\n",
    "    batched=True,\n",
    ").filter(\n",
    "    lambda x: len(x['input_ids']) > token_amount\n",
    ").map(\n",
    "    lambda x: {'input_ids': x['input_ids'][:token_amount]}\n",
    ")\n",
    "# TODO: we can maybe make this faster for the larger dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ac9e5db32c4d1da62096c9c4a013f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import math\n",
    "\n",
    "# MAX_CHUNK_SIZE = 1_000\n",
    "\n",
    "# TODO: move to a separate file or something\n",
    "def get_activations(f_name):\n",
    "    datapoints = dataset.num_rows\n",
    "    _, samp = model.run_with_cache(torch.tensor(dataset[0][\"input_ids\"]).to(device))\n",
    "    num_features = samp[cache_name].shape[-1]\n",
    "    print(f\"num_features: {num_features}\")\n",
    "    acts_final = np.memmap(f'ACTs-{f_name}.mymemmap', dtype='float32', mode='w+', shape=(datapoints, token_amount, num_features))\n",
    "    batch_size = 32\n",
    "\n",
    "    with torch.no_grad(), dataset.formatted_as(\"pt\"):\n",
    "        dl = DataLoader(dataset[\"input_ids\"], batch_size=batch_size)\n",
    "        for i, batch in enumerate(tqdm(dl)):\n",
    "            _, cache = model.run_with_cache(batch.to(device))\n",
    "            batched_neuron_activations = cache[cache_name]\n",
    "            next_size = min((i+1) * batch_size, datapoints)\n",
    "            # print(next_size, batched_neuron_activations.shape)\n",
    "            acts_final[i*batch_size:next_size,:, :] = batched_neuron_activations.view(next_size - i * batch_size, token_amount, -1).cpu().numpy()\n",
    "    return acts_final\n",
    "\n",
    "activations_main = get_activations(f\"layer_{layer_focus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Gaussian Process Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing requirements for pillow: [Errno 2] No such file or directory: '/home/lev/.local/lib/python3.9/site-packages/Pillow-10.0.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpytorch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
